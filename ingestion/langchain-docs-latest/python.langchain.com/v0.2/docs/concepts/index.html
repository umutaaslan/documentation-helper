<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-concepts" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Conceptual guide | 🦜️🔗 LangChain</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://python.langchain.com/v0.2/img/brand/theme-image.png"><meta data-rh="true" name="twitter:image" content="https://python.langchain.com/v0.2/img/brand/theme-image.png"><meta data-rh="true" property="og:url" content="https://python.langchain.com/docs/concepts/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Conceptual guide | 🦜️🔗 LangChain"><meta data-rh="true" name="description" content="This section contains introductions to key parts of LangChain."><meta data-rh="true" property="og:description" content="This section contains introductions to key parts of LangChain."><link data-rh="true" rel="icon" href="/v0.2/img/brand/favicon.png"><link data-rh="true" rel="canonical" href="https://python.langchain.com/docs/concepts/"><link data-rh="true" rel="alternate" href="https://python.langchain.com/v0.2/docs/concepts/" hreflang="en"><link data-rh="true" rel="alternate" href="https://python.langchain.com/v0.2/docs/concepts/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://VAU016LAWS-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="🦜️🔗 LangChain" href="/v0.2/opensearch.xml">


<script src="/v0.2/js/google_analytics.js"></script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F" async></script><link rel="stylesheet" href="/v0.2/assets/css/styles.0e41ce18.css">
<link rel="preload" href="/v0.2/assets/js/runtime~main.238fea3b.js" as="script">
<link rel="preload" href="/v0.2/assets/js/main.34559fbe.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">A newer LangChain version is out! Check out the <a href="https://python.langchain.com/docs/introduction">latest version</a>.</div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/v0.2/"><div class="navbar__logo"><img src="/v0.2/img/brand/wordmark.png" alt="🦜️🔗 LangChain" class="themedImage_ToTc themedImage--light_HNdA"><img src="/v0.2/img/brand/wordmark-dark.png" alt="🦜️🔗 LangChain" class="themedImage_ToTc themedImage--dark_i4oU"></div></a><a class="navbar__item navbar__link" href="/v0.2/docs/integrations/platforms/">Integrations</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">API reference</a><ul class="dropdown__menu"><li><a href="https://python.langchain.com/v0.2/api_reference/reference.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Latest</a></li><li><a href="https://api.python.langchain.com/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Legacy<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">More</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/v0.2/docs/people/">People</a></li><li><a class="dropdown__link" href="/v0.2/docs/contributing/">Contributing</a></li><li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/README.md" target="_blank" rel="noopener noreferrer" class="dropdown__link">Cookbooks<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/v0.2/docs/additional_resources/tutorials/">3rd party tutorials</a></li><li><a class="dropdown__link" href="/v0.2/docs/additional_resources/youtube/">YouTube</a></li><li><a class="dropdown__link" href="/v0.2/docs/additional_resources/arxiv_references/">arXiv</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">v0.2</a><ul class="dropdown__menu"><li><a href="https://python.langchain.com/docs/introduction/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Latest<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/v0.2/docs/introduction/">v0.2</a></li><li><a href="https://python.langchain.com/v0.1/docs/get_started/introduction" target="_blank" rel="noopener noreferrer" class="dropdown__link">v0.1<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">🦜️🔗</a><ul class="dropdown__menu"><li><a href="https://smith.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangSmith<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://docs.smith.langchain.com/" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangSmith Docs<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://smith.langchain.com/hub" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangChain Hub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://js.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">JS/TS Docs<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a href="https://chat.langchain.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">💬<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/v0.2/docs/introduction/">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link" href="/v0.2/docs/tutorials/">Tutorials</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/graph/">Build a Question Answering application over a Graph Database</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/">Tutorials</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/llm_chain/">Build a Simple LLM Application with LCEL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/query_analysis/">Build a Query Analysis System</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/chatbot/">Build a Chatbot</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/qa_chat_history/">Conversational RAG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/extraction/">Build an Extraction Chain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/agents/">Build an Agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/classification/">Tagging</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/data_generation/">data_generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/local_rag/">Build a Local RAG Application</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/pdf_qa/">Build a PDF ingestion and Question/Answering system</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/rag/">Build a Retrieval Augmented Generation (RAG) App</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/retrievers/">Vector stores and retrievers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/sql_qa/">Build a Question/Answering system over SQL data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/tutorials/summarization/">Summarize Text</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link" href="/v0.2/docs/how_to/">How-to guides</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/">How-to guides</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_chain/">How to use tools in a chain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/vectorstore_retriever/">How to use a vectorstore as a retriever</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chatbots_memory/">How to add memory to chatbots</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/example_selectors/">How to use example selectors</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/graph_mapping/">How to map values to a graph database</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/graph_semantic/">How to add a semantic layer over graph database</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/parallel/">How to invoke runnables in parallel</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chat_streaming/">How to stream chat model responses</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/binding/">How to add default invocation args to a Runnable</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chatbots_retrieval/">How to add retrieval to chatbots</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/few_shot_examples_chat/">How to use few shot examples in chat models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/function_calling/">How to do tool/function calling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/graph_prompting/">How to best prompt for Graph-RAG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/installation/">How to install LangChain packages</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/query_few_shot/">How to add examples to the prompt for query analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/few_shot_examples/">How to use few shot examples</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/functions/">How to run custom functions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/output_parser_structured/">How to use output parsers to parse an LLM response into structured format</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/query_no_queries/">How to handle cases where no queries are generated</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/routing/">How to route between sub-chains</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/structured_output/">How to return structured data from a model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/summarize_map_reduce/">How to summarize text through parallelization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/summarize_refine/">How to summarize text through iterative refinement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/summarize_stuff/">How to summarize text in a single LLM call</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/toolkits/">How to use toolkits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_prompting/">How to add ad-hoc tool calling capability to LLMs and Chat Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/agent_executor/">Build an Agent with AgentExecutor (Legacy)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/graph_constructing/">How to construct knowledge graphs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/prompts_partial/">How to partially format prompt templates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/query_multiple_queries/">How to handle multiple queries when doing query analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_builtin/">How to use built-in tools and toolkits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/passthrough/">How to pass through arguments from one step to the next</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/prompts_composition/">How to compose prompts together</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/query_multiple_retrievers/">How to handle multiple retrievers when doing query analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/assign/">How to add values to a chain&#x27;s state</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/query_constructing_filters/">How to construct filters for query analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/configure/">How to configure runtime chain internals</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/query_high_cardinality/">How deal with high cardinality categoricals when doing query analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_custom/">Custom Document Loader</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/HTML_header_metadata_splitter/">How to split by HTML header</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/HTML_section_aware_splitter/">How to split by HTML sections</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/MultiQueryRetriever/">How to use the MultiQueryRetriever</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/add_scores_retriever/">How to add scores to retriever results</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/caching_embeddings/">Caching</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/callbacks_async/">How to use callbacks in async environments</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/callbacks_attach/">How to attach callbacks to a runnable</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/callbacks_constructor/">How to propagate callbacks  constructor</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/callbacks_custom_events/">How to dispatch custom callback events</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/callbacks_runtime/">How to pass callbacks in at runtime</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/character_text_splitter/">How to split by character</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chat_model_caching/">How to cache chat model responses</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chat_model_rate_limiting/">How to handle rate limits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chat_models_universal_init/">How to init any model in one line</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chat_token_usage_tracking/">How to track token usage in ChatModels</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/chatbots_tools/">How to add tools to chatbots</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/code_splitter/">How to split code</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/contextual_compression/">How to do retrieval with contextual compression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/convert_runnable_to_tool/">How to convert Runnables as Tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/custom_callbacks/">How to create custom callback handlers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/custom_chat_model/">How to create a custom chat model class</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/custom_llm/">How to create a custom LLM class</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/custom_retriever/">Custom Retriever</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/custom_tools/">How to create tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/debugging/">How to debug your LLM apps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_csv/">How to load CSVs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_directory/">How to load documents from a directory</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_html/">How to load HTML</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_json/">How to load JSON</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_markdown/">How to load Markdown</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_office_file/">How to load Microsoft Office files</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/document_loader_pdf/">How to load PDFs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/dynamic_chain/">How to create a dynamic (self-constructing) chain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/embed_text/">Text embedding models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/ensemble_retriever/">How to combine results from multiple retrievers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/example_selectors_langsmith/">How to select examples from a LangSmith dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/example_selectors_length_based/">How to select examples by length</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/example_selectors_mmr/">How to select examples by maximal marginal relevance (MMR)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/example_selectors_ngram/">How to select examples by n-gram overlap</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/example_selectors_similarity/">How to select examples by similarity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/extraction_examples/">How to use reference examples when doing extraction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/extraction_long_text/">How to handle long text when doing extraction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/extraction_parse/">How to use prompting alone (no tool calling) to do extraction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/fallbacks/">How to add fallbacks to a runnable</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/filter_messages/">How to filter messages</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/hybrid/">Hybrid Search</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/indexing/">How to use the LangChain indexing API</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/inspect/">How to inspect runnables</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/lcel_cheatsheet/">LangChain Expression Language Cheatsheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/llm_caching/">How to cache LLM responses</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/llm_token_usage_tracking/">How to track token usage for LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/local_llms/">Run models locally</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/logprobs/">How to get log probabilities</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/long_context_reorder/">How to reorder retrieved results to mitigate the &quot;lost in the middle&quot; effect</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/markdown_header_metadata_splitter/">How to split Markdown by Headers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/merge_message_runs/">How to merge consecutive messages of the same type</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/message_history/">How to add message history</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/migrate_agent/">How to migrate from legacy LangChain agents to LangGraph</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/multi_vector/">How to retrieve using multiple vectors per document</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/multimodal_inputs/">How to pass multimodal data directly to models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/multimodal_prompts/">How to use multimodal prompts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/output_parser_custom/">How to create a custom Output Parser</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/output_parser_fixing/">How to use the output-fixing parser</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/output_parser_json/">How to parse JSON output</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/output_parser_retry/">How to retry when a parsing error occurs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/output_parser_xml/">How to parse XML output</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/output_parser_yaml/">How to parse YAML output</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/parent_document_retriever/">How to use the Parent Document Retriever</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/pydantic_compatibility/">How to use LangChain with different Pydantic versions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/qa_chat_history_how_to/">How to add chat history</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/qa_citations/">How to get a RAG application to add citations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/qa_per_user/">How to do per-user retrieval</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/qa_sources/">How to get your RAG application to return sources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/qa_streaming/">How to stream results from your RAG application</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/recursive_json_splitter/">How to split JSON data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/recursive_text_splitter/">How to recursively split text by characters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/response_metadata/">Response metadata</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/runnable_runtime_secrets/">How to pass runtime secrets to runnables</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/self_query/">How to do &quot;self-querying&quot; retrieval</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/semantic-chunker/">How to split text based on semantic similarity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/sequence/">How to chain runnables</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/serialization/">How to save and load LangChain objects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/split_by_token/">How to split text by tokens</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/sql_csv/">How to do question answering over CSVs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/sql_large_db/">How to deal with large databases when doing SQL question-answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/sql_prompting/">How to better prompt when doing SQL question-answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/sql_query_checking/">How to do query validation as part of SQL question-answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/streaming/">How to stream runnables</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/streaming_llm/">How to stream responses from an LLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/time_weighted_vectorstore/">How to use a time-weighted vector store retriever</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_artifacts/">How to return artifacts from a tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_calling/">How to use chat models to call tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_calling_parallel/">How to disable parallel tool calling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_choice/">How to force models to call a tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_configure/">How to access the RunnableConfig from a tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_results_pass_to_model/">How to pass tool outputs to chat models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_runtime/">How to pass run time values to tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_stream_events/">How to stream events from a tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tool_streaming/">How to stream tool calls</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_as_openai_functions/">How to convert tools to OpenAI Functions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_error/">How to handle tool errors</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_few_shot/">How to use few-shot prompting with tool calling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_human/">How to add a human-in-the-loop for tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/tools_model_specific/">How to bind model-specific tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/trim_messages/">How to trim messages</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/vectorstores/">How to create and query vector stores</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/v0.2/docs/concepts/">Conceptual guide</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">Ecosystem</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a href="https://docs.smith.langchain.com/" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK" tabindex="0">🦜🛠️ LangSmith<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a href="https://langchain-ai.github.io/langgraph/" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK" tabindex="0">🦜🕸️ LangGraph<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">Versions</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/overview/">Overview of v0.2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/release_policy/">Release policy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.2/docs/how_to/pydantic_compatibility/">Pydantic compatibility</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/v0_2/">Migrating to v0.2</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/v0_2/">Migrating to LangChain v0.2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/v0_2/migrating_astream_events/">astream_events v2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/v0_2/deprecations/">Changes</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/">Migrating from v0.0 chains</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/">How to migrate from v0.0 chains</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/constitutional_chain/">Migrating from ConstitutionalChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/conversation_chain/">Migrating from ConversationalChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/conversation_retrieval_chain/">Migrating from ConversationalRetrievalChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/llm_chain/">Migrating from LLMChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/llm_math_chain/">Migrating from LLMMathChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/llm_router_chain/">Migrating from LLMRouterChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/map_reduce_chain/">Migrating from MapReduceDocumentsChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/map_rerank_docs_chain/">Migrating from MapRerankDocumentsChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/multi_prompt_chain/">Migrating from MultiPromptChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/refine_docs_chain/">Migrating from RefineDocumentsChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/retrieval_qa/">Migrating from RetrievalQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.2/docs/versions/migrating_chains/stuff_docs_chain/">Migrating from StuffDocumentsChain</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/v0.2/docs/security/">Security</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="theme-doc-version-banner alert alert--warning margin-bottom--md" role="alert"><div>This is documentation for <!-- -->LangChain<!-- --> <b>v0.2</b>, which is no longer actively maintained.</div><div class="margin-top--md">For the current stable version, see <b><a href="https://python.langchain.com/docs/concepts/" target="_blank" rel="noopener noreferrer">this version</a></b> (<!-- -->Latest<!-- -->).</div></div><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/v0.2/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Conceptual guide</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Conceptual guide</h1><p>This section contains introductions to key parts of LangChain.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">​</a></h2><p>LangChain as a framework consists of a number of packages.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="langchain-core"><code>langchain-core</code><a href="#langchain-core" class="hash-link" aria-label="Direct link to langchain-core" title="Direct link to langchain-core">​</a></h3><p>This package contains base abstractions of different components and ways to compose them together.
The interfaces for core components like LLMs, vector stores, retrievers and more are defined here.
No third party integrations are defined here.
The dependencies are kept purposefully very lightweight.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="langchain"><code>langchain</code><a href="#langchain" class="hash-link" aria-label="Direct link to langchain" title="Direct link to langchain">​</a></h3><p>The main <code>langchain</code> package contains chains, agents, and retrieval strategies that make up an application&#x27;s cognitive architecture.
These are NOT third party integrations.
All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="langchain-community"><code>langchain-community</code><a href="#langchain-community" class="hash-link" aria-label="Direct link to langchain-community" title="Direct link to langchain-community">​</a></h3><p>This package contains third party integrations that are maintained by the LangChain community.
Key partner packages are separated out (see below).
This contains all integrations for various components (LLMs, vector stores, retrievers).
All dependencies in this package are optional to keep the package as lightweight as possible.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="partner-packages">Partner packages<a href="#partner-packages" class="hash-link" aria-label="Direct link to Partner packages" title="Direct link to Partner packages">​</a></h3><p>While the long tail of integrations is in <code>langchain-community</code>, we split popular integrations into their own packages (e.g. <code>langchain-openai</code>, <code>langchain-anthropic</code>, etc).
This was done in order to improve support for these important integrations.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="langgraph"><a href="https://langchain-ai.github.io/langgraph" target="_blank" rel="noopener noreferrer"><code>langgraph</code></a><a href="#langgraph" class="hash-link" aria-label="Direct link to langgraph" title="Direct link to langgraph">​</a></h3><p><code>langgraph</code> is an extension of <code>langchain</code> aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.</p><p>LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="langserve"><a href="/v0.2/docs/langserve/"><code>langserve</code></a><a href="#langserve" class="hash-link" aria-label="Direct link to langserve" title="Direct link to langserve">​</a></h3><p>A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="langsmith"><a href="https://docs.smith.langchain.com" target="_blank" rel="noopener noreferrer">LangSmith</a><a href="#langsmith" class="hash-link" aria-label="Direct link to langsmith" title="Direct link to langsmith">​</a></h3><p>A developer platform that lets you debug, test, evaluate, and monitor LLM applications.</p><img src="/v0.2/svg/langchain_stack_062024.svg" alt="Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers." class="themedImage_ToTc themedImage--light_HNdA" title="LangChain Framework Overview" style="width:100%"><img src="/v0.2/svg/langchain_stack_062024_dark.svg" alt="Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers." class="themedImage_ToTc themedImage--dark_i4oU" title="LangChain Framework Overview" style="width:100%"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="langchain-expression-language-lcel">LangChain Expression Language (LCEL)<a href="#langchain-expression-language-lcel" class="hash-link" aria-label="Direct link to LangChain Expression Language (LCEL)" title="Direct link to LangChain Expression Language (LCEL)">​</a></h2><span data-heading-keywords="lcel"></span><p><code>LangChain Expression Language</code>, or <code>LCEL</code>, is a declarative way to chain LangChain components.
LCEL was designed from day 1 to <strong>support putting prototypes in production, with no code changes</strong>, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:</p><ul><li><p><strong>First-class streaming support:</strong>
When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.</p></li><li><p><strong>Async support:</strong>
Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a <a href="/v0.2/docs/langserve/">LangServe</a> server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.</p></li><li><p><strong>Optimized parallel execution:</strong>
Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.</p></li><li><p><strong>Retries and fallbacks:</strong>
Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.</p></li><li><p><strong>Access intermediate results:</strong>
For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every <a href="/v0.2/docs/langserve/">LangServe</a> server.</p></li><li><p><strong>Input and output schemas</strong>
Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.</p></li><li><p><a href="https://docs.smith.langchain.com" target="_blank" rel="noopener noreferrer"><strong>Seamless LangSmith tracing</strong></a>
As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.
With LCEL, <strong>all</strong> steps are automatically logged to <a href="https://docs.smith.langchain.com/" target="_blank" rel="noopener noreferrer">LangSmith</a> for maximum observability and debuggability.</p></li></ul><p>LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as <code>LLMChain</code> and
<code>ConversationalRetrievalChain</code>. Many of these legacy chains hide important details like prompts, and as a wider variety
of viable models emerge, customization has become more and more important.</p><p>If you are currently using one of these legacy chains, please see <a href="/v0.2/docs/versions/migrating_chains/">this guide for guidance on how to migrate</a>.</p><p>For guides on how to do specific tasks with LCEL, check out <a href="/v0.2/docs/how_to/#langchain-expression-language-lcel">the relevant how-to guides</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="runnable-interface">Runnable interface<a href="#runnable-interface" class="hash-link" aria-label="Direct link to Runnable interface" title="Direct link to Runnable interface">​</a></h3><span data-heading-keywords="invoke,runnable"></span><p>To make it as easy as possible to create custom chains, we&#x27;ve implemented a <a href="https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" target="_blank" rel="noopener noreferrer">&quot;Runnable&quot;</a> protocol. Many LangChain components implement the <code>Runnable</code> protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about below.</p><p>This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.
The standard interface includes:</p><ul><li><code>stream</code>: stream back chunks of the response</li><li><code>invoke</code>: call the chain on an input</li><li><code>batch</code>: call the chain on a list of inputs</li></ul><p>These also have corresponding async methods that should be used with <a href="https://docs.python.org/3/library/asyncio.html" target="_blank" rel="noopener noreferrer">asyncio</a> <code>await</code> syntax for concurrency:</p><ul><li><code>astream</code>: stream back chunks of the response async</li><li><code>ainvoke</code>: call the chain on an input async</li><li><code>abatch</code>: call the chain on a list of inputs async</li><li><code>astream_log</code>: stream back intermediate steps as they happen, in addition to the final response</li><li><code>astream_events</code>: <strong>beta</strong> stream events as they happen in the chain (introduced in <code>langchain-core</code> 0.1.14)</li></ul><p>The <strong>input type</strong> and <strong>output type</strong> varies by component:</p><table><thead><tr><th>Component</th><th>Input Type</th><th>Output Type</th></tr></thead><tbody><tr><td>Prompt</td><td>Dictionary</td><td>PromptValue</td></tr><tr><td>ChatModel</td><td>Single string, list of chat messages or a PromptValue</td><td>ChatMessage</td></tr><tr><td>LLM</td><td>Single string, list of chat messages or a PromptValue</td><td>String</td></tr><tr><td>OutputParser</td><td>The output of an LLM or ChatModel</td><td>Depends on the parser</td></tr><tr><td>Retriever</td><td>Single string</td><td>List of Documents</td></tr><tr><td>Tool</td><td>Single string or dictionary, depending on the tool</td><td>Depends on the tool</td></tr></tbody></table><p>All runnables expose input and output <strong>schemas</strong> to inspect the inputs and outputs:</p><ul><li><code>input_schema</code>: an input Pydantic model auto-generated from the structure of the Runnable</li><li><code>output_schema</code>: an output Pydantic model auto-generated from the structure of the Runnable</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="components">Components<a href="#components" class="hash-link" aria-label="Direct link to Components" title="Direct link to Components">​</a></h2><p>LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.
Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="chat-models">Chat models<a href="#chat-models" class="hash-link" aria-label="Direct link to Chat models" title="Direct link to Chat models">​</a></h3><span data-heading-keywords="chat model,chat models"></span><p>Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).
These are traditionally newer models (older models are generally <code>LLMs</code>, see below).
Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.</p><p>Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input. This means you can easily use chat models in place of LLMs.</p><p>When a string is passed in as input, it is converted to a <code>HumanMessage</code> and then passed to the underlying model.</p><p>LangChain does not host any Chat Models, rather we rely on third party integrations.</p><p>We have some standardized parameters when constructing ChatModels:</p><ul><li><code>model</code>: the name of the model</li><li><code>temperature</code>: the sampling temperature</li><li><code>timeout</code>: request timeout</li><li><code>max_tokens</code>: max tokens to generate</li><li><code>stop</code>: default stop sequences</li><li><code>max_retries</code>: max number of times to retry requests</li><li><code>api_key</code>: API key for the model provider</li><li><code>base_url</code>: endpoint to send requests to</li></ul><p>Some important things to note:</p><ul><li>standard params only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can&#x27;t be supported on these.</li><li>standard params are currently only enforced on integrations that have their own integration packages (e.g. <code>langchain-openai</code>, <code>langchain-anthropic</code>, etc.), they&#x27;re not enforced on models in <code>langchain-community</code>.</li></ul><p>ChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the API reference for that model.</p><div class="theme-admonition theme-admonition-important alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>Some chat models have been fine-tuned for <strong>tool calling</strong> and provide a dedicated API for it.
Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.
Please see the <a href="/v0.2/docs/concepts/#functiontool-calling">tool calling section</a> for more information.</p></div></div><p>For specifics on how to use chat models, see the <a href="/v0.2/docs/how_to/#chat-models">relevant how-to guides here</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="multimodality">Multimodality<a href="#multimodality" class="hash-link" aria-label="Direct link to Multimodality" title="Direct link to Multimodality">​</a></h4><p>Some chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning model providers haven&#x27;t standardized on the &quot;best&quot; way to define the API. Multimodal <strong>outputs</strong> are even less common. As such, we&#x27;ve kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction patterns as the field matures.</p><p>In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI&#x27;s content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.</p><p>For specifics on how to use multimodal models, see the <a href="/v0.2/docs/how_to/#multimodal">relevant how-to guides here</a>.</p><p>For a full list of LangChain model providers with multimodal models, <a href="/v0.2/docs/integrations/chat/#advanced-features">check out this table</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="llms">LLMs<a href="#llms" class="hash-link" aria-label="Direct link to LLMs" title="Direct link to LLMs">​</a></h3><span data-heading-keywords="llm,llms"></span><div class="theme-admonition theme-admonition-caution alert alert--warning admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_S0QG"><p>Pure text-in/text-out LLMs tend to be older or lower-level. Many new popular models are best used as <a href="/v0.2/docs/concepts/#chat-models">chat completion models</a>,
even for non-chat use cases.</p><p>You are probably looking for <a href="/v0.2/docs/concepts/#chat-models">the section above instead</a>.</p></div></div><p>Language models that takes a string as input and returns a string.
These are traditionally older models (newer models generally are <a href="/v0.2/docs/concepts/#chat-models">Chat Models</a>, see above).</p><p>Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.
This gives them the same interface as <a href="/v0.2/docs/concepts/#chat-models">Chat Models</a>.
When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.</p><p>LangChain does not host any LLMs, rather we rely on third party integrations.</p><p>For specifics on how to use LLMs, see the <a href="/v0.2/docs/how_to/#llms">how-to guides</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="messages">Messages<a href="#messages" class="hash-link" aria-label="Direct link to Messages" title="Direct link to Messages">​</a></h3><p>Some language models take a list of messages as input and return a message.
There are a few different types of messages.
All messages have a <code>role</code>, <code>content</code>, and <code>response_metadata</code> property.</p><p>The <code>role</code> describes WHO is saying the message. The standard roles are &quot;user&quot;, &quot;assistant&quot;, &quot;system&quot;, and &quot;tool&quot;.
LangChain has different message classes for different roles.</p><p>The <code>content</code> property describes the content of the message.
This can be a few different things:</p><ul><li>A string (most models deal with this type of content)</li><li>A List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and that input location)</li></ul><p>Optionally, messages can have a <code>name</code> property which allows for differentiating between multiple speakers with the same role.
For example, if there are two users in the chat history it can be useful to differentiate between them. Not all models support this.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="humanmessage">HumanMessage<a href="#humanmessage" class="hash-link" aria-label="Direct link to HumanMessage" title="Direct link to HumanMessage">​</a></h4><p>This represents a message with role &quot;user&quot;.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="aimessage">AIMessage<a href="#aimessage" class="hash-link" aria-label="Direct link to AIMessage" title="Direct link to AIMessage">​</a></h4><p>This represents a message with role &quot;assistant&quot;. In addition to the <code>content</code> property, these messages also have:</p><p><strong><code>response_metadata</code></strong></p><p>The <code>response_metadata</code> property contains additional metadata about the response. The data here is often specific to each model provider.
This is where information like log-probs and token usage may be stored.</p><p><strong><code>tool_calls</code></strong></p><p>These represent a decision from an language model to call a tool. They are included as part of an <code>AIMessage</code> output.
They can be accessed from there with the <code>.tool_calls</code> property.</p><p>This property returns a list of <code>ToolCall</code>s. A <code>ToolCall</code> is a dictionary with the following arguments:</p><ul><li><code>name</code>: The name of the tool that should be called.</li><li><code>args</code>: The arguments to that tool.</li><li><code>id</code>: The id of that tool call.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="systemmessage">SystemMessage<a href="#systemmessage" class="hash-link" aria-label="Direct link to SystemMessage" title="Direct link to SystemMessage">​</a></h4><p>This represents a message with role &quot;system&quot;, which tells the model how to behave. Not every model provider supports this.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="toolmessage">ToolMessage<a href="#toolmessage" class="hash-link" aria-label="Direct link to ToolMessage" title="Direct link to ToolMessage">​</a></h4><p>This represents a message with role &quot;tool&quot;, which contains the result of calling a tool. In addition to <code>role</code> and <code>content</code>, this message has:</p><ul><li>a <code>tool_call_id</code> field which conveys the id of the call to the tool that was called to produce this result.</li><li>an <code>artifact</code> field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="legacy-functionmessage">(Legacy) FunctionMessage<a href="#legacy-functionmessage" class="hash-link" aria-label="Direct link to (Legacy) FunctionMessage" title="Direct link to (Legacy) FunctionMessage">​</a></h4><p>This is a legacy message type, corresponding to OpenAI&#x27;s legacy function-calling API. <code>ToolMessage</code> should be used instead to correspond to the updated tool-calling API.</p><p>This represents the result of a function call. In addition to <code>role</code> and <code>content</code>, this message has a <code>name</code> parameter which conveys the name of the function that was called to produce this result.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-templates">Prompt templates<a href="#prompt-templates" class="hash-link" aria-label="Direct link to Prompt templates" title="Direct link to Prompt templates">​</a></h3><span data-heading-keywords="prompt,prompttemplate,chatprompttemplate"></span><p>Prompt templates help to translate user input and parameters into instructions for a language model.
This can be used to guide a model&#x27;s response, helping it understand the context and generate relevant and coherent language-based output.</p><p>Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.</p><p>Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.
The reason this PromptValue exists is to make it easy to switch between strings and messages.</p><p>There are a few different types of prompt templates:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="string-prompttemplates">String PromptTemplates<a href="#string-prompttemplates" class="hash-link" aria-label="Direct link to String PromptTemplates" title="Direct link to String PromptTemplates">​</a></h4><p>These prompt templates are used to format a single string, and generally are used for simpler inputs.
For example, a common way to construct and use a PromptTemplate is as follows:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">prompts </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> PromptTemplate</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt_template </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> PromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;Tell me a joke about {topic}&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;topic&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;cats&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html">PromptTemplate</a></span></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="chatprompttemplates">ChatPromptTemplates<a href="#chatprompttemplates" class="hash-link" aria-label="Direct link to ChatPromptTemplates" title="Direct link to ChatPromptTemplates">​</a></h4><p>These prompt templates are used to format a list of messages. These &quot;templates&quot; consist of a list of templates themselves.
For example, a common way to construct and use a ChatPromptTemplate is as follows:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">prompts </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatPromptTemplate</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt_template </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatPromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_messages</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;system&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;You are a helpful assistant&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;Tell me a joke about {topic}&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;topic&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;cats&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html">ChatPromptTemplate</a></span></div><p>In the above example, this ChatPromptTemplate will construct two messages when called.
The first is a system message, that has no variables to format.
The second is a HumanMessage, and will be formatted by the <code>topic</code> variable the user passes in.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="messagesplaceholder">MessagesPlaceholder<a href="#messagesplaceholder" class="hash-link" aria-label="Direct link to MessagesPlaceholder" title="Direct link to MessagesPlaceholder">​</a></h4><span data-heading-keywords="messagesplaceholder"></span><p>This prompt template is responsible for adding a list of messages in a particular place.
In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.
But what if we wanted the user to pass in a list of messages that we would slot into a particular spot?
This is how you use MessagesPlaceholder.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">prompts </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatPromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> MessagesPlaceholder</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">messages </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> HumanMessage</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt_template </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatPromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_messages</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;system&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;You are a helpful assistant&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    MessagesPlaceholder</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;msgs&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;msgs&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">HumanMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;hi!&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html">ChatPromptTemplate</a> | </span><span><a href="https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html">MessagesPlaceholder</a> | </span><span><a href="https://python.langchain.com/v0.2/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html">HumanMessage</a></span></div><p>This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.
If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).
This is useful for letting a list of messages be slotted into a particular spot.</p><p>An alternative way to accomplish the same thing without using the <code>MessagesPlaceholder</code> class explicitly is:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">prompt_template </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatPromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_messages</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;system&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;You are a helpful assistant&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;placeholder&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;{msgs}&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(0, 128, 0)"># &lt;-- This is the changed part</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>For specifics on how to use prompt templates, see the <a href="/v0.2/docs/how_to/#prompt-templates">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="example-selectors">Example selectors<a href="#example-selectors" class="hash-link" aria-label="Direct link to Example selectors" title="Direct link to Example selectors">​</a></h3><p>One common prompting technique for achieving better performance is to include examples as part of the prompt.
This is known as <a href="/v0.2/docs/concepts/#few-shot-prompting">few-shot prompting</a>.
This gives the language model concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.
Example Selectors are classes responsible for selecting and then formatting examples into prompts.</p><p>For specifics on how to use example selectors, see the <a href="/v0.2/docs/how_to/#example-selectors">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="output-parsers">Output parsers<a href="#output-parsers" class="hash-link" aria-label="Direct link to Output parsers" title="Direct link to Output parsers">​</a></h3><span data-heading-keywords="output parser"></span><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.
More and more models are supporting function (or tool) calling, which handles this automatically.
It is recommended to use function/tool calling rather than output parsing.
See documentation for that <a href="/v0.2/docs/concepts/#function-tool-calling">here</a>.</p></div></div><p><code>Output parser</code> is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.
Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.</p><p>LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:</p><ul><li><strong>Name</strong>: The name of the output parser</li><li><strong>Supports Streaming</strong>: Whether the output parser supports streaming.</li><li><strong>Has Format Instructions</strong>: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.</li><li><strong>Calls LLM</strong>: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.</li><li><strong>Input Type</strong>: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.</li><li><strong>Output Type</strong>: The output type of the object returned by the parser.</li><li><strong>Description</strong>: Our commentary on this output parser and when to use it.</li></ul><table><thead><tr><th>Name</th><th>Supports Streaming</th><th>Has Format Instructions</th><th>Calls LLM</th><th>Input Type</th><th>Output Type</th><th>Description</th></tr></thead><tbody><tr><td><a href="https://python.langchain.com/v0.2/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html#langchain_core.output_parsers.json.JsonOutputParser" target="_blank" rel="noopener noreferrer">JSON</a></td><td>✅</td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td>JSON object</td><td>Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/core/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html#langchain_core.output_parsers.xml.XMLOutputParser" target="_blank" rel="noopener noreferrer">XML</a></td><td>✅</td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>dict</code></td><td>Returns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic&#x27;s).</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/core/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#langchain_core.output_parsers.list.CommaSeparatedListOutputParser" target="_blank" rel="noopener noreferrer">CSV</a></td><td>✅</td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>List[str]</code></td><td>Returns a list of comma separated values.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/langchain/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html#langchain.output_parsers.fix.OutputFixingParser" target="_blank" rel="noopener noreferrer">OutputFixing</a></td><td></td><td></td><td>✅</td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td></td><td>Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/langchain/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html#langchain.output_parsers.retry.RetryWithErrorOutputParser" target="_blank" rel="noopener noreferrer">RetryWithError</a></td><td></td><td></td><td>✅</td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td></td><td>Wraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html#langchain_core.output_parsers.pydantic.PydanticOutputParser" target="_blank" rel="noopener noreferrer">Pydantic</a></td><td></td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>pydantic.BaseModel</code></td><td>Takes a user defined Pydantic model and returns data in that format.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/langchain/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html#langchain.output_parsers.yaml.YamlOutputParser" target="_blank" rel="noopener noreferrer">YAML</a></td><td></td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>pydantic.BaseModel</code></td><td>Takes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/langchain/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser" target="_blank" rel="noopener noreferrer">PandasDataFrame</a></td><td></td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>dict</code></td><td>Useful for doing operations with pandas DataFrames.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/langchain/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser" target="_blank" rel="noopener noreferrer">Enum</a></td><td></td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>Enum</code></td><td>Parses response into one of the provided enum values.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/langchain/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser" target="_blank" rel="noopener noreferrer">Datetime</a></td><td></td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>datetime.datetime</code></td><td>Parses response into a datetime string.</td></tr><tr><td><a href="https://python.langchain.com/v0.2/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser" target="_blank" rel="noopener noreferrer">Structured</a></td><td></td><td>✅</td><td></td><td><code>str</code> <!-- -->|<!-- --> <code>Message</code></td><td><code>Dict[str, str]</code></td><td>An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.</td></tr></tbody></table><p>For specifics on how to use output parsers, see the <a href="/v0.2/docs/how_to/#output-parsers">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="chat-history">Chat history<a href="#chat-history" class="hash-link" aria-label="Direct link to Chat history" title="Direct link to Chat history">​</a></h3><p>Most LLM applications have a conversational interface.
An essential component of a conversation is being able to refer to information introduced earlier in the conversation.
At bare minimum, a conversational system should be able to access some window of past messages directly.</p><p>The concept of <code>ChatHistory</code> refers to a class in LangChain which can be used to wrap an arbitrary chain.
This <code>ChatHistory</code> will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database.
Future interactions will then load those messages and pass them into the chain as part of the input.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="documents">Documents<a href="#documents" class="hash-link" aria-label="Direct link to Documents" title="Direct link to Documents">​</a></h3><span data-heading-keywords="document,documents"></span><p>A Document object in LangChain contains information about some data. It has two attributes:</p><ul><li><code>page_content: str</code>: The content of this document. Currently is only a string.</li><li><code>metadata: dict</code>: Arbitrary metadata associated with this document. Can track the document id, file name, etc.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="document-loaders">Document loaders<a href="#document-loaders" class="hash-link" aria-label="Direct link to Document loaders" title="Direct link to Document loaders">​</a></h3><span data-heading-keywords="document loader,document loaders"></span><p>These classes load Document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.</p><p>Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the <code>.load</code> method.
An example use case is as follows:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_community</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">document_loaders</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">csv_loader </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> CSVLoader</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">loader </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> CSVLoader</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">  </span><span class="token comment" style="color:rgb(0, 128, 0)"># &lt;-- Integration specific parameters here</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">data </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> loader</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">load</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/community/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html">CSVLoader</a></span></div><p>For specifics on how to use document loaders, see the <a href="/v0.2/docs/how_to/#document-loaders">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-splitters">Text splitters<a href="#text-splitters" class="hash-link" aria-label="Direct link to Text splitters" title="Direct link to Text splitters">​</a></h3><p>Once you&#x27;ve loaded documents, you&#x27;ll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model&#x27;s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.</p><p>When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What &quot;semantically related&quot; means could depend on the type of text. This notebook showcases several ways to do that.</p><p>At a high level, text splitters work as following:</p><ol><li>Split the text up into small, semantically meaningful chunks (often sentences).</li><li>Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).</li><li>Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).</li></ol><p>That means there are two different axes along which you can customize your text splitter:</p><ol><li>How the text is split</li><li>How the chunk size is measured</li></ol><p>For specifics on how to use text splitters, see the <a href="/v0.2/docs/how_to/#text-splitters">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="embedding-models">Embedding models<a href="#embedding-models" class="hash-link" aria-label="Direct link to Embedding models" title="Direct link to Embedding models">​</a></h3><span data-heading-keywords="embedding,embeddings"></span><p>Embedding models create a vector representation of a piece of text. You can think of a vector as an array of numbers that captures the semantic meaning of the text.
By representing the text in this way, you can perform mathematical operations that allow you to do things like search for other pieces of text that are most similar in meaning.
These natural language search capabilities underpin many types of <a href="/v0.2/docs/concepts/#retrieval">context retrieval</a>,
where we provide an LLM with the relevant data it needs to effectively respond to a query.</p><p><img loading="lazy" src="/v0.2/assets/images/embeddings-9c2616450a3b4f497a2d95a696b5f1a7.png" width="2300" height="1198" class="img_ev3q"></p><p>The <code>Embeddings</code> class is a class designed for interfacing with text embedding models. There are many different embedding model providers (OpenAI, Cohere, Hugging Face, etc) and local models, and this class is designed to provide a standard interface for all of them.</p><p>The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).</p><p>For specifics on how to use embedding models, see the <a href="/v0.2/docs/how_to/#embedding-models">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="vector-stores">Vector stores<a href="#vector-stores" class="hash-link" aria-label="Direct link to Vector stores" title="Direct link to Vector stores">​</a></h3><span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span><p>One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors,
and then at query time to embed the unstructured query and retrieve the embedding vectors that are &#x27;most similar&#x27; to the embedded query.
A vector store takes care of storing embedded data and performing vector search for you.</p><p>Most vector stores can also store metadata about embedded vectors and support filtering on that metadata before
similarity search, allowing you more control over returned documents.</p><p>Vector stores can be converted to the retriever interface by doing:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">vectorstore </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> MyVectorStore</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">retriever </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> vectorstore</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">as_retriever</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>For specifics on how to use vector stores, see the <a href="/v0.2/docs/how_to/#vector-stores">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="retrievers">Retrievers<a href="#retrievers" class="hash-link" aria-label="Direct link to Retrievers" title="Direct link to Retrievers">​</a></h3><span data-heading-keywords="retriever,retrievers"></span><p>A retriever is an interface that returns documents given an unstructured query.
It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) them.
Retrievers can be created from vector stores, but are also broad enough to include <a href="/v0.2/docs/integrations/retrievers/wikipedia/">Wikipedia search</a> and <a href="/v0.2/docs/integrations/retrievers/amazon_kendra_retriever/">Amazon Kendra</a>.</p><p>Retrievers accept a string query as input and return a list of Document&#x27;s as output.</p><p>For specifics on how to use retrievers, see the <a href="/v0.2/docs/how_to/#retrievers">relevant how-to guides here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-value-stores">Key-value stores<a href="#key-value-stores" class="hash-link" aria-label="Direct link to Key-value stores" title="Direct link to Key-value stores">​</a></h3><p>For some techniques, such as <a href="/v0.2/docs/how_to/multi_vector/">indexing and retrieval with multiple vectors per document</a> or
<a href="/v0.2/docs/how_to/caching_embeddings/">caching embeddings</a>, having a form of key-value (KV) storage is helpful.</p><p>LangChain includes a <a href="https://python.langchain.com/v0.2/api_reference/core/stores/langchain_core.stores.BaseStore.html" target="_blank" rel="noopener noreferrer"><code>BaseStore</code></a> interface,
which allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a
more specific <code>BaseStore[str, bytes]</code> instance that stores binary data (referred to as a <code>ByteStore</code>), and internally take care of
encoding and decoding data for their specific needs.</p><p>This means that as a user, you only need to think about one type of store rather than different ones for different types of data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="interface">Interface<a href="#interface" class="hash-link" aria-label="Direct link to Interface" title="Direct link to Interface">​</a></h4><p>All <a href="https://python.langchain.com/v0.2/api_reference/core/stores/langchain_core.stores.BaseStore.html" target="_blank" rel="noopener noreferrer"><code>BaseStores</code></a> support the following interface. Note that the interface allows
for modifying <strong>multiple</strong> key-value pairs at once:</p><ul><li><code>mget(key: Sequence[str]) -&gt; List[Optional[bytes]]</code>: get the contents of multiple keys, returning <code>None</code> if the key does not exist</li><li><code>mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -&gt; None</code>: set the contents of multiple keys</li><li><code>mdelete(key: Sequence[str]) -&gt; None</code>: delete multiple keys</li><li><code>yield_keys(prefix: Optional[str] = None) -&gt; Iterator[str]</code>: yield all keys in the store, optionally filtering by a prefix</li></ul><p>For key-value store implementations, see <a href="/v0.2/docs/integrations/stores/">this section</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tools">Tools<a href="#tools" class="hash-link" aria-label="Direct link to Tools" title="Direct link to Tools">​</a></h3><span data-heading-keywords="tool,tools"></span><p>Tools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.
Tools are needed whenever you want a model to control parts of your code or call out to external APIs.</p><p>A tool consists of:</p><ol><li>The <code>name</code> of the tool.</li><li>A <code>description</code> of what the tool does.</li><li>A <code>JSON schema</code> defining the inputs to the tool.</li><li>A <code>function</code> (and, optionally, an async variant of the function).</li></ol><p>When a tool is bound to a model, the name, description and JSON schema are provided as context to the model.
Given a list of tools and a set of instructions, a model can request to call one or more tools with specific inputs.
Typical usage may look like the following:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">tools </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"> </span><span class="token comment" style="color:rgb(0, 128, 0)"># Define a list of tools</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">llm_with_tools </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">bind_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">ai_msg </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> llm_with_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;do xyz...&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># -&gt; AIMessage(tool_calls=[ToolCall(...), ...], ...)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The <code>AIMessage</code> returned from the model MAY have <code>tool_calls</code> associated with it.
Read <a href="/v0.2/docs/concepts/#aimessage">this guide</a> for more information on what the response type may look like.</p><p>Once the chosen tools are invoked, the results can be passed back to the model so that it can complete whatever task
it&#x27;s performing.
There are generally two different ways to invoke the tool and pass back the response:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="invoke-with-just-the-arguments">Invoke with just the arguments<a href="#invoke-with-just-the-arguments" class="hash-link" aria-label="Direct link to Invoke with just the arguments" title="Direct link to Invoke with just the arguments">​</a></h4><p>When you invoke a tool with just the arguments, you will get back the raw tool output (usually a string).
This generally looks like:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token comment" style="color:rgb(0, 128, 0)"># You will want to previously check that the LLM returned tool calls</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tool_call </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ai_msg</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tool_calls</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># ToolCall(args={...}, id=..., ...)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tool_output </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> tool</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;args&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tool_message </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ToolMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">tool_output</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    tool_call_id</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;id&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    name</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;name&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Note that the <code>content</code> field will generally be passed back to the model.
If you do not want the raw tool response to be passed to the model, but you still want to keep it around,
you can transform the tool output but also pass it as an artifact (read more about <a href="/v0.2/docs/concepts/#toolmessage"><code>ToolMessage.artifact</code> here</a>)</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain"> </span><span class="token comment" style="color:rgb(0, 128, 0)"># Same code as above</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">response_for_llm </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> transform</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">response</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tool_message </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ToolMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">response_for_llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    tool_call_id</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;id&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    name</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;name&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    artifact</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">tool_output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="invoke-with-toolcall">Invoke with <code>ToolCall</code><a href="#invoke-with-toolcall" class="hash-link" aria-label="Direct link to invoke-with-toolcall" title="Direct link to invoke-with-toolcall">​</a></h4><p>The other way to invoke a tool is to call it with the full <code>ToolCall</code> that was generated by the model.
When you do this, the tool will return a ToolMessage.
The benefits of this are that you don&#x27;t have to write the logic yourself to transform the tool output into a ToolMessage.
This generally looks like:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">tool_call </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ai_msg</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tool_calls</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># -&gt; ToolCall(args={...}, id=..., ...)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tool_message </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> tool</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># -&gt; ToolMessage(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;tool result foobar...&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    tool_call_id</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    name</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;tool_name&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>If you are invoking the tool this way and want to include an <a href="/v0.2/docs/concepts/#toolmessage">artifact</a> for the ToolMessage, you will need to have the tool return two things.
Read more about <a href="/v0.2/docs/how_to/tool_artifacts/">defining tools that return artifacts here</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="best-practices">Best practices<a href="#best-practices" class="hash-link" aria-label="Direct link to Best practices" title="Direct link to Best practices">​</a></h4><p>When designing tools to be used by a model, it is important to keep in mind that:</p><ul><li>Chat models that have explicit <a href="/v0.2/docs/concepts/#functiontool-calling">tool-calling APIs</a> will be better at tool calling than non-fine-tuned models.</li><li>Models will perform better if the tools have well-chosen names, descriptions, and JSON schemas. This another form of prompt engineering.</li><li>Simple, narrowly scoped tools are easier for models to use than complex tools.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="related">Related<a href="#related" class="hash-link" aria-label="Direct link to Related" title="Direct link to Related">​</a></h4><p>For specifics on how to use tools, see the <a href="/v0.2/docs/how_to/#tools">tools how-to guides</a>.</p><p>To use a pre-built tool, see the <a href="/v0.2/docs/integrations/tools/">tool integration docs</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="toolkits">Toolkits<a href="#toolkits" class="hash-link" aria-label="Direct link to Toolkits" title="Direct link to Toolkits">​</a></h3><span data-heading-keywords="toolkit,toolkits"></span><p>Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.</p><p>All Toolkits expose a <code>get_tools</code> method which returns a list of tools.
You can therefore do:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token comment" style="color:rgb(0, 128, 0)"># Initialize a toolkit</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">toolkit </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ExampleTookit</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Get list of tools</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tools </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> toolkit</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">get_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agents">Agents<a href="#agents" class="hash-link" aria-label="Direct link to Agents" title="Direct link to Agents">​</a></h3><p>By themselves, language models can&#x27;t take actions - they just output text.
A big use case for LangChain is creating <strong>agents</strong>.
Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be.
The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.</p><p><a href="https://github.com/langchain-ai/langgraph" target="_blank" rel="noopener noreferrer">LangGraph</a> is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.
Please check out that documentation for a more in depth overview of agent concepts.</p><p>There is a legacy <code>agent</code> concept in LangChain that we are moving towards deprecating: <code>AgentExecutor</code>.
AgentExecutor was essentially a runtime for agents.
It was a great place to get started, however, it was not flexible enough as you started to have more customized agents.
In order to solve that we built LangGraph to be this flexible, highly-controllable runtime.</p><p>If you are still using AgentExecutor, do not fear: we still have a guide on <a href="/v0.2/docs/how_to/agent_executor/">how to use AgentExecutor</a>.
It is recommended, however, that you start to transition to LangGraph.
In order to assist in this, we have put together a <a href="/v0.2/docs/how_to/migrate_agent/">transition guide on how to do so</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="react-agents">ReAct agents<a href="#react-agents" class="hash-link" aria-label="Direct link to ReAct agents" title="Direct link to ReAct agents">​</a></h4><span data-heading-keywords="react,react agent"></span><p>One popular architecture for building agents is <a href="https://arxiv.org/abs/2210.03629" target="_blank" rel="noopener noreferrer"><strong>ReAct</strong></a>.
ReAct combines reasoning and acting in an iterative process - in fact the name &quot;ReAct&quot; stands for &quot;Reason&quot; and &quot;Act&quot;.</p><p>The general flow looks like this:</p><ul><li>The model will &quot;think&quot; about what step to take in response to an input and any previous observations.</li><li>The model will then choose an action from available tools (or choose to respond to the user).</li><li>The model will generate arguments to that tool.</li><li>The agent runtime (executor) will parse out the chosen tool and call it with the generated arguments.</li><li>The executor will return the results of the tool call back to the model as an observation.</li><li>This process repeats until the agent chooses to respond.</li></ul><p>There are general prompting based implementations that do not require any model-specific features, but the most
reliable implementations use features like <a href="/v0.2/docs/how_to/tool_calling/">tool calling</a> to reliably format outputs
and reduce variance.</p><p>Please see the <a href="https://langchain-ai.github.io/langgraph/" target="_blank" rel="noopener noreferrer">LangGraph documentation</a> for more information,
or <a href="/v0.2/docs/how_to/migrate_agent/">this how-to guide</a> for specific information on migrating to LangGraph.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="callbacks">Callbacks<a href="#callbacks" class="hash-link" aria-label="Direct link to Callbacks" title="Direct link to Callbacks">​</a></h3><p>LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.</p><p>You can subscribe to these events by using the <code>callbacks</code> argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="callback-events">Callback Events<a href="#callback-events" class="hash-link" aria-label="Direct link to Callback Events" title="Direct link to Callback Events">​</a></h4><table><thead><tr><th>Event</th><th>Event Trigger</th><th>Associated Method</th></tr></thead><tbody><tr><td>Chat model start</td><td>When a chat model starts</td><td><code>on_chat_model_start</code></td></tr><tr><td>LLM start</td><td>When a llm starts</td><td><code>on_llm_start</code></td></tr><tr><td>LLM new token</td><td>When an llm OR chat model emits a new token</td><td><code>on_llm_new_token</code></td></tr><tr><td>LLM ends</td><td>When an llm OR chat model ends</td><td><code>on_llm_end</code></td></tr><tr><td>LLM errors</td><td>When an llm OR chat model errors</td><td><code>on_llm_error</code></td></tr><tr><td>Chain start</td><td>When a chain starts running</td><td><code>on_chain_start</code></td></tr><tr><td>Chain end</td><td>When a chain ends</td><td><code>on_chain_end</code></td></tr><tr><td>Chain error</td><td>When a chain errors</td><td><code>on_chain_error</code></td></tr><tr><td>Tool start</td><td>When a tool starts running</td><td><code>on_tool_start</code></td></tr><tr><td>Tool end</td><td>When a tool ends</td><td><code>on_tool_end</code></td></tr><tr><td>Tool error</td><td>When a tool errors</td><td><code>on_tool_error</code></td></tr><tr><td>Agent action</td><td>When an agent takes an action</td><td><code>on_agent_action</code></td></tr><tr><td>Agent finish</td><td>When an agent ends</td><td><code>on_agent_finish</code></td></tr><tr><td>Retriever start</td><td>When a retriever starts</td><td><code>on_retriever_start</code></td></tr><tr><td>Retriever end</td><td>When a retriever ends</td><td><code>on_retriever_end</code></td></tr><tr><td>Retriever error</td><td>When a retriever errors</td><td><code>on_retriever_error</code></td></tr><tr><td>Text</td><td>When arbitrary text is run</td><td><code>on_text</code></td></tr><tr><td>Retry</td><td>When a retry event is run</td><td><code>on_retry</code></td></tr></tbody></table><h4 class="anchor anchorWithStickyNavbar_LWe7" id="callback-handlers">Callback handlers<a href="#callback-handlers" class="hash-link" aria-label="Direct link to Callback handlers" title="Direct link to Callback handlers">​</a></h4><p>Callback handlers can either be <code>sync</code> or <code>async</code>:</p><ul><li>Sync callback handlers implement the <a href="https://python.langchain.com/v0.2/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html" target="_blank" rel="noopener noreferrer">BaseCallbackHandler</a> interface.</li><li>Async callback handlers implement the <a href="https://python.langchain.com/v0.2/api_reference/core/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html" target="_blank" rel="noopener noreferrer">AsyncCallbackHandler</a> interface.</li></ul><p>During run-time LangChain configures an appropriate callback manager (e.g., <a href="https://python.langchain.com/v0.2/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManager.html" target="_blank" rel="noopener noreferrer">CallbackManager</a> or <a href="https://python.langchain.com/v0.2/api_reference/core/callbacks/langchain_core.callbacks.manager.AsyncCallbackManager.html" target="_blank" rel="noopener noreferrer">AsyncCallbackManager</a> which will be responsible for calling the appropriate method on each &quot;registered&quot; callback handler when the event is triggered.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="passing-callbacks">Passing callbacks<a href="#passing-callbacks" class="hash-link" aria-label="Direct link to Passing callbacks" title="Direct link to Passing callbacks">​</a></h4><p>The <code>callbacks</code> property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:</p><p>The callbacks are available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:</p><ul><li><strong>Request time callbacks</strong>: Passed at the time of the request in addition to the input data.
Available on all standard <code>Runnable</code> objects. These callbacks are INHERITED by all children
of the object they are defined on. For example, <code>chain.invoke({&quot;number&quot;: 25}, {&quot;callbacks&quot;: [handler]})</code>.</li><li><strong>Constructor callbacks</strong>: <code>chain = TheNameOfSomeChain(callbacks=[handler])</code>. These callbacks
are passed as arguments to the constructor of the object. The callbacks are scoped
only to the object they are defined on, and are <strong>not</strong> inherited by any children of the object.</li></ul><div class="theme-admonition theme-admonition-warning alert alert--danger admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_S0QG"><p>Constructor callbacks are scoped only to the object they are defined on. They are <strong>not</strong> inherited by children
of the object.</p></div></div><p>If you&#x27;re creating a custom chain or runnable, you need to remember to propagate request time
callbacks to any child objects.</p><div class="theme-admonition theme-admonition-important alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Async in Python&lt;=3.10</div><div class="admonitionContent_S0QG"><p>Any <code>RunnableLambda</code>, a <code>RunnableGenerator</code>, or <code>Tool</code> that invokes other runnables
and is running <code>async</code> in python&lt;=3.10, will have to propagate callbacks to child
objects manually. This is because LangChain cannot automatically propagate
callbacks to child objects in this case.</p><p>This is a common reason why you may fail to see events being emitted from custom
runnables or tools.</p></div></div><p>For specifics on how to use callbacks, see the <a href="/v0.2/docs/how_to/#callbacks">relevant how-to guides here</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="techniques">Techniques<a href="#techniques" class="hash-link" aria-label="Direct link to Techniques" title="Direct link to Techniques">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="streaming">Streaming<a href="#streaming" class="hash-link" aria-label="Direct link to Streaming" title="Direct link to Streaming">​</a></h3><span data-heading-keywords="stream,streaming"></span><p>Individual LLM calls often run for much longer than traditional resource requests.
This compounds when you build more complex chains or agents that require multiple reasoning steps.</p><p>Fortunately, LLMs generate output iteratively, which means it&#x27;s possible to show sensible intermediate results
before the final response is ready. Consuming output as soon as it becomes available has therefore become a vital part of the UX
around building apps with LLMs to help alleviate latency issues, and LangChain aims to have first-class support for streaming.</p><p>Below, we&#x27;ll discuss some concepts and considerations around streaming in LangChain.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="stream-and-astream"><code>.stream()</code> and <code>.astream()</code><a href="#stream-and-astream" class="hash-link" aria-label="Direct link to stream-and-astream" title="Direct link to stream-and-astream">​</a></h4><p>Most modules in LangChain include the <code>.stream()</code> method (and the equivalent <code>.astream()</code> method for <a href="https://docs.python.org/3/library/asyncio.html" target="_blank" rel="noopener noreferrer">async</a> environments) as an ergonomic streaming interface.
<code>.stream()</code> returns an iterator, which you can consume with a simple <code>for</code> loop. Here&#x27;s an example with a chat model:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_anthropic </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatAnthropic</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatAnthropic</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">model</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;claude-3-sonnet-20240229&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">for</span><span class="token plain"> chunk </span><span class="token keyword" style="color:rgb(0, 0, 255)">in</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">stream</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;what color is the sky?&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token keyword" style="color:rgb(0, 0, 255)">print</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">chunk</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">content</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> end</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;|&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> flush</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html">ChatAnthropic</a></span></div><p>For models (or other components) that don&#x27;t support streaming natively, this iterator would just yield a single chunk, but
you could still use the same general pattern when calling them. Using <code>.stream()</code> will also automatically call the model in streaming mode
without the need to provide additional config.</p><p>The type of each outputted chunk depends on the type of component - for example, chat models yield <a href="https://python.langchain.com/v0.2/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html" target="_blank" rel="noopener noreferrer"><code>AIMessageChunks</code></a>.
Because this method is part of <a href="/v0.2/docs/concepts/#langchain-expression-language-lcel">LangChain Expression Language</a>,
you can handle formatting differences from different outputs using an <a href="/v0.2/docs/concepts/#output-parsers">output parser</a> to transform
each yielded chunk.</p><p>You can check out <a href="/v0.2/docs/how_to/streaming/#using-stream">this guide</a> for more detail on how to use <code>.stream()</code>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="astream_events"><code>.astream_events()</code><a href="#astream_events" class="hash-link" aria-label="Direct link to astream_events" title="Direct link to astream_events">​</a></h4><span data-heading-keywords="astream_events,stream_events,stream events"></span><p>While the <code>.stream()</code> method is intuitive, it can only return the final generated value of your chain. This is fine for single LLM calls,
but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of
the chain alongside the final output - for example, returning sources alongside the final generation when building a chat
over documents app.</p><p>There are ways to do this <a href="/v0.2/docs/concepts/#callbacks-1">using callbacks</a>, or by constructing your chain in such a way that it passes intermediate
values to the end with something like chained <a href="/v0.2/docs/how_to/passthrough/"><code>.assign()</code></a> calls, but LangChain also includes an
<code>.astream_events()</code> method that combines the flexibility of callbacks with the ergonomics of <code>.stream()</code>. When called, it returns an iterator
which yields <a href="/v0.2/docs/how_to/streaming/#event-reference">various types of events</a> that you can filter and process according
to the needs of your project.</p><p>Here&#x27;s one small example that prints just events containing streamed chat model output:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">output_parsers </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> StrOutputParser</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">prompts </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatPromptTemplate</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_anthropic </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatAnthropic</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatAnthropic</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">model</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;claude-3-sonnet-20240229&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatPromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;tell me a joke about {topic}&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">parser </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> StrOutputParser</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chain </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">|</span><span class="token plain"> model </span><span class="token operator" style="color:rgb(0, 0, 0)">|</span><span class="token plain"> parser</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">async</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(0, 0, 255)">for</span><span class="token plain"> event </span><span class="token keyword" style="color:rgb(0, 0, 255)">in</span><span class="token plain"> chain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">astream_events</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;topic&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;parrot&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> version</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;v2&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    kind </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> event</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;event&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token keyword" style="color:rgb(0, 0, 255)">if</span><span class="token plain"> kind </span><span class="token operator" style="color:rgb(0, 0, 0)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;on_chat_model_stream&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">        </span><span class="token keyword" style="color:rgb(0, 0, 255)">print</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">event</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> end</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;|&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> flush</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html">StrOutputParser</a> | </span><span><a href="https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html">ChatPromptTemplate</a> | </span><span><a href="https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html">ChatAnthropic</a></span></div><p>You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!</p><p>See <a href="/v0.2/docs/how_to/streaming/#using-stream-events">this guide</a> for more detailed information on how to use <code>.astream_events()</code>,
including a table listing available events.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="callbacks-1">Callbacks<a href="#callbacks-1" class="hash-link" aria-label="Direct link to Callbacks" title="Direct link to Callbacks">​</a></h4><p>The lowest level way to stream outputs from LLMs in LangChain is via the <a href="/v0.2/docs/concepts/#callbacks">callbacks</a> system. You can pass a
callback handler that handles the <a href="https://python.langchain.com/v0.2/api_reference/langchain/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html#langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.on_llm_new_token" target="_blank" rel="noopener noreferrer"><code>on_llm_new_token</code></a> event into LangChain components. When that component is invoked, any
<a href="/v0.2/docs/concepts/#llms">LLM</a> or <a href="/v0.2/docs/concepts/#chat-models">chat model</a> contained in the component calls
the callback with the generated token. Within the callback, you could pipe the tokens into some other destination, e.g. a HTTP response.
You can also handle the <a href="https://python.langchain.com/v0.2/api_reference/langchain/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html#langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.on_llm_end" target="_blank" rel="noopener noreferrer"><code>on_llm_end</code></a> event to perform any necessary cleanup.</p><p>You can see <a href="/v0.2/docs/how_to/#callbacks">this how-to section</a> for more specifics on using callbacks.</p><p>Callbacks were the first technique for streaming introduced in LangChain. While powerful and generalizable,
they can be unwieldy for developers. For example:</p><ul><li>You need to explicitly initialize and manage some aggregator or other stream to collect results.</li><li>The execution order isn&#x27;t explicitly guaranteed, and you could theoretically have a callback run after the <code>.invoke()</code> method finishes.</li><li>Providers would often make you pass an additional parameter to stream outputs instead of returning them all at once.</li><li>You would often ignore the result of the actual model call in favor of callback results.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="tokens">Tokens<a href="#tokens" class="hash-link" aria-label="Direct link to Tokens" title="Direct link to Tokens">​</a></h4><p>The unit that most model providers use to measure input and output is via a unit called a <strong>token</strong>.
Tokens are the basic units that language models read and generate when processing or producing text.
The exact definition of a token can vary depending on the specific way the model was trained -
for instance, in English, a token could be a single word like &quot;apple&quot;, or a part of a word like &quot;app&quot;.</p><p>When you send a model a prompt, the words and characters in the prompt are encoded into tokens using a <strong>tokenizer</strong>.
The model then streams back generated output tokens, which the tokenizer decodes into human-readable text.
The below example shows how OpenAI models tokenize <code>LangChain is cool!</code>:</p><p><img loading="lazy" src="/v0.2/assets/images/tokenization-10f566ab6774724e63dd99646f69655c.png" width="1422" height="1084" class="img_ev3q"></p><p>You can see that it gets split into 5 different tokens, and that the boundaries between tokens are not exactly the same as word boundaries.</p><p>The reason language models use tokens rather than something more immediately intuitive like &quot;characters&quot;
has to do with how they process and understand text. At a high-level, language models iteratively predict their next generated output based on
the initial input and their previous generations. Training the model using tokens language models to handle linguistic
units (like words or subwords) that carry meaning, rather than individual characters, which makes it easier for the model
to learn and understand the structure of the language, including grammar and context.
Furthermore, using tokens can also improve efficiency, since the model processes fewer units of text compared to character-level processing.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="functiontool-calling">Function/tool calling<a href="#functiontool-calling" class="hash-link" aria-label="Direct link to Function/tool calling" title="Direct link to Function/tool calling">​</a></h3><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>We use the term <code>tool calling</code> interchangeably with <code>function calling</code>. Although
function calling is sometimes meant to refer to invocations of a single function,
we treat all models as though they can return multiple tool or function calls in
each message.</p></div></div><p>Tool calling allows a <a href="/v0.2/docs/concepts/#chat-models">chat model</a> to respond to a given prompt by generating output that
matches a user-defined schema.</p><p>While the name implies that the model is performing
some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.
One common example where you <strong>wouldn&#x27;t</strong> want to call a function with the generated arguments
is if you want to <a href="/v0.2/docs/concepts/#structured-output">extract structured output matching some schema</a>
from unstructured text. You would give the model an &quot;extraction&quot; tool that takes
parameters matching the desired schema, then treat the generated output as your final
result.</p><p><img loading="lazy" alt="Diagram of a tool call by a chat model" src="/v0.2/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png" width="1684" height="432" class="img_ev3q"></p><p>Tool calling is not universal, but is supported by many popular LLM providers, including <a href="/v0.2/docs/integrations/chat/anthropic/">Anthropic</a>,
<a href="/v0.2/docs/integrations/chat/cohere/">Cohere</a>, <a href="/v0.2/docs/integrations/chat/google_vertex_ai_palm/">Google</a>,
<a href="/v0.2/docs/integrations/chat/mistralai/">Mistral</a>, <a href="/v0.2/docs/integrations/chat/openai/">OpenAI</a>, and even for locally-running models via <a href="/v0.2/docs/integrations/chat/ollama/">Ollama</a>.</p><p>LangChain provides a standardized interface for tool calling that is consistent across different models.</p><p>The standard interface consists of:</p><ul><li><code>ChatModel.bind_tools()</code>: a method for specifying which tools are available for a model to call. This method accepts <a href="/v0.2/docs/concepts/#tools">LangChain tools</a> as well as <a href="https://pydantic.dev/" target="_blank" rel="noopener noreferrer">Pydantic</a> objects.</li><li><code>AIMessage.tool_calls</code>: an attribute on the <code>AIMessage</code> returned from the model for accessing the tool calls requested by the model.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="tool-usage">Tool usage<a href="#tool-usage" class="hash-link" aria-label="Direct link to Tool usage" title="Direct link to Tool usage">​</a></h4><p>After the model calls tools, you can use the tool by invoking it, then passing the arguments back to the model.
LangChain provides the <a href="/v0.2/docs/concepts/#tools"><code>Tool</code></a> abstraction to help you handle this.</p><p>The general flow is this:</p><ol><li>Generate tool calls with a chat model in response to a query.</li><li>Invoke the appropriate tools using the generated tool call as arguments.</li><li>Format the result of the tool invocations as <a href="/v0.2/docs/concepts/#toolmessage"><code>ToolMessages</code></a>.</li><li>Pass the entire list of messages back to the model so that it can generate a final answer (or call more tools).</li></ol><p><img loading="lazy" alt="Diagram of a complete tool calling flow" src="/v0.2/assets/images/tool_calling_flow-ead8d93a8b69c88e3076457ed28f41ae.png" width="2612" height="744" class="img_ev3q"></p><p>This is how tool calling <a href="/v0.2/docs/concepts/#agents">agents</a> perform tasks and answer queries.</p><p>Check out some more focused guides below:</p><ul><li><a href="/v0.2/docs/how_to/tool_calling/">How to use chat models to call tools</a></li><li><a href="/v0.2/docs/how_to/tool_results_pass_to_model/">How to pass tool outputs to chat models</a></li><li><a href="https://langchain-ai.github.io/langgraph/tutorials/introduction/" target="_blank" rel="noopener noreferrer">Building an agent with LangGraph</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="structured-output">Structured output<a href="#structured-output" class="hash-link" aria-label="Direct link to Structured output" title="Direct link to Structured output">​</a></h3><p>LLMs are capable of generating arbitrary text. This enables the model to respond appropriately to a wide
range of inputs, but for some use-cases, it can be useful to constrain the LLM&#x27;s output
to a specific format or structure. This is referred to as <strong>structured output</strong>.</p><p>For example, if the output is to be stored in a relational database,
it is much easier if the model generates output that adheres to a defined schema or format.
<a href="/v0.2/docs/tutorials/extraction/">Extracting specific information</a> from unstructured text is another
case where this is particularly useful. Most commonly, the output format will be JSON,
though other formats such as <a href="/v0.2/docs/how_to/output_parser_yaml/">YAML</a> can be useful too. Below, we&#x27;ll discuss
a few ways to get structured output from models in LangChain.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="with_structured_output"><code>.with_structured_output()</code><a href="#with_structured_output" class="hash-link" aria-label="Direct link to with_structured_output" title="Direct link to with_structured_output">​</a></h4><p>For convenience, some LangChain chat models support a <a href="/v0.2/docs/how_to/structured_output/#the-with_structured_output-method"><code>.with_structured_output()</code></a>
method. This method only requires a schema as input, and returns a dict or Pydantic object.
Generally, this method is only present on models that support one of the more advanced methods described below,
and will use one of them under the hood. It takes care of importing a suitable output parser and
formatting the schema in the right format for the model.</p><p>Here&#x27;s an example:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> typing </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> Optional</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">pydantic_v1 </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> BaseModel</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> Field</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">class</span><span class="token plain"> </span><span class="token class-name" style="color:rgb(38, 127, 153)">Joke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">BaseModel</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">&quot;&quot;&quot;Joke to tell user.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    setup</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(0, 112, 193)">str</span><span class="token plain"> </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> Field</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">description</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;The setup of the joke&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    punchline</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(0, 112, 193)">str</span><span class="token plain"> </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> Field</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">description</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;The punchline to the joke&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    rating</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> Optional</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token builtin" style="color:rgb(0, 112, 193)">int</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> Field</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">description</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;How funny the joke is, from 1 to 10&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">structured_llm </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">with_structured_output</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">Joke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">structured_llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;Tell me a joke about cats&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">Joke(setup=&#x27;Why was the cat sitting on the computer?&#x27;, punchline=&#x27;To keep an eye on the mouse!&#x27;, rating=None)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We recommend this method as a starting point when working with structured output:</p><ul><li>It uses other model-specific features under the hood, without the need to import an output parser.</li><li>For the models that use tool calling, no special prompting is needed.</li><li>If multiple underlying techniques are supported, you can supply a <code>method</code> parameter to
<a href="/v0.2/docs/how_to/structured_output/#advanced-specifying-the-method-for-structuring-outputs">toggle which one is used</a>.</li></ul><p>You may want or need to use other techniques if:</p><ul><li>The chat model you are using does not support tool calling.</li><li>You are working with very complex schemas and the model is having trouble generating outputs that conform.</li></ul><p>For more information, check out this <a href="/v0.2/docs/how_to/structured_output/#the-with_structured_output-method">how-to guide</a>.</p><p>You can also check out <a href="/v0.2/docs/integrations/chat/#advanced-features">this table</a> for a list of models that support
<code>with_structured_output()</code>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="raw-prompting">Raw prompting<a href="#raw-prompting" class="hash-link" aria-label="Direct link to Raw prompting" title="Direct link to Raw prompting">​</a></h4><p>The most intuitive way to get a model to structure output is to ask nicely.
In addition to your query, you can give instructions describing what kind of output you&#x27;d like, then
parse the output using an <a href="/v0.2/docs/concepts/#output-parsers">output parser</a> to convert the raw
model message or string output into something more easily manipulated.</p><p>The biggest benefit to raw prompting is its flexibility:</p><ul><li>Raw prompting does not require any special model features, only sufficient reasoning capability to understand
the passed schema.</li><li>You can prompt for any format you&#x27;d like, not just JSON. This can be useful if the model you
are using is more heavily trained on a certain type of data, such as XML or YAML.</li></ul><p>However, there are some drawbacks too:</p><ul><li>LLMs are non-deterministic, and prompting a LLM to consistently output data in the exactly correct format
for smooth parsing can be surprisingly difficult and model-specific.</li><li>Individual models have quirks depending on the data they were trained on, and optimizing prompts can be quite difficult.
Some may be better at interpreting <a href="https://json-schema.org/" target="_blank" rel="noopener noreferrer">JSON schema</a>, others may be best with TypeScript definitions,
and still others may prefer XML.</li></ul><p>While features offered by model providers may increase reliability, prompting techniques remain important for tuning your
results no matter which method you choose.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="json-mode">JSON mode<a href="#json-mode" class="hash-link" aria-label="Direct link to JSON mode" title="Direct link to JSON mode">​</a></h4><span data-heading-keywords="json mode"></span><p>Some models, such as <a href="/v0.2/docs/integrations/chat/mistralai/">Mistral</a>, <a href="/v0.2/docs/integrations/chat/openai/">OpenAI</a>,
<a href="/v0.2/docs/integrations/chat/together/">Together AI</a> and <a href="/v0.2/docs/integrations/chat/ollama/">Ollama</a>,
support a feature called <strong>JSON mode</strong>, usually enabled via config.</p><p>When enabled, JSON mode will constrain the model&#x27;s output to always be some sort of valid JSON.
Often they require some custom prompting, but it&#x27;s usually much less burdensome than completely raw prompting and
more along the lines of, <code>&quot;you must always return JSON&quot;</code>. The <a href="/v0.2/docs/how_to/output_parser_json/">output also generally easier to parse</a>.</p><p>It&#x27;s also generally simpler to use directly and more commonly available than tool calling, and can give
more flexibility around prompting and shaping results than tool calling.</p><p>Here&#x27;s an example:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">prompts </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatPromptTemplate</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_openai </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatOpenAI</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">output_parsers</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">json </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> SimpleJsonOutputParser</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatOpenAI</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    model</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;gpt-4o&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    model_kwargs</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;response_format&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;type&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;json_object&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatPromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;Answer the user&#x27;s question to the best of your ability.&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token string" style="color:rgb(163, 21, 21)">&#x27;You must always output a JSON object with an &quot;answer&quot; key and a &quot;followup_question&quot; key.&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;{question}&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chain </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">|</span><span class="token plain"> model </span><span class="token operator" style="color:rgb(0, 0, 0)">|</span><span class="token plain"> SimpleJsonOutputParser</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;question&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;What is the powerhouse of the cell?&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html">ChatPromptTemplate</a> | </span><span><a href="https://python.langchain.com/v0.2/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html">ChatOpenAI</a> | </span><span><a href="https://python.langchain.com/v0.2/api_reference/core/output_parsers/langchain_core.output_parsers.json.SimpleJsonOutputParser.html">SimpleJsonOutputParser</a></span></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">{&#x27;answer&#x27;: &#x27;The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.&#x27;,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"> &#x27;followup_question&#x27;: &#x27;Would you like to know more about how mitochondria produce energy?&#x27;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>For a full list of model providers that support JSON mode, see <a href="/v0.2/docs/integrations/chat/#advanced-features">this table</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="structured-output-tool-calling">Tool calling<a href="#structured-output-tool-calling" class="hash-link" aria-label="Direct link to Tool calling" title="Direct link to Tool calling">​</a></h4><p>For models that support it, <a href="/v0.2/docs/concepts/#functiontool-calling">tool calling</a> can be very convenient for structured output. It removes the
guesswork around how best to prompt schemas in favor of a built-in model feature.</p><p>It works by first binding the desired schema either directly or via a <a href="/v0.2/docs/concepts/#tools">LangChain tool</a> to a
<a href="/v0.2/docs/concepts/#chat-models">chat model</a> using the <code>.bind_tools()</code> method. The model will then generate an <code>AIMessage</code> containing
a <code>tool_calls</code> field containing <code>args</code> that match the desired shape.</p><p>There are several acceptable formats you can use to bind tools to a model in LangChain. Here&#x27;s one example:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">pydantic_v1 </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> BaseModel</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> Field</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_openai </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatOpenAI</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">class</span><span class="token plain"> </span><span class="token class-name" style="color:rgb(38, 127, 153)">ResponseFormatter</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">BaseModel</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">&quot;&quot;&quot;Always use this tool to structure your response to the user.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    answer</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(0, 112, 193)">str</span><span class="token plain"> </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> Field</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">description</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;The answer to the user&#x27;s question&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    followup_question</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(0, 112, 193)">str</span><span class="token plain"> </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> Field</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">description</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;A followup question the user could ask&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatOpenAI</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    model</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;gpt-4o&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    temperature</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model_with_tools </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">bind_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">ResponseFormatter</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">ai_msg </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> model_with_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;What is the powerhouse of the cell?&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">ai_msg</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tool_calls</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;args&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/v0.2/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html">ChatOpenAI</a></span></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">{&#x27;answer&#x27;: &quot;The powerhouse of the cell is the mitochondrion. It generates most of the cell&#x27;s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.&quot;,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"> &#x27;followup_question&#x27;: &#x27;How do mitochondria generate ATP?&#x27;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Tool calling is a generally consistent way to get a model to generate structured output, and is the default technique
used for the <a href="/v0.2/docs/concepts/#with_structured_output"><code>.with_structured_output()</code></a> method when a model supports it.</p><p>The following how-to guides are good practical resources for using function/tool calling for structured output:</p><ul><li><a href="/v0.2/docs/how_to/structured_output/">How to return structured data from an LLM</a></li><li><a href="/v0.2/docs/how_to/tool_calling/">How to use a model to call tools</a></li></ul><p>For a full list of model providers that support tool calling, <a href="/v0.2/docs/integrations/chat/#advanced-features">see this table</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="few-shot-prompting">Few-shot prompting<a href="#few-shot-prompting" class="hash-link" aria-label="Direct link to Few-shot prompting" title="Direct link to Few-shot prompting">​</a></h3><p>One of the most effective ways to improve model performance is to give a model examples of
what you want it to do. The technique of adding example inputs and expected outputs
to a model prompt is known as &quot;few-shot prompting&quot;. The technique is based on the
<a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">Language Models are Few-Shot Learners</a> paper.
There are a few things to think about when doing few-shot prompting:</p><ol><li>How are examples generated?</li><li>How many examples are in each prompt?</li><li>How are examples selected at runtime?</li><li>How are examples formatted in the prompt?</li></ol><p>Here are the considerations for each.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-generating-examples">1. Generating examples<a href="#1-generating-examples" class="hash-link" aria-label="Direct link to 1. Generating examples" title="Direct link to 1. Generating examples">​</a></h4><p>The first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.</p><p>At a high-level, the basic ways to generate examples are:</p><ul><li>Manual: a person/people generates examples they think are useful.</li><li>Better model: a better (presumably more expensive/slower) model&#x27;s responses are used as examples for a worse (presumably cheaper/faster) model.</li><li>User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).</li><li>LLM feedback: same as user feedback but the process is automated by having models evaluate themselves.</li></ul><p>Which approach is best depends on your task. For tasks where a small number core principles need to be understood really well, it can be valuable hand-craft a few really good examples.
For tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there&#x27;s a higher likelihood of there being some highly relevant examples for any runtime input.</p><p><strong>Single-turn v.s. multi-turn examples</strong></p><p>Another dimension to think about when generating examples is what the example is actually showing.</p><p>The simplest types of examples just have a user input and an expected model output. These are single-turn examples.</p><p>One more complex type if example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer.
This is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where its useful to show common errors and spell out exactly why they&#x27;re wrong and what should be done instead.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-number-of-examples">2. Number of examples<a href="#2-number-of-examples" class="hash-link" aria-label="Direct link to 2. Number of examples" title="Direct link to 2. Number of examples">​</a></h4><p>Once we have a dataset of examples, we need to think about how many examples should be in each prompt.
The key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency.
And beyond some threshold having too many examples can start to confuse the model.
Finding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints.
Anecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples.
But, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-selecting-examples">3. Selecting examples<a href="#3-selecting-examples" class="hash-link" aria-label="Direct link to 3. Selecting examples" title="Direct link to 3. Selecting examples">​</a></h4><p>Assuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:</p><ul><li>Randomly</li><li>By (semantic or keyword-based) similarity of the inputs</li><li>Based on some other constraints, like token size</li></ul><p>LangChain has a number of <a href="/v0.2/docs/concepts/#example-selectors"><code>ExampleSelectors</code></a> which make it easy to use any of these techniques.</p><p>Generally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="4-formatting-examples">4. Formatting examples<a href="#4-formatting-examples" class="hash-link" aria-label="Direct link to 4. Formatting examples" title="Direct link to 4. Formatting examples">​</a></h4><p>Most state-of-the-art models these days are chat models, so we&#x27;ll focus on formatting examples for those. Our basic options are to insert the examples:</p><ul><li>In the system prompt as a string</li><li>As their own messages</li></ul><p>If we insert our examples into the system prompt as a string, we&#x27;ll need to make sure it&#x27;s clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language" target="_blank" rel="noopener noreferrer">ChatML</a>, XML, TypeScript, etc.</p><p>If we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign <a href="/v0.2/docs/concepts/#messages">names</a> to our messages like <code>&quot;example_user&quot;</code> and <code>&quot;example_assistant&quot;</code> to make it clear that these messages correspond to different actors than the latest input message.</p><p><strong>Formatting tool call examples</strong></p><p>One area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.</p><ul><li>Some models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call,</li><li>Some models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage,</li><li>Some models require that tools are passed in to the model if there are any tool calls / ToolMessages in the chat history.</li></ul><p>These requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints.
In these cases it&#x27;s especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.</p><p>You can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks <a href="https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/" target="_blank" rel="noopener noreferrer">here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="retrieval">Retrieval<a href="#retrieval" class="hash-link" aria-label="Direct link to Retrieval" title="Direct link to Retrieval">​</a></h3><p>LLMs are trained on a large but fixed dataset, limiting their ability to reason over private or recent information.
Fine-tuning an LLM with specific facts is one way to mitigate this, but is often <a href="https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts" target="_blank" rel="noopener noreferrer">poorly suited for factual recall</a> and <a href="https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise" target="_blank" rel="noopener noreferrer">can be costly</a>.
<code>Retrieval</code> is the process of providing relevant information to an LLM to improve its response for a given input.
<code>Retrieval augmented generation</code> (<code>RAG</code>) <a href="https://arxiv.org/abs/2005.11401" target="_blank" rel="noopener noreferrer">paper</a> is the process of grounding the LLM generation (output) using the retrieved information.</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><ul><li>See our RAG from Scratch <a href="https://github.com/langchain-ai/rag-from-scratch" target="_blank" rel="noopener noreferrer">code</a> and <a href="https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&amp;feature=shared" target="_blank" rel="noopener noreferrer">video series</a>.</li><li>For a high-level guide on retrieval, see this <a href="/v0.2/docs/tutorials/rag/">tutorial on RAG</a>.</li></ul></div></div><p>RAG is only as good as the retrieved documents’ relevance and quality. Fortunately, an emerging set of techniques can be employed to design and improve RAG systems. We&#x27;ve focused on taxonomizing and summarizing many of these techniques (see below figure) and will share some high-level strategic guidance in the following sections.
You can and should experiment with using different pieces together. You might also find <a href="https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application" target="_blank" rel="noopener noreferrer">this LangSmith guide</a> useful for showing how to evaluate different iterations of your app.</p><p><img loading="lazy" src="/v0.2/assets/images/rag_landscape-627f1d0fd46b92bc2db0af8f99ec3724.png" width="1586" height="1202" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="query-translation">Query Translation<a href="#query-translation" class="hash-link" aria-label="Direct link to Query Translation" title="Direct link to Query Translation">​</a></h4><p>First, consider the user input(s) to your RAG system. Ideally, a RAG system can handle a wide range of inputs, from poorly worded questions to complex multi-part queries.
<strong>Using an LLM to review and optionally modify the input is the central idea behind query translation.</strong> This serves as a general buffer, optimizing raw user inputs for your retrieval system.
For example, this can be as simple as extracting keywords or as complex as generating multiple sub-questions for a complex query.</p><table><thead><tr><th>Name</th><th>When to use</th><th>Description</th></tr></thead><tbody><tr><td><a href="/v0.2/docs/how_to/MultiQueryRetriever/">Multi-query</a></td><td>When you need to cover multiple perspectives of a question.</td><td>Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, return the unique documents for all queries.</td></tr><tr><td><a href="https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb" target="_blank" rel="noopener noreferrer">Decomposition</a></td><td>When a question can be broken down into smaller subproblems.</td><td>Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).</td></tr><tr><td><a href="https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb" target="_blank" rel="noopener noreferrer">Step-back</a></td><td>When a higher-level conceptual understanding is required.</td><td>First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. <a href="https://arxiv.org/pdf/2310.06117" target="_blank" rel="noopener noreferrer">Paper</a>.</td></tr><tr><td><a href="https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb" target="_blank" rel="noopener noreferrer">HyDE</a></td><td>If you have challenges retrieving relevant documents using the raw user inputs.</td><td>Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. <a href="https://arxiv.org/abs/2212.10496" target="_blank" rel="noopener noreferrer">Paper</a>.</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>See our RAG from Scratch videos for a few different specific approaches:</p><ul><li><a href="https://youtu.be/JChPi0CRnDY?feature=shared" target="_blank" rel="noopener noreferrer">Multi-query</a></li><li><a href="https://youtu.be/h0OPWlEOank?feature=shared" target="_blank" rel="noopener noreferrer">Decomposition</a></li><li><a href="https://youtu.be/xn1jEjRyJ2U?feature=shared" target="_blank" rel="noopener noreferrer">Step-back</a></li><li><a href="https://youtu.be/SaDzIVkYqyY?feature=shared" target="_blank" rel="noopener noreferrer">HyDE</a></li></ul></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="routing">Routing<a href="#routing" class="hash-link" aria-label="Direct link to Routing" title="Direct link to Routing">​</a></h4><p>Second, consider the data sources available to your RAG system. You want to query across more than one database or across structured and unstructured data sources. <strong>Using an LLM to review the input and route it to the appropriate data source is a simple and effective approach for querying across sources.</strong></p><table><thead><tr><th>Name</th><th>When to use</th><th>Description</th></tr></thead><tbody><tr><td><a href="/v0.2/docs/how_to/routing/">Logical routing</a></td><td>When you can prompt an LLM with rules to decide where to route the input.</td><td>Logical routing can use an LLM to reason about the query and choose which datastore is most appropriate.</td></tr><tr><td><a href="/v0.2/docs/how_to/routing/#routing-by-semantic-similarity">Semantic routing</a></td><td>When semantic similarity is an effective way to determine where to route the input.</td><td>Semantic routing embeds both query and, typically a set of prompts. It then chooses the appropriate prompt based upon similarity.</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>See our RAG from Scratch video on <a href="https://youtu.be/pfpIndq7Fi8?feature=shared" target="_blank" rel="noopener noreferrer">routing</a>.  </p></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="query-construction">Query Construction<a href="#query-construction" class="hash-link" aria-label="Direct link to Query Construction" title="Direct link to Query Construction">​</a></h4><p>Third, consider whether any of your data sources require specific query formats. Many structured databases use SQL. Vector stores often have specific syntax for applying keyword filters to document metadata. <strong>Using an LLM to convert a natural language query into a query syntax is a popular and powerful approach.</strong>
In particular, <a href="/v0.2/docs/tutorials/sql_qa/">text-to-SQL</a>, <a href="/v0.2/docs/tutorials/graph/">text-to-Cypher</a>, and <a href="/v0.2/docs/tutorials/query_analysis/#query-analysis">query analysis for metadata filters</a> are useful ways to interact with structured, graph, and vector databases respectively. </p><table><thead><tr><th>Name</th><th>When to Use</th><th>Description</th></tr></thead><tbody><tr><td><a href="/v0.2/docs/tutorials/sql_qa/">Text to SQL</a></td><td>If users are asking questions that require information housed in a relational database, accessible via SQL.</td><td>This uses an LLM to transform user input into a SQL query.</td></tr><tr><td><a href="/v0.2/docs/tutorials/graph/">Text-to-Cypher</a></td><td>If users are asking questions that require information housed in a graph database, accessible via Cypher.</td><td>This uses an LLM to transform user input into a Cypher query.</td></tr><tr><td><a href="/v0.2/docs/how_to/self_query/">Self Query</a></td><td>If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.</td><td>This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>See our <a href="https://blog.langchain.dev/query-construction/" target="_blank" rel="noopener noreferrer">blog post overview</a> and RAG from Scratch video on <a href="https://youtu.be/kl6NwWYxvbM?feature=shared" target="_blank" rel="noopener noreferrer">query construction</a>, the process of text-to-DSL where DSL is a domain specific language required to interact with a given database. This converts user questions into structured queries. </p></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="indexing">Indexing<a href="#indexing" class="hash-link" aria-label="Direct link to Indexing" title="Direct link to Indexing">​</a></h4><p>Fourth, consider the design of your document index. A simple and powerful idea is to <strong>decouple the documents that you index for retrieval from the documents that you pass to the LLM for generation.</strong> Indexing frequently uses embedding models with vector stores, which <a href="/v0.2/docs/concepts/#embedding-models">compress the semantic information in documents to fixed-size vectors</a>.</p><p>Many RAG approaches focus on splitting documents into chunks and retrieving some number based on similarity to an input question for the LLM. But chunk size and chunk number can be difficult to set and affect results if they do not provide full context for the LLM to answer a question. Furthermore, LLMs are increasingly capable of processing millions of tokens. </p><p>Two approaches can address this tension: (1) <a href="/v0.2/docs/how_to/multi_vector/">Multi Vector</a> retriever using an LLM to translate documents into any form (e.g., often into a summary) that is well-suited for indexing, but returns full documents to the LLM for generation. (2) <a href="/v0.2/docs/how_to/parent_document_retriever/">ParentDocument</a> retriever embeds document chunks, but also returns full documents. The idea is to get the best of both worlds: use concise representations (summaries or chunks) for retrieval, but use the full documents for answer generation.</p><table><thead><tr><th>Name</th><th>Index Type</th><th>Uses an LLM</th><th>When to Use</th><th>Description</th></tr></thead><tbody><tr><td><a href="/v0.2/docs/how_to/vectorstore_retriever/">Vector store</a></td><td>Vector store</td><td>No</td><td>If you are just getting started and looking for something quick and easy.</td><td>This is the simplest method and the one that is easiest to get started with. It involves creating embeddings for each piece of text.</td></tr><tr><td><a href="/v0.2/docs/how_to/parent_document_retriever/">ParentDocument</a></td><td>Vector store + Document Store</td><td>No</td><td>If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.</td><td>This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).</td></tr><tr><td><a href="/v0.2/docs/how_to/multi_vector/">Multi Vector</a></td><td>Vector store + Document Store</td><td>Sometimes during indexing</td><td>If you are able to extract information from documents that you think is more relevant to index than the text itself.</td><td>This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.</td></tr><tr><td><a href="/v0.2/docs/how_to/time_weighted_vectorstore/">Time-Weighted Vector store</a></td><td>Vector store</td><td>No</td><td>If you have timestamps associated with your documents, and you want to retrieve the most recent ones</td><td>This fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents)</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><ul><li>See our RAG from Scratch video on <a href="https://youtu.be/bjb_EMsTDKI?feature=shared" target="_blank" rel="noopener noreferrer">indexing fundamentals</a></li><li>See our RAG from Scratch video on <a href="https://youtu.be/gTCU9I6QqCE?feature=shared" target="_blank" rel="noopener noreferrer">multi vector retriever</a></li></ul></div></div><p>Fifth, consider ways to improve the quality of your similarity search itself. Embedding models compress text into fixed-length (vector) representations that capture the semantic content of the document. This compression is useful for search / retrieval, but puts a heavy burden on that single vector representation to capture the semantic nuance / detail of the document. In some cases, irrelevant or redundant content can dilute the semantic usefulness of the embedding.</p><p><a href="https://docs.google.com/presentation/d/1IRhAdGjIevrrotdplHNcc4aXgIYyKamUKTWtB3m3aMU/edit?usp=sharing" target="_blank" rel="noopener noreferrer">ColBERT</a> is an interesting approach to address this with a higher granularity embeddings: (1) produce a contextually influenced embedding for each token in the document and query, (2) score similarity between each query token and all document tokens, (3) take the max, (4) do this for all query tokens, and (5) take the sum of the max scores (in step 3) for all query tokens to get a query-document similarity score; this token-wise scoring can yield strong results. </p><p><img loading="lazy" src="/v0.2/assets/images/colbert-0bf5bd7485724d0005a2f5bdadbdaedb.png" width="1236" height="695" class="img_ev3q"></p><p>There are some additional tricks to improve the quality of your retrieval. Embeddings excel at capturing semantic information, but may struggle with keyword-based queries. Many <a href="/v0.2/docs/integrations/retrievers/pinecone_hybrid_search/">vector stores</a> offer built-in <a href="https://docs.pinecone.io/guides/data/understanding-hybrid-search" target="_blank" rel="noopener noreferrer">hybrid-search</a> to combine keyword and semantic similarity, which marries the benefits of both approaches. Furthermore, many vector stores have <a href="https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/" target="_blank" rel="noopener noreferrer">maximal marginal relevance</a>, which attempts to diversify the results of a search to avoid returning similar and redundant documents. </p><table><thead><tr><th>Name</th><th>When to use</th><th>Description</th></tr></thead><tbody><tr><td><a href="/v0.2/docs/integrations/providers/ragatouille/#using-colbert-as-a-reranker">ColBERT</a></td><td>When higher granularity embeddings are needed.</td><td>ColBERT uses contextually influenced embeddings for each token in the document and query to get a granular query-document similarity score. <a href="https://arxiv.org/abs/2112.01488" target="_blank" rel="noopener noreferrer">Paper</a>.</td></tr><tr><td><a href="/v0.2/docs/integrations/retrievers/pinecone_hybrid_search/">Hybrid search</a></td><td>When combining keyword-based and semantic similarity.</td><td>Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. <a href="https://arxiv.org/abs/2210.11934" target="_blank" rel="noopener noreferrer">Paper</a>.</td></tr><tr><td><a href="/v0.2/docs/integrations/vectorstores/pinecone/#maximal-marginal-relevance-searches">Maximal Marginal Relevance (MMR)</a></td><td>When needing to diversify search results.</td><td>MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>See our RAG from Scratch video on <a href="https://youtu.be/cN6S0Ehm7_8?feature=shared%3E" target="_blank" rel="noopener noreferrer">ColBERT</a>.</p></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="post-processing">Post-processing<a href="#post-processing" class="hash-link" aria-label="Direct link to Post-processing" title="Direct link to Post-processing">​</a></h4><p>Sixth, consider ways to filter or rank retrieved documents. This is very useful if you are <a href="/v0.2/docs/integrations/retrievers/cohere-reranker/#doing-reranking-with-coherererank">combining documents returned from multiple sources</a>, since it can can down-rank less relevant documents and / or <a href="/v0.2/docs/how_to/contextual_compression/#more-built-in-compressors-filters">compress similar documents</a>. </p><table><thead><tr><th>Name</th><th>Index Type</th><th>Uses an LLM</th><th>When to Use</th><th>Description</th></tr></thead><tbody><tr><td><a href="/v0.2/docs/how_to/contextual_compression/">Contextual Compression</a></td><td>Any</td><td>Sometimes</td><td>If you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.</td><td>This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.</td></tr><tr><td><a href="/v0.2/docs/how_to/ensemble_retriever/">Ensemble</a></td><td>Any</td><td>No</td><td>If you have multiple retrieval methods and want to try combining them.</td><td>This fetches documents from multiple retrievers and then combines them.</td></tr><tr><td><a href="/v0.2/docs/integrations/retrievers/cohere-reranker/">Re-ranking</a></td><td>Any</td><td>Yes</td><td>If you want to rank retrieved documents based upon relevance, especially if you want to combine results from multiple retrieval methods .</td><td>Given a query and a list of documents, Rerank indexes the documents from most to least semantically relevant to the query.</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>See our RAG from Scratch video on <a href="https://youtu.be/77qELPbNgxA?feature=shared" target="_blank" rel="noopener noreferrer">RAG-Fusion</a> (<a href="https://arxiv.org/abs/2402.03367" target="_blank" rel="noopener noreferrer">paper</a>), on approach for post-processing across multiple queries:  Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, and combine the ranks of multiple search result lists to produce a single, unified ranking with <a href="https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1" target="_blank" rel="noopener noreferrer">Reciprocal Rank Fusion (RRF)</a>.</p></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="generation">Generation<a href="#generation" class="hash-link" aria-label="Direct link to Generation" title="Direct link to Generation">​</a></h4><p><strong>Finally, consider ways to build self-correction into your RAG system.</strong> RAG systems can suffer from low quality retrieval (e.g., if a user question is out of the domain for the index) and / or hallucinations in generation. A naive retrieve-generate pipeline has no ability to detect or self-correct from these kinds of errors. The concept of <a href="https://x.com/karpathy/status/1748043513156272416" target="_blank" rel="noopener noreferrer">&quot;flow engineering&quot;</a> has been introduced <a href="https://arxiv.org/abs/2401.08500" target="_blank" rel="noopener noreferrer">in the context of code generation</a>: iteratively build an answer to a code question with unit tests to check and self-correct errors. Several works have applied this RAG, such as Self-RAG and Corrective-RAG. In both cases, checks for document relevance, hallucinations, and / or answer quality are performed in the RAG answer generation flow.</p><p>We&#x27;ve found that graphs are a great way to reliably express logical flows and have implemented ideas from several of these papers <a href="https://github.com/langchain-ai/langgraph/tree/main/examples/rag" target="_blank" rel="noopener noreferrer">using LangGraph</a>, as shown in the figure below (red - routing, blue - fallback, green - self-correction):</p><ul><li><strong>Routing:</strong> Adaptive RAG (<a href="https://arxiv.org/abs/2403.14403" target="_blank" rel="noopener noreferrer">paper</a>). Route questions to different retrieval approaches, as discussed above </li><li><strong>Fallback:</strong> Corrective RAG (<a href="https://arxiv.org/pdf/2401.15884.pdf" target="_blank" rel="noopener noreferrer">paper</a>). Fallback to web search if docs are not relevant to query</li><li><strong>Self-correction:</strong> Self-RAG (<a href="https://arxiv.org/abs/2310.11511" target="_blank" rel="noopener noreferrer">paper</a>). Fix answers w/ hallucinations or don’t address question</li></ul><p><img loading="lazy" src="/v0.2/assets/images/langgraph_rag-f039b41ef268bf46783706e58726fd9c.png" width="1721" height="510" class="img_ev3q"></p><table><thead><tr><th>Name</th><th>When to use</th><th>Description</th></tr></thead><tbody><tr><td>Self-RAG</td><td>When needing to fix answers with hallucinations or irrelevant content.</td><td>Self-RAG performs checks for document relevance, hallucinations, and answer quality during the RAG answer generation flow, iteratively building an answer and self-correcting errors.</td></tr><tr><td>Corrective-RAG</td><td>When needing a fallback mechanism for low relevance docs.</td><td>Corrective-RAG includes a fallback (e.g., to web search) if the retrieved documents are not relevant to the query, ensuring higher quality and more relevant retrieval.</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>See several videos and cookbooks showcasing RAG with LangGraph: </p><ul><li><a href="https://www.youtube.com/watch?v=E2shqsYwxck" target="_blank" rel="noopener noreferrer">LangGraph Corrective RAG</a></li><li><a href="https://www.youtube.com/watch?v=-ROS6gfYIts" target="_blank" rel="noopener noreferrer">LangGraph combining Adaptive, Self-RAG, and Corrective RAG</a> </li><li><a href="https://github.com/langchain-ai/langgraph/tree/main/examples/rag" target="_blank" rel="noopener noreferrer">Cookbooks for RAG using LangGraph</a></li></ul><p>See our LangGraph RAG recipes with partners:</p><ul><li><a href="https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/langchain" target="_blank" rel="noopener noreferrer">Meta</a></li><li><a href="https://github.com/mistralai/cookbook/tree/main/third_party/langchain" target="_blank" rel="noopener noreferrer">Mistral</a></li></ul></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-splitting">Text splitting<a href="#text-splitting" class="hash-link" aria-label="Direct link to Text splitting" title="Direct link to Text splitting">​</a></h3><p>LangChain offers many different types of <code>text splitters</code>.
These all live in the <code>langchain-text-splitters</code> package.</p><p>Table columns:</p><ul><li><strong>Name</strong>: Name of the text splitter</li><li><strong>Classes</strong>: Classes that implement this text splitter</li><li><strong>Splits On</strong>: How this text splitter splits text</li><li><strong>Adds Metadata</strong>: Whether or not this text splitter adds metadata about where each chunk came from.</li><li><strong>Description</strong>: Description of the splitter, including recommendation on when to use it.</li></ul><table><thead><tr><th>Name</th><th>Classes</th><th>Splits On</th><th>Adds Metadata</th><th>Description</th></tr></thead><tbody><tr><td>Recursive</td><td><a href="/v0.2/docs/how_to/recursive_text_splitter/">RecursiveCharacterTextSplitter</a>, <a href="/v0.2/docs/how_to/recursive_json_splitter/">RecursiveJsonSplitter</a></td><td>A list of user defined characters</td><td></td><td>Recursively splits text. This splitting is trying to keep related pieces of text next to each other. This is the <code>recommended way</code> to start splitting text.</td></tr><tr><td>HTML</td><td><a href="/v0.2/docs/how_to/HTML_header_metadata_splitter/">HTMLHeaderTextSplitter</a>, <a href="/v0.2/docs/how_to/HTML_section_aware_splitter/">HTMLSectionSplitter</a></td><td>HTML specific characters</td><td>✅</td><td>Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML)</td></tr><tr><td>Markdown</td><td><a href="/v0.2/docs/how_to/markdown_header_metadata_splitter/">MarkdownHeaderTextSplitter</a>,</td><td>Markdown specific characters</td><td>✅</td><td>Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)</td></tr><tr><td>Code</td><td><a href="/v0.2/docs/how_to/code_splitter/">many languages</a></td><td>Code (Python, JS) specific characters</td><td></td><td>Splits text based on characters specific to coding languages. 15 different languages are available to choose from.</td></tr><tr><td>Token</td><td><a href="/v0.2/docs/how_to/split_by_token/">many classes</a></td><td>Tokens</td><td></td><td>Splits text on tokens. There exist a few different ways to measure tokens.</td></tr><tr><td>Character</td><td><a href="/v0.2/docs/how_to/character_text_splitter/">CharacterTextSplitter</a></td><td>A user defined character</td><td></td><td>Splits text based on a user defined character. One of the simpler methods.</td></tr><tr><td>Semantic Chunker (Experimental)</td><td><a href="/v0.2/docs/how_to/semantic-chunker/">SemanticChunker</a></td><td>Sentences</td><td></td><td>First splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from <a href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb" target="_blank" rel="noopener noreferrer">Greg Kamradt</a></td></tr><tr><td>Integration: AI21 Semantic</td><td><a href="/v0.2/docs/integrations/document_transformers/ai21_semantic_text_splitter/">AI21SemanticTextSplitter</a></td><td></td><td>✅</td><td>Identifies distinct topics that form coherent pieces of text and splits along those.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation">Evaluation<a href="#evaluation" class="hash-link" aria-label="Direct link to Evaluation" title="Direct link to Evaluation">​</a></h3><span data-heading-keywords="evaluation,evaluate"></span><p>Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing the model&#x27;s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.
This process is vital for building reliable applications.</p><p><img loading="lazy" src="/v0.2/assets/images/langsmith_evaluate-7d48643f3e4c50d77234e13feb95144d.png" width="887" height="421" class="img_ev3q"></p><p><a href="https://docs.smith.langchain.com/" target="_blank" rel="noopener noreferrer">LangSmith</a> helps with this process in a few ways:</p><ul><li>It makes it easier to create and curate datasets via its tracing and annotation features</li><li>It provides an evaluation framework that helps you define metrics and run your app against your dataset</li><li>It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code</li></ul><p>To learn more, check out <a href="https://docs.smith.langchain.com/concepts/evaluation" target="_blank" rel="noopener noreferrer">this LangSmith guide</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tracing">Tracing<a href="#tracing" class="hash-link" aria-label="Direct link to Tracing" title="Direct link to Tracing">​</a></h3><span data-heading-keywords="trace,tracing"></span><p>A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called <code>runs</code>. These can be individual calls from a model, retriever,
tool, or sub-chains.
Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.</p><p>For a deeper dive, check out <a href="https://docs.smith.langchain.com/concepts/tracing" target="_blank" rel="noopener noreferrer">this LangSmith conceptual guide</a>.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><div style="display:flex;flex-direction:column"><hr><h4>Was this page helpful?</h4><div style="display:flex;gap:5px"><div style="display:flex;align-items:center;padding-top:10px;padding-bottom:10px;padding-left:22px;padding-right:22px;border:1px solid gray;border-radius:6px;gap:10px;cursor:pointer;font-size:16px;font-weight:600" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="#166534" style="width:24px;height:24px"><path stroke-linecap="round" stroke-linejoin="round" d="M6.633 10.25c.806 0 1.533-.446 2.031-1.08a9.041 9.041 0 0 1 2.861-2.4c.723-.384 1.35-.956 1.653-1.715a4.498 4.498 0 0 0 .322-1.672V2.75a.75.75 0 0 1 .75-.75 2.25 2.25 0 0 1 2.25 2.25c0 1.152-.26 2.243-.723 3.218-.266.558.107 1.282.725 1.282m0 0h3.126c1.026 0 1.945.694 2.054 1.715.045.422.068.85.068 1.285a11.95 11.95 0 0 1-2.649 7.521c-.388.482-.987.729-1.605.729H13.48c-.483 0-.964-.078-1.423-.23l-3.114-1.04a4.501 4.501 0 0 0-1.423-.23H5.904m10.598-9.75H14.25M5.904 18.5c.083.205.173.405.27.602.197.4-.078.898-.523.898h-.908c-.889 0-1.713-.518-1.972-1.368a12 12 0 0 1-.521-3.507c0-1.553.295-3.036.831-4.398C3.387 9.953 4.167 9.5 5 9.5h1.053c.472 0 .745.556.5.96a8.958 8.958 0 0 0-1.302 4.665c0 1.194.232 2.333.654 3.375Z"></path></svg></div><div style="display:flex;align-items:center;padding-top:10px;padding-bottom:10px;padding-left:22px;padding-right:22px;border:1px solid gray;border-radius:6px;gap:10px;cursor:pointer;font-size:16px;font-weight:600" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="#991b1b" style="width:24px;height:24px"><path stroke-linecap="round" stroke-linejoin="round" d="M7.498 15.25H4.372c-1.026 0-1.945-.694-2.054-1.715a12.137 12.137 0 0 1-.068-1.285c0-2.848.992-5.464 2.649-7.521C5.287 4.247 5.886 4 6.504 4h4.016a4.5 4.5 0 0 1 1.423.23l3.114 1.04a4.5 4.5 0 0 0 1.423.23h1.294M7.498 15.25c.618 0 .991.724.725 1.282A7.471 7.471 0 0 0 7.5 19.75 2.25 2.25 0 0 0 9.75 22a.75.75 0 0 0 .75-.75v-.633c0-.573.11-1.14.322-1.672.304-.76.93-1.33 1.653-1.715a9.04 9.04 0 0 0 2.86-2.4c.498-.634 1.226-1.08 2.032-1.08h.384m-10.253 1.5H9.7m8.075-9.75c.01.05.027.1.05.148.593 1.2.925 2.55.925 3.977 0 1.487-.36 2.89-.999 4.125m.023-8.25c-.076-.365.183-.75.575-.75h.908c.889 0 1.713.518 1.972 1.368.339 1.11.521 2.287.521 3.507 0 1.553-.295 3.036-.831 4.398-.306.774-1.086 1.227-1.918 1.227h-1.053c-.472 0-.745-.556-.5-.96a8.95 8.95 0 0 0 .303-.54"></path></svg></div></div><br><h4>You can also leave detailed feedback<!-- --> <a target="_blank" href="https://github.com/langchain-ai/langchain/issues/new?assignees=&amp;labels=03+-+Documentation&amp;projects=&amp;template=documentation.yml&amp;title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E">on GitHub</a>.</h4></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/v0.2/docs/how_to/vectorstores/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">How to create and query vector stores</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/v0.2/docs/versions/overview/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Overview of v0.2</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#architecture" class="table-of-contents__link toc-highlight">Architecture</a><ul><li><a href="#langchain-core" class="table-of-contents__link toc-highlight"><code>langchain-core</code></a></li><li><a href="#langchain" class="table-of-contents__link toc-highlight"><code>langchain</code></a></li><li><a href="#langchain-community" class="table-of-contents__link toc-highlight"><code>langchain-community</code></a></li><li><a href="#partner-packages" class="table-of-contents__link toc-highlight">Partner packages</a></li><li><a href="#langgraph" class="table-of-contents__link toc-highlight"><code>langgraph</code></a></li><li><a href="#langserve" class="table-of-contents__link toc-highlight"><code>langserve</code></a></li><li><a href="#langsmith" class="table-of-contents__link toc-highlight">LangSmith</a></li></ul></li><li><a href="#langchain-expression-language-lcel" class="table-of-contents__link toc-highlight">LangChain Expression Language (LCEL)</a><ul><li><a href="#runnable-interface" class="table-of-contents__link toc-highlight">Runnable interface</a></li></ul></li><li><a href="#components" class="table-of-contents__link toc-highlight">Components</a><ul><li><a href="#chat-models" class="table-of-contents__link toc-highlight">Chat models</a></li><li><a href="#llms" class="table-of-contents__link toc-highlight">LLMs</a></li><li><a href="#messages" class="table-of-contents__link toc-highlight">Messages</a></li><li><a href="#prompt-templates" class="table-of-contents__link toc-highlight">Prompt templates</a></li><li><a href="#example-selectors" class="table-of-contents__link toc-highlight">Example selectors</a></li><li><a href="#output-parsers" class="table-of-contents__link toc-highlight">Output parsers</a></li><li><a href="#chat-history" class="table-of-contents__link toc-highlight">Chat history</a></li><li><a href="#documents" class="table-of-contents__link toc-highlight">Documents</a></li><li><a href="#document-loaders" class="table-of-contents__link toc-highlight">Document loaders</a></li><li><a href="#text-splitters" class="table-of-contents__link toc-highlight">Text splitters</a></li><li><a href="#embedding-models" class="table-of-contents__link toc-highlight">Embedding models</a></li><li><a href="#vector-stores" class="table-of-contents__link toc-highlight">Vector stores</a></li><li><a href="#retrievers" class="table-of-contents__link toc-highlight">Retrievers</a></li><li><a href="#key-value-stores" class="table-of-contents__link toc-highlight">Key-value stores</a></li><li><a href="#tools" class="table-of-contents__link toc-highlight">Tools</a></li><li><a href="#toolkits" class="table-of-contents__link toc-highlight">Toolkits</a></li><li><a href="#agents" class="table-of-contents__link toc-highlight">Agents</a></li><li><a href="#callbacks" class="table-of-contents__link toc-highlight">Callbacks</a></li></ul></li><li><a href="#techniques" class="table-of-contents__link toc-highlight">Techniques</a><ul><li><a href="#streaming" class="table-of-contents__link toc-highlight">Streaming</a></li><li><a href="#functiontool-calling" class="table-of-contents__link toc-highlight">Function/tool calling</a></li><li><a href="#structured-output" class="table-of-contents__link toc-highlight">Structured output</a></li><li><a href="#few-shot-prompting" class="table-of-contents__link toc-highlight">Few-shot prompting</a></li><li><a href="#retrieval" class="table-of-contents__link toc-highlight">Retrieval</a></li><li><a href="#text-splitting" class="table-of-contents__link toc-highlight">Text splitting</a></li><li><a href="#evaluation" class="table-of-contents__link toc-highlight">Evaluation</a></li><li><a href="#tracing" class="table-of-contents__link toc-highlight">Tracing</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/LangChainAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">GitHub</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/langchain-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">Organization<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="footer__link-item">Python<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/langchain-ai/langchainjs" target="_blank" rel="noopener noreferrer" class="footer__link-item">JS/TS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://langchain.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Homepage<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://blog.langchain.dev" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/@LangChain" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 LangChain, Inc.</div></div></div></footer></div>
<script src="/v0.2/assets/js/runtime~main.238fea3b.js"></script>
<script src="/v0.2/assets/js/main.34559fbe.js"></script>
</body>
</html>