<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-additional_resources/arxiv_references" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">arXiv | ü¶úÔ∏èüîó LangChain</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://python.langchain.com/v0.2/img/brand/theme-image.png"><meta data-rh="true" name="twitter:image" content="https://python.langchain.com/v0.2/img/brand/theme-image.png"><meta data-rh="true" property="og:url" content="https://python.langchain.com/docs/additional_resources/arxiv_references/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="arXiv | ü¶úÔ∏èüîó LangChain"><meta data-rh="true" name="description" content="LangChain implements the latest research in the field of Natural Language Processing."><meta data-rh="true" property="og:description" content="LangChain implements the latest research in the field of Natural Language Processing."><link data-rh="true" rel="icon" href="/v0.2/img/brand/favicon.png"><link data-rh="true" rel="canonical" href="https://python.langchain.com/docs/additional_resources/arxiv_references/"><link data-rh="true" rel="alternate" href="https://python.langchain.com/v0.2/docs/additional_resources/arxiv_references/" hreflang="en"><link data-rh="true" rel="alternate" href="https://python.langchain.com/v0.2/docs/additional_resources/arxiv_references/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://VAU016LAWS-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="ü¶úÔ∏èüîó LangChain" href="/v0.2/opensearch.xml">


<script src="/v0.2/js/google_analytics.js"></script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F" async></script><link rel="stylesheet" href="/v0.2/assets/css/styles.0e41ce18.css">
<link rel="preload" href="/v0.2/assets/js/runtime~main.238fea3b.js" as="script">
<link rel="preload" href="/v0.2/assets/js/main.34559fbe.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">A newer LangChain version is out! Check out the <a href="https://python.langchain.com/docs/introduction">latest version</a>.</div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/v0.2/"><div class="navbar__logo"><img src="/v0.2/img/brand/wordmark.png" alt="ü¶úÔ∏èüîó LangChain" class="themedImage_ToTc themedImage--light_HNdA"><img src="/v0.2/img/brand/wordmark-dark.png" alt="ü¶úÔ∏èüîó LangChain" class="themedImage_ToTc themedImage--dark_i4oU"></div></a><a class="navbar__item navbar__link" href="/v0.2/docs/integrations/platforms/">Integrations</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">API reference</a><ul class="dropdown__menu"><li><a href="https://python.langchain.com/v0.2/api_reference/reference.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Latest</a></li><li><a href="https://api.python.langchain.com/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Legacy<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">More</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/v0.2/docs/people/">People</a></li><li><a class="dropdown__link" href="/v0.2/docs/contributing/">Contributing</a></li><li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/README.md" target="_blank" rel="noopener noreferrer" class="dropdown__link">Cookbooks<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/v0.2/docs/additional_resources/tutorials/">3rd party tutorials</a></li><li><a class="dropdown__link" href="/v0.2/docs/additional_resources/youtube/">YouTube</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/v0.2/docs/additional_resources/arxiv_references/">arXiv</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">v0.2</a><ul class="dropdown__menu"><li><a href="https://python.langchain.com/docs/introduction/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Latest<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/v0.2/docs/introduction/">v0.2</a></li><li><a href="https://python.langchain.com/v0.1/docs/get_started/introduction" target="_blank" rel="noopener noreferrer" class="dropdown__link">v0.1<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">ü¶úÔ∏èüîó</a><ul class="dropdown__menu"><li><a href="https://smith.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangSmith<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://docs.smith.langchain.com/" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangSmith Docs<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://smith.langchain.com/hub" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangChain Hub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://js.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">JS/TS Docs<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a href="https://chat.langchain.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">üí¨<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="theme-doc-version-banner alert alert--warning margin-bottom--md" role="alert"><div>This is documentation for <!-- -->LangChain<!-- --> <b>v0.2</b>, which is no longer actively maintained.</div><div class="margin-top--md">For the current stable version, see <b><a href="https://python.langchain.com/docs/additional_resources/arxiv_references/" target="_blank" rel="noopener noreferrer">this version</a></b> (<!-- -->Latest<!-- -->).</div></div><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>arXiv</h1><p>LangChain implements the latest research in the field of Natural Language Processing.
This page contains <code>arXiv</code> papers referenced in the LangChain Documentation, API Reference,
Templates, and Cookbooks.</p><p>From the opposite direction, scientists use <code>LangChain</code> in research and reference it in the research papers. </p><p><code>arXiv</code> papers with references to:
<a href="https://arxiv.org/search/?query=langchain&amp;searchtype=all&amp;source=header" target="_blank" rel="noopener noreferrer">LangChain</a> | <a href="https://arxiv.org/search/?query=langgraph&amp;searchtype=all&amp;source=header" target="_blank" rel="noopener noreferrer">LangGraph</a> | <a href="https://arxiv.org/search/?query=langsmith&amp;searchtype=all&amp;source=header" target="_blank" rel="noopener noreferrer">LangSmith</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">‚Äã</a></h2><table><thead><tr><th>arXiv id / Title</th><th>Authors</th><th>Published date üîª</th><th>LangChain Documentation</th></tr></thead><tbody><tr><td><code>2403.14403v2</code> <a href="http://arxiv.org/abs/2403.14403v2" target="_blank" rel="noopener noreferrer">Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</a></td><td>Soyeong Jeong, Jinheon Baek, Sukmin Cho,  et al.</td><td>2024<!-- -->‚Äë<!-- -->03<!-- -->‚Äë<!-- -->21</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></td></tr><tr><td><code>2402.03620v1</code> <a href="http://arxiv.org/abs/2402.03620v1" target="_blank" rel="noopener noreferrer">Self-Discover: Large Language Models Self-Compose Reasoning Structures</a></td><td>Pei Zhou, Jay Pujara, Xiang Ren,  et al.</td><td>2024<!-- -->‚Äë<!-- -->02<!-- -->‚Äë<!-- -->06</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/self-discover.ipynb" target="_blank" rel="noopener noreferrer">Self-Discover</a></td></tr><tr><td><code>2402.03367v2</code> <a href="http://arxiv.org/abs/2402.03367v2" target="_blank" rel="noopener noreferrer">RAG-Fusion: a New Take on Retrieval-Augmented Generation</a></td><td>Zackary Rackauckas</td><td>2024<!-- -->‚Äë<!-- -->01<!-- -->‚Äë<!-- -->31</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></td></tr><tr><td><code>2401.18059v1</code> <a href="http://arxiv.org/abs/2401.18059v1" target="_blank" rel="noopener noreferrer">RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</a></td><td>Parth Sarthi, Salman Abdullah, Aditi Tuli,  et al.</td><td>2024<!-- -->‚Äë<!-- -->01<!-- -->‚Äë<!-- -->31</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb" target="_blank" rel="noopener noreferrer">Raptor</a></td></tr><tr><td><code>2401.15884v2</code> <a href="http://arxiv.org/abs/2401.15884v2" target="_blank" rel="noopener noreferrer">Corrective Retrieval Augmented Generation</a></td><td>Shi-Qi Yan, Jia-Chen Gu, Yun Zhu,  et al.</td><td>2024<!-- -->‚Äë<!-- -->01<!-- -->‚Äë<!-- -->29</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_crag.ipynb" target="_blank" rel="noopener noreferrer">Langgraph Crag</a></td></tr><tr><td><code>2401.08500v1</code> <a href="http://arxiv.org/abs/2401.08500v1" target="_blank" rel="noopener noreferrer">Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</a></td><td>Tal Ridnik, Dedy Kredo, Itamar Friedman</td><td>2024<!-- -->‚Äë<!-- -->01<!-- -->‚Äë<!-- -->16</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></td></tr><tr><td><code>2401.04088v1</code> <a href="http://arxiv.org/abs/2401.04088v1" target="_blank" rel="noopener noreferrer">Mixtral of Experts</a></td><td>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,  et al.</td><td>2024<!-- -->‚Äë<!-- -->01<!-- -->‚Äë<!-- -->08</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/together_ai.ipynb" target="_blank" rel="noopener noreferrer">Together Ai</a></td></tr><tr><td><code>2312.06648v2</code> <a href="http://arxiv.org/abs/2312.06648v2" target="_blank" rel="noopener noreferrer">Dense X Retrieval: What Retrieval Granularity Should We Use?</a></td><td>Tong Chen, Hongwei Wang, Sihao Chen,  et al.</td><td>2023<!-- -->‚Äë<!-- -->12<!-- -->‚Äë<!-- -->11</td><td><code>Template:</code> <a href="https://python.langchain.com/docs/templates/propositional-retrieval" target="_blank" rel="noopener noreferrer">propositional-retrieval</a></td></tr><tr><td><code>2311.09210v1</code> <a href="http://arxiv.org/abs/2311.09210v1" target="_blank" rel="noopener noreferrer">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a></td><td>Wenhao Yu, Hongming Zhang, Xiaoman Pan,  et al.</td><td>2023<!-- -->‚Äë<!-- -->11<!-- -->‚Äë<!-- -->15</td><td><code>Template:</code> <a href="https://python.langchain.com/docs/templates/chain-of-note-wiki" target="_blank" rel="noopener noreferrer">chain-of-note-wiki</a></td></tr><tr><td><code>2310.11511v1</code> <a href="http://arxiv.org/abs/2310.11511v1" target="_blank" rel="noopener noreferrer">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</a></td><td>Akari Asai, Zeqiu Wu, Yizhong Wang,  et al.</td><td>2023<!-- -->‚Äë<!-- -->10<!-- -->‚Äë<!-- -->17</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_self_rag.ipynb" target="_blank" rel="noopener noreferrer">Langgraph Self Rag</a></td></tr><tr><td><code>2310.06117v2</code> <a href="http://arxiv.org/abs/2310.06117v2" target="_blank" rel="noopener noreferrer">Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</a></td><td>Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,  et al.</td><td>2023<!-- -->‚Äë<!-- -->10<!-- -->‚Äë<!-- -->09</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a>, <code>Template:</code> <a href="https://python.langchain.com/docs/templates/stepback-qa-prompting" target="_blank" rel="noopener noreferrer">stepback-qa-prompting</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb" target="_blank" rel="noopener noreferrer">Stepback-Qa</a></td></tr><tr><td><code>2307.15337v3</code> <a href="http://arxiv.org/abs/2307.15337v3" target="_blank" rel="noopener noreferrer">Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation</a></td><td>Xuefei Ning, Zinan Lin, Zixuan Zhou,  et al.</td><td>2023<!-- -->‚Äë<!-- -->07<!-- -->‚Äë<!-- -->28</td><td><code>Template:</code> <a href="https://python.langchain.com/docs/templates/skeleton-of-thought" target="_blank" rel="noopener noreferrer">skeleton-of-thought</a></td></tr><tr><td><code>2307.09288v2</code> <a href="http://arxiv.org/abs/2307.09288v2" target="_blank" rel="noopener noreferrer">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></td><td>Hugo Touvron, Louis Martin, Kevin Stone,  et al.</td><td>2023<!-- -->‚Äë<!-- -->07<!-- -->‚Äë<!-- -->18</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb" target="_blank" rel="noopener noreferrer">Semi Structured Rag</a></td></tr><tr><td><code>2307.03172v3</code> <a href="http://arxiv.org/abs/2307.03172v3" target="_blank" rel="noopener noreferrer">Lost in the Middle: How Language Models Use Long Contexts</a></td><td>Nelson F. Liu, Kevin Lin, John Hewitt,  et al.</td><td>2023<!-- -->‚Äë<!-- -->07<!-- -->‚Äë<!-- -->06</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/how_to/long_context_reorder" target="_blank" rel="noopener noreferrer">docs/how_to/long_context_reorder</a></td></tr><tr><td><code>2305.14283v3</code> <a href="http://arxiv.org/abs/2305.14283v3" target="_blank" rel="noopener noreferrer">Query Rewriting for Retrieval-Augmented Large Language Models</a></td><td>Xinbei Ma, Yeyun Gong, Pengcheng He,  et al.</td><td>2023<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->23</td><td><code>Template:</code> <a href="https://python.langchain.com/docs/templates/rewrite-retrieve-read" target="_blank" rel="noopener noreferrer">rewrite-retrieve-read</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb" target="_blank" rel="noopener noreferrer">Rewrite</a></td></tr><tr><td><code>2305.08291v1</code> <a href="http://arxiv.org/abs/2305.08291v1" target="_blank" rel="noopener noreferrer">Large Language Model Guided Tree-of-Thought</a></td><td>Jieyi Long</td><td>2023<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->15</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.tot" target="_blank" rel="noopener noreferrer">langchain_experimental.tot</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/tree_of_thought.ipynb" target="_blank" rel="noopener noreferrer">Tree Of Thought</a></td></tr><tr><td><code>2305.04091v3</code> <a href="http://arxiv.org/abs/2305.04091v3" target="_blank" rel="noopener noreferrer">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</a></td><td>Lei Wang, Wanyu Xu, Yihuai Lan,  et al.</td><td>2023<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->06</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/plan_and_execute_agent.ipynb" target="_blank" rel="noopener noreferrer">Plan And Execute Agent</a></td></tr><tr><td><code>2305.02156v1</code> <a href="http://arxiv.org/abs/2305.02156v1" target="_blank" rel="noopener noreferrer">Zero-Shot Listwise Document Reranking with a Large Language Model</a></td><td>Xueguang Ma, Xinyu Zhang, Ronak Pradeep,  et al.</td><td>2023<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->03</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/how_to/contextual_compression" target="_blank" rel="noopener noreferrer">docs/how_to/contextual_compression</a>, <code>API:</code> <a href="https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html#langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank" target="_blank" rel="noopener noreferrer">langchain...LLMListwiseRerank</a></td></tr><tr><td><code>2304.08485v2</code> <a href="http://arxiv.org/abs/2304.08485v2" target="_blank" rel="noopener noreferrer">Visual Instruction Tuning</a></td><td>Haotian Liu, Chunyuan Li, Qingyang Wu,  et al.</td><td>2023<!-- -->‚Äë<!-- -->04<!-- -->‚Äë<!-- -->17</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb" target="_blank" rel="noopener noreferrer">Semi Structured Multi Modal Rag Llama2</a>, <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb" target="_blank" rel="noopener noreferrer">Semi Structured And Multi Modal Rag</a></td></tr><tr><td><code>2304.03442v2</code> <a href="http://arxiv.org/abs/2304.03442v2" target="_blank" rel="noopener noreferrer">Generative Agents: Interactive Simulacra of Human Behavior</a></td><td>Joon Sung Park, Joseph C. O&#x27;Brien, Carrie J. Cai,  et al.</td><td>2023<!-- -->‚Äë<!-- -->04<!-- -->‚Äë<!-- -->07</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/generative_agents_interactive_simulacra_of_human_behavior.ipynb" target="_blank" rel="noopener noreferrer">Generative Agents Interactive Simulacra Of Human Behavior</a>, <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/multiagent_bidding.ipynb" target="_blank" rel="noopener noreferrer">Multiagent Bidding</a></td></tr><tr><td><code>2303.17760v2</code> <a href="http://arxiv.org/abs/2303.17760v2" target="_blank" rel="noopener noreferrer">CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Language Model Society</a></td><td>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,  et al.</td><td>2023<!-- -->‚Äë<!-- -->03<!-- -->‚Äë<!-- -->31</td><td><code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/camel_role_playing.ipynb" target="_blank" rel="noopener noreferrer">Camel Role Playing</a></td></tr><tr><td><code>2303.17580v4</code> <a href="http://arxiv.org/abs/2303.17580v4" target="_blank" rel="noopener noreferrer">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></td><td>Yongliang Shen, Kaitao Song, Xu Tan,  et al.</td><td>2023<!-- -->‚Äë<!-- -->03<!-- -->‚Äë<!-- -->30</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.autonomous_agents" target="_blank" rel="noopener noreferrer">langchain_experimental.autonomous_agents</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb" target="_blank" rel="noopener noreferrer">Hugginggpt</a></td></tr><tr><td><code>2301.10226v4</code> <a href="http://arxiv.org/abs/2301.10226v4" target="_blank" rel="noopener noreferrer">A Watermark for Large Language Models</a></td><td>John Kirchenbauer, Jonas Geiping, Yuxin Wen,  et al.</td><td>2023<!-- -->‚Äë<!-- -->01<!-- -->‚Äë<!-- -->24</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI.html#langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI" target="_blank" rel="noopener noreferrer">langchain_community...OCIModelDeploymentTGI</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_huggingface...HuggingFaceEndpoint</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceTextGenInference</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceEndpoint</a></td></tr><tr><td><code>2212.10496v1</code> <a href="http://arxiv.org/abs/2212.10496v1" target="_blank" rel="noopener noreferrer">Precise Zero-Shot Dense Retrieval without Relevance Labels</a></td><td>Luyu Gao, Xueguang Ma, Jimmy Lin,  et al.</td><td>2022<!-- -->‚Äë<!-- -->12<!-- -->‚Äë<!-- -->20</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a>, <code>API:</code> <a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html#langchain.chains.hyde.base.HypotheticalDocumentEmbedder" target="_blank" rel="noopener noreferrer">langchain...HypotheticalDocumentEmbedder</a>, <code>Template:</code> <a href="https://python.langchain.com/docs/templates/hyde" target="_blank" rel="noopener noreferrer">hyde</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb" target="_blank" rel="noopener noreferrer">Hypothetical Document Embeddings</a></td></tr><tr><td><code>2212.08073v1</code> <a href="http://arxiv.org/abs/2212.08073v1" target="_blank" rel="noopener noreferrer">Constitutional AI: Harmlessness from AI Feedback</a></td><td>Yuntao Bai, Saurav Kadavath, Sandipan Kundu,  et al.</td><td>2022<!-- -->‚Äë<!-- -->12<!-- -->‚Äë<!-- -->15</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/versions/migrating_chains/constitutional_chain" target="_blank" rel="noopener noreferrer">docs/versions/migrating_chains/constitutional_chain</a></td></tr><tr><td><code>2212.07425v3</code> <a href="http://arxiv.org/abs/2212.07425v3" target="_blank" rel="noopener noreferrer">Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments</a></td><td>Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande,  et al.</td><td>2022<!-- -->‚Äë<!-- -->12<!-- -->‚Äë<!-- -->12</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.fallacy_removal" target="_blank" rel="noopener noreferrer">langchain_experimental.fallacy_removal</a></td></tr><tr><td><code>2211.13892v2</code> <a href="http://arxiv.org/abs/2211.13892v2" target="_blank" rel="noopener noreferrer">Complementary Explanations for Effective In-Context Learning</a></td><td>Xi Ye, Srinivasan Iyer, Asli Celikyilmaz,  et al.</td><td>2022<!-- -->‚Äë<!-- -->11<!-- -->‚Äë<!-- -->25</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/example_selectors/langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector.html#langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector" target="_blank" rel="noopener noreferrer">langchain_core...MaxMarginalRelevanceExampleSelector</a></td></tr><tr><td><code>2211.10435v2</code> <a href="http://arxiv.org/abs/2211.10435v2" target="_blank" rel="noopener noreferrer">PAL: Program-aided Language Models</a></td><td>Luyu Gao, Aman Madaan, Shuyan Zhou,  et al.</td><td>2022<!-- -->‚Äë<!-- -->11<!-- -->‚Äë<!-- -->18</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.pal_chain" target="_blank" rel="noopener noreferrer">langchain_experimental.pal_chain</a>, <a href="https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#langchain_experimental.pal_chain.base.PALChain" target="_blank" rel="noopener noreferrer">langchain_experimental...PALChain</a>, <code>Cookbook:</code> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/program_aided_language_model.ipynb" target="_blank" rel="noopener noreferrer">Program Aided Language Model</a></td></tr><tr><td><code>2210.11934v2</code> <a href="http://arxiv.org/abs/2210.11934v2" target="_blank" rel="noopener noreferrer">An Analysis of Fusion Functions for Hybrid Retrieval</a></td><td>Sebastian Bruch, Siyu Gai, Amir Ingber</td><td>2022<!-- -->‚Äë<!-- -->10<!-- -->‚Äë<!-- -->21</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></td></tr><tr><td><code>2210.03629v3</code> <a href="http://arxiv.org/abs/2210.03629v3" target="_blank" rel="noopener noreferrer">ReAct: Synergizing Reasoning and Acting in Language Models</a></td><td>Shunyu Yao, Jeffrey Zhao, Dian Yu,  et al.</td><td>2022<!-- -->‚Äë<!-- -->10<!-- -->‚Äë<!-- -->06</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/integrations/tools/ionic_shopping" target="_blank" rel="noopener noreferrer">docs/integrations/tools/ionic_shopping</a>, <a href="https://python.langchain.com/v0.2/docs/integrations/providers/cohere" target="_blank" rel="noopener noreferrer">docs/integrations/providers/cohere</a>, <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a>, <code>API:</code> <a href="https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent" target="_blank" rel="noopener noreferrer">langchain...create_react_agent</a>, <a href="https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain" target="_blank" rel="noopener noreferrer">langchain...TrajectoryEvalChain</a></td></tr><tr><td><code>2209.10785v2</code> <a href="http://arxiv.org/abs/2209.10785v2" target="_blank" rel="noopener noreferrer">Deep Lake: a Lakehouse for Deep Learning</a></td><td>Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan,  et al.</td><td>2022<!-- -->‚Äë<!-- -->09<!-- -->‚Äë<!-- -->22</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/integrations/providers/activeloop_deeplake" target="_blank" rel="noopener noreferrer">docs/integrations/providers/activeloop_deeplake</a></td></tr><tr><td><code>2205.13147v4</code> <a href="http://arxiv.org/abs/2205.13147v4" target="_blank" rel="noopener noreferrer">Matryoshka Representation Learning</a></td><td>Aditya Kusupati, Gantavya Bhatt, Aniket Rege,  et al.</td><td>2022<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->26</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/integrations/providers/snowflake" target="_blank" rel="noopener noreferrer">docs/integrations/providers/snowflake</a></td></tr><tr><td><code>2205.12654v1</code> <a href="http://arxiv.org/abs/2205.12654v1" target="_blank" rel="noopener noreferrer">Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages</a></td><td>Kevin Heffernan, Onur √áelebi, Holger Schwenk</td><td>2022<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->25</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.laser.LaserEmbeddings.html#langchain_community.embeddings.laser.LaserEmbeddings" target="_blank" rel="noopener noreferrer">langchain_community...LaserEmbeddings</a></td></tr><tr><td><code>2204.00498v1</code> <a href="http://arxiv.org/abs/2204.00498v1" target="_blank" rel="noopener noreferrer">Evaluating the Text-to-SQL Capabilities of Large Language Models</a></td><td>Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau</td><td>2022<!-- -->‚Äë<!-- -->03<!-- -->‚Äë<!-- -->15</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/tutorials/sql_qa" target="_blank" rel="noopener noreferrer">docs/tutorials/sql_qa</a>, <code>API:</code> <a href="https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase" target="_blank" rel="noopener noreferrer">langchain_community...SQLDatabase</a>, <a href="https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.spark_sql.SparkSQL.html#langchain_community.utilities.spark_sql.SparkSQL" target="_blank" rel="noopener noreferrer">langchain_community...SparkSQL</a></td></tr><tr><td><code>2202.00666v5</code> <a href="http://arxiv.org/abs/2202.00666v5" target="_blank" rel="noopener noreferrer">Locally Typical Sampling</a></td><td>Clara Meister, Tiago Pimentel, Gian Wiher,  et al.</td><td>2022<!-- -->‚Äë<!-- -->02<!-- -->‚Äë<!-- -->01</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_huggingface...HuggingFaceEndpoint</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceTextGenInference</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceEndpoint</a></td></tr><tr><td><code>2112.01488v3</code> <a href="http://arxiv.org/abs/2112.01488v3" target="_blank" rel="noopener noreferrer">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a></td><td>Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,  et al.</td><td>2021<!-- -->‚Äë<!-- -->12<!-- -->‚Äë<!-- -->02</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/integrations/retrievers/ragatouille" target="_blank" rel="noopener noreferrer">docs/integrations/retrievers/ragatouille</a>, <a href="https://python.langchain.com/v0.2/docs/integrations/providers/ragatouille" target="_blank" rel="noopener noreferrer">docs/integrations/providers/ragatouille</a>, <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a>, <a href="https://python.langchain.com/v0.2/docs/integrations/providers/dspy" target="_blank" rel="noopener noreferrer">docs/integrations/providers/dspy</a></td></tr><tr><td><code>2103.00020v1</code> <a href="http://arxiv.org/abs/2103.00020v1" target="_blank" rel="noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision</a></td><td>Alec Radford, Jong Wook Kim, Chris Hallacy,  et al.</td><td>2021<!-- -->‚Äë<!-- -->02<!-- -->‚Äë<!-- -->26</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.open_clip" target="_blank" rel="noopener noreferrer">langchain_experimental.open_clip</a></td></tr><tr><td><code>2005.14165v4</code> <a href="http://arxiv.org/abs/2005.14165v4" target="_blank" rel="noopener noreferrer">Language Models are Few-Shot Learners</a></td><td>Tom B. Brown, Benjamin Mann, Nick Ryder,  et al.</td><td>2020<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->28</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></td></tr><tr><td><code>2005.11401v4</code> <a href="http://arxiv.org/abs/2005.11401v4" target="_blank" rel="noopener noreferrer">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></td><td>Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.</td><td>2020<!-- -->‚Äë<!-- -->05<!-- -->‚Äë<!-- -->22</td><td><code>Docs:</code> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></td></tr><tr><td><code>1909.05858v2</code> <a href="http://arxiv.org/abs/1909.05858v2" target="_blank" rel="noopener noreferrer">CTRL: A Conditional Transformer Language Model for Controllable Generation</a></td><td>Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al.</td><td>2019<!-- -->‚Äë<!-- -->09<!-- -->‚Äë<!-- -->11</td><td><code>API:</code> <a href="https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_huggingface...HuggingFaceEndpoint</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceTextGenInference</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceEndpoint</a></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-rag-learning-to-adapt-retrieval-augmented-large-language-models-through-question-complexity">Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity<a href="#adaptive-rag-learning-to-adapt-retrieval-augmented-large-language-models-through-question-complexity" class="hash-link" aria-label="Direct link to Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity" title="Direct link to Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Soyeong Jeong, Jinheon Baek, Sukmin Cho,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2403.14403v2" target="_blank" rel="noopener noreferrer">2403.14403v2</a>  <strong>Published Date:</strong> 2024-03-21</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li></ul></li></ul><p><strong>Abstract:</strong> Retrieval-Augmented Large Language Models (LLMs), which incorporate the
non-parametric knowledge from external knowledge bases into LLMs, have emerged
as a promising approach to enhancing response accuracy in several tasks, such
as Question-Answering (QA). However, even though there are various approaches
dealing with queries of different complexities, they either handle simple
queries with unnecessary computational overhead or fail to adequately address
complex multi-step queries; yet, not all user requests fall into only one of
the simple or complex categories. In this work, we propose a novel adaptive QA
framework, that can dynamically select the most suitable strategy for
(retrieval-augmented) LLMs from the simplest to the most sophisticated ones
based on the query complexity. Also, this selection process is operationalized
with a classifier, which is a smaller LM trained to predict the complexity
level of incoming queries with automatically collected labels, obtained from
actual predicted outcomes of models and inherent inductive biases in datasets.
This approach offers a balanced strategy, seamlessly adapting between the
iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval
methods, in response to a range of query complexities. We validate our model on
a set of open-domain QA datasets, covering multiple query complexities, and
show that ours enhances the overall efficiency and accuracy of QA systems,
compared to relevant baselines including the adaptive retrieval approaches.
Code is available at: <a href="https://github.com/starsuzi/Adaptive-RAG" target="_blank" rel="noopener noreferrer">https://github.com/starsuzi/Adaptive-RAG</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="self-discover-large-language-models-self-compose-reasoning-structures">Self-Discover: Large Language Models Self-Compose Reasoning Structures<a href="#self-discover-large-language-models-self-compose-reasoning-structures" class="hash-link" aria-label="Direct link to Self-Discover: Large Language Models Self-Compose Reasoning Structures" title="Direct link to Self-Discover: Large Language Models Self-Compose Reasoning Structures">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Pei Zhou, Jay Pujara, Xiang Ren,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2402.03620v1" target="_blank" rel="noopener noreferrer">2402.03620v1</a>  <strong>Published Date:</strong> 2024-02-06</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/self-discover.ipynb" target="_blank" rel="noopener noreferrer">self-discover</a></li></ul></li></ul><p><strong>Abstract:</strong> We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the
task-intrinsic reasoning structures to tackle complex reasoning problems that
are challenging for typical prompting methods. Core to the framework is a
self-discovery process where LLMs select multiple atomic reasoning modules such
as critical thinking and step-by-step thinking, and compose them into an
explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER
substantially improves GPT-4 and PaLM 2&#x27;s performance on challenging reasoning
benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as
much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER
outperforms inference-intensive methods such as CoT-Self-Consistency by more
than 20%, while requiring 10-40x fewer inference compute. Finally, we show that
the self-discovered reasoning structures are universally applicable across
model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share
commonalities with human reasoning patterns.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="rag-fusion-a-new-take-on-retrieval-augmented-generation">RAG-Fusion: a New Take on Retrieval-Augmented Generation<a href="#rag-fusion-a-new-take-on-retrieval-augmented-generation" class="hash-link" aria-label="Direct link to RAG-Fusion: a New Take on Retrieval-Augmented Generation" title="Direct link to RAG-Fusion: a New Take on Retrieval-Augmented Generation">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Zackary Rackauckas</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2402.03367v2" target="_blank" rel="noopener noreferrer">2402.03367v2</a>  <strong>Published Date:</strong> 2024-01-31</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li></ul></li></ul><p><strong>Abstract:</strong> Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries&#x27; relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="raptor-recursive-abstractive-processing-for-tree-organized-retrieval">RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval<a href="#raptor-recursive-abstractive-processing-for-tree-organized-retrieval" class="hash-link" aria-label="Direct link to RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval" title="Direct link to RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Parth Sarthi, Salman Abdullah, Aditi Tuli,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2401.18059v1" target="_blank" rel="noopener noreferrer">2401.18059v1</a>  <strong>Published Date:</strong> 2024-01-31</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb" target="_blank" rel="noopener noreferrer">RAPTOR</a></li></ul></li></ul><p><strong>Abstract:</strong> Retrieval-augmented language models can better adapt to changes in world
state and incorporate long-tail knowledge. However, most existing methods
retrieve only short contiguous chunks from a retrieval corpus, limiting
holistic understanding of the overall document context. We introduce the novel
approach of recursively embedding, clustering, and summarizing chunks of text,
constructing a tree with differing levels of summarization from the bottom up.
At inference time, our RAPTOR model retrieves from this tree, integrating
information across lengthy documents at different levels of abstraction.
Controlled experiments show that retrieval with recursive summaries offers
significant improvements over traditional retrieval-augmented LMs on several
tasks. On question-answering tasks that involve complex, multi-step reasoning,
we show state-of-the-art results; for example, by coupling RAPTOR retrieval
with the use of GPT-4, we can improve the best performance on the QuALITY
benchmark by 20% in absolute accuracy.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="corrective-retrieval-augmented-generation">Corrective Retrieval Augmented Generation<a href="#corrective-retrieval-augmented-generation" class="hash-link" aria-label="Direct link to Corrective Retrieval Augmented Generation" title="Direct link to Corrective Retrieval Augmented Generation">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Shi-Qi Yan, Jia-Chen Gu, Yun Zhu,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2401.15884v2" target="_blank" rel="noopener noreferrer">2401.15884v2</a>  <strong>Published Date:</strong> 2024-01-29</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_crag.ipynb" target="_blank" rel="noopener noreferrer">langgraph_crag</a></li></ul></li></ul><p><strong>Abstract:</strong> Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-generation-with-alphacodium-from-prompt-engineering-to-flow-engineering">Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering<a href="#code-generation-with-alphacodium-from-prompt-engineering-to-flow-engineering" class="hash-link" aria-label="Direct link to Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering" title="Direct link to Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Tal Ridnik, Dedy Kredo, Itamar Friedman</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2401.08500v1" target="_blank" rel="noopener noreferrer">2401.08500v1</a>  <strong>Published Date:</strong> 2024-01-16</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li></ul></li></ul><p><strong>Abstract:</strong> Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
<a href="https://github.com/Codium-ai/AlphaCodium" target="_blank" rel="noopener noreferrer">https://github.com/Codium-ai/AlphaCodium</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mixtral-of-experts">Mixtral of Experts<a href="#mixtral-of-experts" class="hash-link" aria-label="Direct link to Mixtral of Experts" title="Direct link to Mixtral of Experts">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2401.04088v1" target="_blank" rel="noopener noreferrer">2401.04088v1</a>  <strong>Published Date:</strong> 2024-01-08</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/together_ai.ipynb" target="_blank" rel="noopener noreferrer">together_ai</a></li></ul></li></ul><p><strong>Abstract:</strong> We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.
Mixtral has the same architecture as Mistral 7B, with the difference that each
layer is composed of 8 feedforward blocks (i.e. experts). For every token, at
each layer, a router network selects two experts to process the current state
and combine their outputs. Even though each token only sees two experts, the
selected experts can be different at each timestep. As a result, each token has
access to 47B parameters, but only uses 13B active parameters during inference.
Mixtral was trained with a context size of 32k tokens and it outperforms or
matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,
Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and
multilingual benchmarks. We also provide a model fine-tuned to follow
instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,
Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both
the base and instruct models are released under the Apache 2.0 license.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dense-x-retrieval-what-retrieval-granularity-should-we-use">Dense X Retrieval: What Retrieval Granularity Should We Use?<a href="#dense-x-retrieval-what-retrieval-granularity-should-we-use" class="hash-link" aria-label="Direct link to Dense X Retrieval: What Retrieval Granularity Should We Use?" title="Direct link to Dense X Retrieval: What Retrieval Granularity Should We Use?">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Tong Chen, Hongwei Wang, Sihao Chen,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2312.06648v2" target="_blank" rel="noopener noreferrer">2312.06648v2</a>  <strong>Published Date:</strong> 2023-12-11</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Template:</strong> <a href="https://python.langchain.com/docs/templates/propositional-retrieval" target="_blank" rel="noopener noreferrer">propositional-retrieval</a></li></ul></li></ul><p><strong>Abstract:</strong> Dense retrieval has become a prominent method to obtain relevant context or
world knowledge in open-domain NLP tasks. When we use a learned dense retriever
on a retrieval corpus at inference time, an often-overlooked design choice is
the retrieval unit in which the corpus is indexed, e.g. document, passage, or
sentence. We discover that the retrieval unit choice significantly impacts the
performance of both retrieval and downstream tasks. Distinct from the typical
approach of using passages or sentences, we introduce a novel retrieval unit,
proposition, for dense retrieval. Propositions are defined as atomic
expressions within text, each encapsulating a distinct factoid and presented in
a concise, self-contained natural language format. We conduct an empirical
comparison of different retrieval granularity. Our results reveal that
proposition-based retrieval significantly outperforms traditional passage or
sentence-based methods in dense retrieval. Moreover, retrieval by proposition
also enhances the performance of downstream QA tasks, since the retrieved texts
are more condensed with question-relevant information, reducing the need for
lengthy input tokens and minimizing the inclusion of extraneous, irrelevant
information.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="chain-of-note-enhancing-robustness-in-retrieval-augmented-language-models">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models<a href="#chain-of-note-enhancing-robustness-in-retrieval-augmented-language-models" class="hash-link" aria-label="Direct link to Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models" title="Direct link to Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Wenhao Yu, Hongming Zhang, Xiaoman Pan,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2311.09210v1" target="_blank" rel="noopener noreferrer">2311.09210v1</a>  <strong>Published Date:</strong> 2023-11-15</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Template:</strong> <a href="https://python.langchain.com/docs/templates/chain-of-note-wiki" target="_blank" rel="noopener noreferrer">chain-of-note-wiki</a></li></ul></li></ul><p><strong>Abstract:</strong> Retrieval-augmented language models (RALMs) represent a substantial
advancement in the capabilities of large language models, notably in reducing
factual hallucination by leveraging external knowledge sources. However, the
reliability of the retrieved information is not always guaranteed. The
retrieval of irrelevant data can lead to misguided responses, and potentially
causing the model to overlook its inherent knowledge, even when it possesses
adequate information to address the query. Moreover, standard RALMs often
struggle to assess whether they possess adequate knowledge, both intrinsic and
retrieved, to provide an accurate answer. In situations where knowledge is
lacking, these systems should ideally respond with &quot;unknown&quot; when the answer is
unattainable. In response to these challenges, we introduces Chain-of-Noting
(CoN), a novel approach aimed at improving the robustness of RALMs in facing
noisy, irrelevant documents and in handling unknown scenarios. The core idea of
CoN is to generate sequential reading notes for retrieved documents, enabling a
thorough evaluation of their relevance to the given question and integrating
this information to formulate the final answer. We employed ChatGPT to create
training data for CoN, which was subsequently trained on an LLaMa-2 7B model.
Our experiments across four open-domain QA benchmarks show that RALMs equipped
with CoN significantly outperform standard RALMs. Notably, CoN achieves an
average improvement of +7.9 in EM score given entirely noisy retrieved
documents and +10.5 in rejection rates for real-time questions that fall
outside the pre-training knowledge scope.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="self-rag-learning-to-retrieve-generate-and-critique-through-self-reflection">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection<a href="#self-rag-learning-to-retrieve-generate-and-critique-through-self-reflection" class="hash-link" aria-label="Direct link to Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection" title="Direct link to Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Akari Asai, Zeqiu Wu, Yizhong Wang,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2310.11511v1" target="_blank" rel="noopener noreferrer">2310.11511v1</a>  <strong>Published Date:</strong> 2023-10-17</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_self_rag.ipynb" target="_blank" rel="noopener noreferrer">langgraph_self_rag</a></li></ul></li></ul><p><strong>Abstract:</strong> Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM&#x27;s
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="take-a-step-back-evoking-reasoning-via-abstraction-in-large-language-models">Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models<a href="#take-a-step-back-evoking-reasoning-via-abstraction-in-large-language-models" class="hash-link" aria-label="Direct link to Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models" title="Direct link to Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2310.06117v2" target="_blank" rel="noopener noreferrer">2310.06117v2</a>  <strong>Published Date:</strong> 2023-10-09</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li><li><strong>Template:</strong> <a href="https://python.langchain.com/docs/templates/stepback-qa-prompting" target="_blank" rel="noopener noreferrer">stepback-qa-prompting</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb" target="_blank" rel="noopener noreferrer">stepback-qa</a></li></ul></li></ul><p><strong>Abstract:</strong> We present Step-Back Prompting, a simple prompting technique that enables
LLMs to do abstractions to derive high-level concepts and first principles from
instances containing specific details. Using the concepts and principles to
guide reasoning, LLMs significantly improve their abilities in following a
correct reasoning path towards the solution. We conduct experiments of
Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe
substantial performance gains on various challenging reasoning-intensive tasks
including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back
Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%
and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="skeleton-of-thought-prompting-llms-for-efficient-parallel-generation">Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation<a href="#skeleton-of-thought-prompting-llms-for-efficient-parallel-generation" class="hash-link" aria-label="Direct link to Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation" title="Direct link to Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Xuefei Ning, Zinan Lin, Zixuan Zhou,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2307.15337v3" target="_blank" rel="noopener noreferrer">2307.15337v3</a>  <strong>Published Date:</strong> 2023-07-28</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Template:</strong> <a href="https://python.langchain.com/docs/templates/skeleton-of-thought" target="_blank" rel="noopener noreferrer">skeleton-of-thought</a></li></ul></li></ul><p><strong>Abstract:</strong> This work aims at decreasing the end-to-end generation latency of large
language models (LLMs). One of the major causes of the high generation latency
is the sequential decoding approach adopted by almost all state-of-the-art
LLMs. In this work, motivated by the thinking and writing process of humans, we
propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the
skeleton of the answer, and then conducts parallel API calls or batched
decoding to complete the contents of each skeleton point in parallel. Not only
does SoT provide considerable speed-ups across 12 LLMs, but it can also
potentially improve the answer quality on several question categories. SoT is
an initial attempt at data-centric optimization for inference efficiency, and
showcases the potential of eliciting high-quality answers by explicitly
planning the answer structure in language.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="llama-2-open-foundation-and-fine-tuned-chat-models">Llama 2: Open Foundation and Fine-Tuned Chat Models<a href="#llama-2-open-foundation-and-fine-tuned-chat-models" class="hash-link" aria-label="Direct link to Llama 2: Open Foundation and Fine-Tuned Chat Models" title="Direct link to Llama 2: Open Foundation and Fine-Tuned Chat Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Hugo Touvron, Louis Martin, Kevin Stone,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2307.09288v2" target="_blank" rel="noopener noreferrer">2307.09288v2</a>  <strong>Published Date:</strong> 2023-07-18</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb" target="_blank" rel="noopener noreferrer">Semi_Structured_RAG</a></li></ul></li></ul><p><strong>Abstract:</strong> In this work, we develop and release Llama 2, a collection of pretrained and
fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70
billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
dialogue use cases. Our models outperform open-source chat models on most
benchmarks we tested, and based on our human evaluations for helpfulness and
safety, may be a suitable substitute for closed-source models. We provide a
detailed description of our approach to fine-tuning and safety improvements of
Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lost-in-the-middle-how-language-models-use-long-contexts">Lost in the Middle: How Language Models Use Long Contexts<a href="#lost-in-the-middle-how-language-models-use-long-contexts" class="hash-link" aria-label="Direct link to Lost in the Middle: How Language Models Use Long Contexts" title="Direct link to Lost in the Middle: How Language Models Use Long Contexts">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Nelson F. Liu, Kevin Lin, John Hewitt,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2307.03172v3" target="_blank" rel="noopener noreferrer">2307.03172v3</a>  <strong>Published Date:</strong> 2023-07-06</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/how_to/long_context_reorder" target="_blank" rel="noopener noreferrer">docs/how_to/long_context_reorder</a></li></ul></li></ul><p><strong>Abstract:</strong> While recent language models have the ability to take long contexts as input,
relatively little is known about how well they use longer context. We analyze
the performance of language models on two tasks that require identifying
relevant information in their input contexts: multi-document question answering
and key-value retrieval. We find that performance can degrade significantly
when changing the position of relevant information, indicating that current
language models do not robustly make use of information in long input contexts.
In particular, we observe that performance is often highest when relevant
information occurs at the beginning or end of the input context, and
significantly degrades when models must access relevant information in the
middle of long contexts, even for explicitly long-context models. Our analysis
provides a better understanding of how language models use their input context
and provides new evaluation protocols for future long-context language models.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="query-rewriting-for-retrieval-augmented-large-language-models">Query Rewriting for Retrieval-Augmented Large Language Models<a href="#query-rewriting-for-retrieval-augmented-large-language-models" class="hash-link" aria-label="Direct link to Query Rewriting for Retrieval-Augmented Large Language Models" title="Direct link to Query Rewriting for Retrieval-Augmented Large Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Xinbei Ma, Yeyun Gong, Pengcheng He,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2305.14283v3" target="_blank" rel="noopener noreferrer">2305.14283v3</a>  <strong>Published Date:</strong> 2023-05-23</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Template:</strong> <a href="https://python.langchain.com/docs/templates/rewrite-retrieve-read" target="_blank" rel="noopener noreferrer">rewrite-retrieve-read</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb" target="_blank" rel="noopener noreferrer">rewrite</a></li></ul></li></ul><p><strong>Abstract:</strong> Large Language Models (LLMs) play powerful, black-box readers in the
retrieve-then-read pipeline, making remarkable progress in knowledge-intensive
tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of
the previous retrieve-then-read for the retrieval-augmented LLMs from the
perspective of the query rewriting. Unlike prior studies focusing on adapting
either the retriever or the reader, our approach pays attention to the
adaptation of the search query itself, for there is inevitably a gap between
the input text and the needed knowledge in retrieval. We first prompt an LLM to
generate the query, then use a web search engine to retrieve contexts.
Furthermore, to better align the query to the frozen modules, we propose a
trainable scheme for our pipeline. A small language model is adopted as a
trainable rewriter to cater to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader by reinforcement learning.
Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice
QA. Experiments results show consistent performance improvement, indicating
that our framework is proven effective and scalable, and brings a new framework
for retrieval-augmented LLM.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-model-guided-tree-of-thought">Large Language Model Guided Tree-of-Thought<a href="#large-language-model-guided-tree-of-thought" class="hash-link" aria-label="Direct link to Large Language Model Guided Tree-of-Thought" title="Direct link to Large Language Model Guided Tree-of-Thought">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Jieyi Long</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2305.08291v1" target="_blank" rel="noopener noreferrer">2305.08291v1</a>  <strong>Published Date:</strong> 2023-05-15</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.tot" target="_blank" rel="noopener noreferrer">langchain_experimental.tot</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/tree_of_thought.ipynb" target="_blank" rel="noopener noreferrer">tree_of_thought</a></li></ul></li></ul><p><strong>Abstract:</strong> In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel
approach aimed at improving the problem-solving capabilities of auto-regressive
large language models (LLMs). The ToT technique is inspired by the human mind&#x27;s
approach for solving complex reasoning tasks through trial and error. In this
process, the human mind explores the solution space through a tree-like thought
process, allowing for backtracking when necessary. To implement ToT as a
software system, we augment an LLM with additional modules including a prompter
agent, a checker module, a memory module, and a ToT controller. In order to
solve a given problem, these modules engage in a multi-round conversation with
the LLM. The memory module records the conversation and state history of the
problem solving process, which allows the system to backtrack to the previous
steps of the thought-process and explore other directions from there. To verify
the effectiveness of the proposed technique, we implemented a ToT-based solver
for the Sudoku Puzzle. Experimental results show that the ToT framework can
significantly increase the success rate of Sudoku puzzle solving. Our
implementation of the ToT-based Sudoku solver is available on GitHub:
\url{<a href="https://github.com/jieyilong/tree-of-thought-puzzle-solver%7D" target="_blank" rel="noopener noreferrer">https://github.com/jieyilong/tree-of-thought-puzzle-solver}</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="plan-and-solve-prompting-improving-zero-shot-chain-of-thought-reasoning-by-large-language-models">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models<a href="#plan-and-solve-prompting-improving-zero-shot-chain-of-thought-reasoning-by-large-language-models" class="hash-link" aria-label="Direct link to Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models" title="Direct link to Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Lei Wang, Wanyu Xu, Yihuai Lan,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2305.04091v3" target="_blank" rel="noopener noreferrer">2305.04091v3</a>  <strong>Published Date:</strong> 2023-05-06</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/plan_and_execute_agent.ipynb" target="_blank" rel="noopener noreferrer">plan_and_execute_agent</a></li></ul></li></ul><p><strong>Abstract:</strong> Large language models (LLMs) have recently been shown to deliver impressive
performance in various NLP tasks. To tackle multi-step reasoning tasks,
few-shot chain-of-thought (CoT) prompting includes a few manually crafted
step-by-step reasoning demonstrations which enable LLMs to explicitly generate
reasoning steps and improve their reasoning task accuracy. To eliminate the
manual effort, Zero-shot-CoT concatenates the target problem statement with
&quot;Let&#x27;s think step by step&quot; as an input prompt to LLMs. Despite the success of
Zero-shot-CoT, it still suffers from three pitfalls: calculation errors,
missing-step errors, and semantic misunderstanding errors. To address the
missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of
two components: first, devising a plan to divide the entire task into smaller
subtasks, and then carrying out the subtasks according to the plan. To address
the calculation errors and improve the quality of generated reasoning steps, we
extend PS prompting with more detailed instructions and derive PS+ prompting.
We evaluate our proposed prompting strategy on ten datasets across three
reasoning problems. The experimental results over GPT-3 show that our proposed
zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets
by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought
Prompting, and has comparable performance with 8-shot CoT prompting on the math
reasoning problem. The code can be found at
<a href="https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting" target="_blank" rel="noopener noreferrer">https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot-listwise-document-reranking-with-a-large-language-model">Zero-Shot Listwise Document Reranking with a Large Language Model<a href="#zero-shot-listwise-document-reranking-with-a-large-language-model" class="hash-link" aria-label="Direct link to Zero-Shot Listwise Document Reranking with a Large Language Model" title="Direct link to Zero-Shot Listwise Document Reranking with a Large Language Model">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Xueguang Ma, Xinyu Zhang, Ronak Pradeep,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2305.02156v1" target="_blank" rel="noopener noreferrer">2305.02156v1</a>  <strong>Published Date:</strong> 2023-05-03</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/how_to/contextual_compression" target="_blank" rel="noopener noreferrer">docs/how_to/contextual_compression</a></li><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html#langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank" target="_blank" rel="noopener noreferrer">langchain...LLMListwiseRerank</a></li></ul></li></ul><p><strong>Abstract:</strong> Supervised ranking methods based on bi-encoder or cross-encoder architectures
have shown success in multi-stage text ranking tasks, but they require large
amounts of relevance judgments as training data. In this work, we propose
Listwise Reranker with a Large Language Model (LRL), which achieves strong
reranking effectiveness without using any task-specific training data.
Different from the existing pointwise ranking methods, where documents are
scored independently and ranked according to the scores, LRL directly generates
a reordered list of document identifiers given the candidate documents.
Experiments on three TREC web search datasets demonstrate that LRL not only
outperforms zero-shot pointwise methods when reranking first-stage retrieval
results, but can also act as a final-stage reranker to improve the top-ranked
results of a pointwise method for improved efficiency. Additionally, we apply
our approach to subsets of MIRACL, a recent multilingual retrieval dataset,
with results showing its potential to generalize across different languages.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="visual-instruction-tuning">Visual Instruction Tuning<a href="#visual-instruction-tuning" class="hash-link" aria-label="Direct link to Visual Instruction Tuning" title="Direct link to Visual Instruction Tuning">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Haotian Liu, Chunyuan Li, Qingyang Wu,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2304.08485v2" target="_blank" rel="noopener noreferrer">2304.08485v2</a>  <strong>Published Date:</strong> 2023-04-17</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb" target="_blank" rel="noopener noreferrer">Semi_structured_multi_modal_RAG_LLaMA2</a>, <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb" target="_blank" rel="noopener noreferrer">Semi_structured_and_multi_modal_RAG</a></li></ul></li></ul><p><strong>Abstract:</strong> Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="generative-agents-interactive-simulacra-of-human-behavior">Generative Agents: Interactive Simulacra of Human Behavior<a href="#generative-agents-interactive-simulacra-of-human-behavior" class="hash-link" aria-label="Direct link to Generative Agents: Interactive Simulacra of Human Behavior" title="Direct link to Generative Agents: Interactive Simulacra of Human Behavior">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Joon Sung Park, Joseph C. O&#x27;Brien, Carrie J. Cai,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2304.03442v2" target="_blank" rel="noopener noreferrer">2304.03442v2</a>  <strong>Published Date:</strong> 2023-04-07</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/generative_agents_interactive_simulacra_of_human_behavior.ipynb" target="_blank" rel="noopener noreferrer">generative_agents_interactive_simulacra_of_human_behavior</a>, <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/multiagent_bidding.ipynb" target="_blank" rel="noopener noreferrer">multiagent_bidding</a></li></ul></li></ul><p><strong>Abstract:</strong> Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent&#x27;s experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine&#x27;s
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="camel-communicative-agents-for-mind-exploration-of-large-language-model-society">CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Language Model Society<a href="#camel-communicative-agents-for-mind-exploration-of-large-language-model-society" class="hash-link" aria-label="Direct link to CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Language Model Society" title="Direct link to CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Language Model Society">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2303.17760v2" target="_blank" rel="noopener noreferrer">2303.17760v2</a>  <strong>Published Date:</strong> 2023-03-31</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/camel_role_playing.ipynb" target="_blank" rel="noopener noreferrer">camel_role_playing</a></li></ul></li></ul><p><strong>Abstract:</strong> The rapid advancement of chat-based language models has led to remarkable
progress in complex task-solving. However, their success heavily relies on
human input to guide the conversation, which can be challenging and
time-consuming. This paper explores the potential of building scalable
techniques to facilitate autonomous cooperation among communicative agents, and
provides insight into their &quot;cognitive&quot; processes. To address the challenges of
achieving autonomous cooperation, we propose a novel communicative agent
framework named role-playing. Our approach involves using inception prompting
to guide chat agents toward task completion while maintaining consistency with
human intentions. We showcase how role-playing can be used to generate
conversational data for studying the behaviors and capabilities of a society of
agents, providing a valuable resource for investigating conversational language
models. In particular, we conduct comprehensive studies on
instruction-following cooperation in multi-agent settings. Our contributions
include introducing a novel communicative agent framework, offering a scalable
approach for studying the cooperative behaviors and capabilities of multi-agent
systems, and open-sourcing our library to support research on communicative
agents and beyond: <a href="https://github.com/camel-ai/camel" target="_blank" rel="noopener noreferrer">https://github.com/camel-ai/camel</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hugginggpt-solving-ai-tasks-with-chatgpt-and-its-friends-in-hugging-face">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face<a href="#hugginggpt-solving-ai-tasks-with-chatgpt-and-its-friends-in-hugging-face" class="hash-link" aria-label="Direct link to HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face" title="Direct link to HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Yongliang Shen, Kaitao Song, Xu Tan,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2303.17580v4" target="_blank" rel="noopener noreferrer">2303.17580v4</a>  <strong>Published Date:</strong> 2023-03-30</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.autonomous_agents" target="_blank" rel="noopener noreferrer">langchain_experimental.autonomous_agents</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb" target="_blank" rel="noopener noreferrer">hugginggpt</a></li></ul></li></ul><p><strong>Abstract:</strong> Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-watermark-for-large-language-models">A Watermark for Large Language Models<a href="#a-watermark-for-large-language-models" class="hash-link" aria-label="Direct link to A Watermark for Large Language Models" title="Direct link to A Watermark for Large Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> John Kirchenbauer, Jonas Geiping, Yuxin Wen,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2301.10226v4" target="_blank" rel="noopener noreferrer">2301.10226v4</a>  <strong>Published Date:</strong> 2023-01-24</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI.html#langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI" target="_blank" rel="noopener noreferrer">langchain_community...OCIModelDeploymentTGI</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_huggingface...HuggingFaceEndpoint</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceTextGenInference</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceEndpoint</a></li></ul></li></ul><p><strong>Abstract:</strong> Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of &quot;green&quot; tokens
before a word is generated, and then softly promoting use of green tokens
during sampling. We propose a statistical test for detecting the watermark with
interpretable p-values, and derive an information-theoretic framework for
analyzing the sensitivity of the watermark. We test the watermark using a
multi-billion parameter model from the Open Pretrained Transformer (OPT)
family, and discuss robustness and security.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="precise-zero-shot-dense-retrieval-without-relevance-labels">Precise Zero-Shot Dense Retrieval without Relevance Labels<a href="#precise-zero-shot-dense-retrieval-without-relevance-labels" class="hash-link" aria-label="Direct link to Precise Zero-Shot Dense Retrieval without Relevance Labels" title="Direct link to Precise Zero-Shot Dense Retrieval without Relevance Labels">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Luyu Gao, Xueguang Ma, Jimmy Lin,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2212.10496v1" target="_blank" rel="noopener noreferrer">2212.10496v1</a>  <strong>Published Date:</strong> 2022-12-20</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html#langchain.chains.hyde.base.HypotheticalDocumentEmbedder" target="_blank" rel="noopener noreferrer">langchain...HypotheticalDocumentEmbedder</a></li><li><strong>Template:</strong> <a href="https://python.langchain.com/docs/templates/hyde" target="_blank" rel="noopener noreferrer">hyde</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb" target="_blank" rel="noopener noreferrer">hypothetical_document_embeddings</a></li></ul></li></ul><p><strong>Abstract:</strong> While dense retrieval has been shown effective and efficient across tasks and
languages, it remains difficult to create effective fully zero-shot dense
retrieval systems when no relevance label is available. In this paper, we
recognize the difficulty of zero-shot learning and encoding relevance. Instead,
we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a
query, HyDE first zero-shot instructs an instruction-following language model
(e.g. InstructGPT) to generate a hypothetical document. The document captures
relevance patterns but is unreal and may contain false details. Then, an
unsupervised contrastively learned encoder~(e.g. Contriever) encodes the
document into an embedding vector. This vector identifies a neighborhood in the
corpus embedding space, where similar real documents are retrieved based on
vector similarity. This second step ground the generated document to the actual
corpus, with the encoder&#x27;s dense bottleneck filtering out the incorrect
details. Our experiments show that HyDE significantly outperforms the
state-of-the-art unsupervised dense retriever Contriever and shows strong
performance comparable to fine-tuned retrievers, across various tasks (e.g. web
search, QA, fact verification) and languages~(e.g. sw, ko, ja).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="constitutional-ai-harmlessness-from-ai-feedback">Constitutional AI: Harmlessness from AI Feedback<a href="#constitutional-ai-harmlessness-from-ai-feedback" class="hash-link" aria-label="Direct link to Constitutional AI: Harmlessness from AI Feedback" title="Direct link to Constitutional AI: Harmlessness from AI Feedback">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Yuntao Bai, Saurav Kadavath, Sandipan Kundu,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2212.08073v1" target="_blank" rel="noopener noreferrer">2212.08073v1</a>  <strong>Published Date:</strong> 2022-12-15</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/versions/migrating_chains/constitutional_chain" target="_blank" rel="noopener noreferrer">docs/versions/migrating_chains/constitutional_chain</a></li></ul></li></ul><p><strong>Abstract:</strong> As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as &#x27;Constitutional AI&#x27;. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use &#x27;RL from AI Feedback&#x27; (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="robust-and-explainable-identification-of-logical-fallacies-in-natural-language-arguments">Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments<a href="#robust-and-explainable-identification-of-logical-fallacies-in-natural-language-arguments" class="hash-link" aria-label="Direct link to Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments" title="Direct link to Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2212.07425v3" target="_blank" rel="noopener noreferrer">2212.07425v3</a>  <strong>Published Date:</strong> 2022-12-12</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.fallacy_removal" target="_blank" rel="noopener noreferrer">langchain_experimental.fallacy_removal</a></li></ul></li></ul><p><strong>Abstract:</strong> The spread of misinformation, propaganda, and flawed argumentation has been
amplified in the Internet era. Given the volume of data and the subtlety of
identifying violations of argumentation norms, supporting information analytics
tasks, like content moderation, with trustworthy methods that can identify
logical fallacies is essential. In this paper, we formalize prior theoretical
work on logical fallacies into a comprehensive three-stage evaluation framework
of detection, coarse-grained, and fine-grained classification. We adapt
existing evaluation datasets for each stage of the evaluation. We employ three
families of robust and explainable methods based on prototype reasoning,
instance-based reasoning, and knowledge injection. The methods combine language
models with background knowledge and explainable mechanisms. Moreover, we
address data sparsity with strategies for data augmentation and curriculum
learning. Our three-stage framework natively consolidates prior datasets and
methods from existing tasks, like propaganda detection, serving as an
overarching evaluation testbed. We extensively evaluate these methods on our
datasets, focusing on their robustness and explainability. Our results provide
insight into the strengths and weaknesses of the methods on different
components and fallacy classes, indicating that fallacy identification is a
challenging task that may require specialized forms of reasoning to capture
various classes. We share our open-source code and data on GitHub to support
further work on logical fallacy identification.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="complementary-explanations-for-effective-in-context-learning">Complementary Explanations for Effective In-Context Learning<a href="#complementary-explanations-for-effective-in-context-learning" class="hash-link" aria-label="Direct link to Complementary Explanations for Effective In-Context Learning" title="Direct link to Complementary Explanations for Effective In-Context Learning">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Xi Ye, Srinivasan Iyer, Asli Celikyilmaz,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2211.13892v2" target="_blank" rel="noopener noreferrer">2211.13892v2</a>  <strong>Published Date:</strong> 2022-11-25</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/example_selectors/langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector.html#langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector" target="_blank" rel="noopener noreferrer">langchain_core...MaxMarginalRelevanceExampleSelector</a></li></ul></li></ul><p><strong>Abstract:</strong> Large language models (LLMs) have exhibited remarkable capabilities in
learning from explanations in prompts, but there has been limited understanding
of exactly how these explanations function or why they are effective. This work
aims to better understand the mechanisms by which explanations are used for
in-context learning. We first study the impact of two different factors on the
performance of prompts with explanations: the computation trace (the way the
solution is decomposed) and the natural language used to express the prompt. By
perturbing explanations on three controlled tasks, we show that both factors
contribute to the effectiveness of explanations. We further study how to form
maximally effective sets of explanations for solving a given test query. We
find that LLMs can benefit from the complementarity of the explanation set:
diverse reasoning skills shown by different exemplars can lead to better
performance. Therefore, we propose a maximal marginal relevance-based exemplar
selection approach for constructing exemplar sets that are both relevant as
well as complementary, which successfully improves the in-context learning
performance across three real-world tasks on multiple LLMs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pal-program-aided-language-models">PAL: Program-aided Language Models<a href="#pal-program-aided-language-models" class="hash-link" aria-label="Direct link to PAL: Program-aided Language Models" title="Direct link to PAL: Program-aided Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Luyu Gao, Aman Madaan, Shuyan Zhou,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2211.10435v2" target="_blank" rel="noopener noreferrer">2211.10435v2</a>  <strong>Published Date:</strong> 2022-11-18</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.pal_chain" target="_blank" rel="noopener noreferrer">langchain_experimental.pal_chain</a>, <a href="https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#langchain_experimental.pal_chain.base.PALChain" target="_blank" rel="noopener noreferrer">langchain_experimental...PALChain</a></li><li><strong>Cookbook:</strong> <a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/program_aided_language_model.ipynb" target="_blank" rel="noopener noreferrer">program_aided_language_model</a></li></ul></li></ul><p><strong>Abstract:</strong> Large language models (LLMs) have recently demonstrated an impressive ability
to perform arithmetic and symbolic reasoning tasks, when provided with a few
examples at test time (&quot;few-shot prompting&quot;). Much of this success can be
attributed to prompting methods such as &quot;chain-of-thought&#x27;&#x27;, which employ LLMs
for both understanding the problem description by decomposing it into steps, as
well as solving each step of the problem. While LLMs seem to be adept at this
sort of step-by-step decomposition, LLMs often make logical and arithmetic
mistakes in the solution part, even when the problem is decomposed correctly.
In this paper, we present Program-Aided Language models (PAL): a novel approach
that uses the LLM to read natural language problems and generate programs as
the intermediate reasoning steps, but offloads the solution step to a runtime
such as a Python interpreter. With PAL, decomposing the natural language
problem into runnable steps remains the only learning task for the LLM, while
solving is delegated to the interpreter. We demonstrate this synergy between a
neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and
algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all
these natural language reasoning tasks, generating code using an LLM and
reasoning using a Python interpreter leads to more accurate results than much
larger models. For example, PAL using Codex achieves state-of-the-art few-shot
accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B
which uses chain-of-thought by absolute 15% top-1. Our code and data are
publicly available at <a href="http://reasonwithpal.com/" target="_blank" rel="noopener noreferrer">http://reasonwithpal.com/</a> .</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="an-analysis-of-fusion-functions-for-hybrid-retrieval">An Analysis of Fusion Functions for Hybrid Retrieval<a href="#an-analysis-of-fusion-functions-for-hybrid-retrieval" class="hash-link" aria-label="Direct link to An Analysis of Fusion Functions for Hybrid Retrieval" title="Direct link to An Analysis of Fusion Functions for Hybrid Retrieval">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Sebastian Bruch, Siyu Gai, Amir Ingber</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2210.11934v2" target="_blank" rel="noopener noreferrer">2210.11934v2</a>  <strong>Published Date:</strong> 2022-10-21</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li></ul></li></ul><p><strong>Abstract:</strong> We study hybrid search in text retrieval where lexical and semantic search
are fused together with the intuition that the two are complementary in how
they model relevance. In particular, we examine fusion by a convex combination
(CC) of lexical and semantic scores, as well as the Reciprocal Rank Fusion
(RRF) method, and identify their advantages and potential pitfalls. Contrary to
existing studies, we find RRF to be sensitive to its parameters; that the
learning of a CC fusion is generally agnostic to the choice of score
normalization; that CC outperforms RRF in in-domain and out-of-domain settings;
and finally, that CC is sample efficient, requiring only a small set of
training examples to tune its only parameter to a target domain.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="react-synergizing-reasoning-and-acting-in-language-models">ReAct: Synergizing Reasoning and Acting in Language Models<a href="#react-synergizing-reasoning-and-acting-in-language-models" class="hash-link" aria-label="Direct link to ReAct: Synergizing Reasoning and Acting in Language Models" title="Direct link to ReAct: Synergizing Reasoning and Acting in Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Shunyu Yao, Jeffrey Zhao, Dian Yu,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2210.03629v3" target="_blank" rel="noopener noreferrer">2210.03629v3</a>  <strong>Published Date:</strong> 2022-10-06</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/integrations/tools/ionic_shopping" target="_blank" rel="noopener noreferrer">docs/integrations/tools/ionic_shopping</a>, <a href="https://python.langchain.com/v0.2/docs/integrations/providers/cohere" target="_blank" rel="noopener noreferrer">docs/integrations/providers/cohere</a>, <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent" target="_blank" rel="noopener noreferrer">langchain...create_react_agent</a>, <a href="https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain" target="_blank" rel="noopener noreferrer">langchain...TrajectoryEvalChain</a></li></ul></li></ul><p><strong>Abstract:</strong> While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: <a href="https://react-lm.github.io" target="_blank" rel="noopener noreferrer">https://react-lm.github.io</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deep-lake-a-lakehouse-for-deep-learning">Deep Lake: a Lakehouse for Deep Learning<a href="#deep-lake-a-lakehouse-for-deep-learning" class="hash-link" aria-label="Direct link to Deep Lake: a Lakehouse for Deep Learning" title="Direct link to Deep Lake: a Lakehouse for Deep Learning">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2209.10785v2" target="_blank" rel="noopener noreferrer">2209.10785v2</a>  <strong>Published Date:</strong> 2022-09-22</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/integrations/providers/activeloop_deeplake" target="_blank" rel="noopener noreferrer">docs/integrations/providers/activeloop_deeplake</a></li></ul></li></ul><p><strong>Abstract:</strong> Traditional data lakes provide critical data infrastructure for analytical
workloads by enabling time travel, running SQL queries, ingesting data with
ACID transactions, and visualizing petabyte-scale datasets on cloud storage.
They allow organizations to break down data silos, unlock data-driven
decision-making, improve operational efficiency, and reduce costs. However, as
deep learning usage increases, traditional data lakes are not well-designed for
applications such as natural language processing (NLP), audio processing,
computer vision, and applications involving non-tabular datasets. This paper
presents Deep Lake, an open-source lakehouse for deep learning applications
developed at Activeloop. Deep Lake maintains the benefits of a vanilla data
lake with one key difference: it stores complex data, such as images, videos,
annotations, as well as tabular data, in the form of tensors and rapidly
streams the data over the network to (a) Tensor Query Language, (b) in-browser
visualization engine, or (c) deep learning frameworks without sacrificing GPU
utilization. Datasets stored in Deep Lake can be accessed from PyTorch,
TensorFlow, JAX, and integrate with numerous MLOps tools.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="matryoshka-representation-learning">Matryoshka Representation Learning<a href="#matryoshka-representation-learning" class="hash-link" aria-label="Direct link to Matryoshka Representation Learning" title="Direct link to Matryoshka Representation Learning">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Aditya Kusupati, Gantavya Bhatt, Aniket Rege,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2205.13147v4" target="_blank" rel="noopener noreferrer">2205.13147v4</a>  <strong>Published Date:</strong> 2022-05-26</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/integrations/providers/snowflake" target="_blank" rel="noopener noreferrer">docs/integrations/providers/snowflake</a></li></ul></li></ul><p><strong>Abstract:</strong> Learned representations are a central component in modern ML systems, serving
a multitude of downstream tasks. When training such representations, it is
often the case that computational and statistical constraints for each
downstream task are unknown. In this context rigid, fixed capacity
representations can be either over or under-accommodating to the task at hand.
This leads us to ask: can we design a flexible representation that can adapt to
multiple downstream tasks with varying computational resources? Our main
contribution is Matryoshka Representation Learning (MRL) which encodes
information at different granularities and allows a single embedding to adapt
to the computational constraints of downstream tasks. MRL minimally modifies
existing representation learning pipelines and imposes no additional cost
during inference and deployment. MRL learns coarse-to-fine representations that
are at least as accurate and rich as independently trained low-dimensional
representations. The flexibility within the learned Matryoshka Representations
offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at
the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale
retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for
long-tail few-shot classification, all while being as robust as the original
representations. Finally, we show that MRL extends seamlessly to web-scale
datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),
vision + language (ALIGN) and language (BERT). MRL code and pretrained models
are open-sourced at <a href="https://github.com/RAIVNLab/MRL" target="_blank" rel="noopener noreferrer">https://github.com/RAIVNLab/MRL</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="bitext-mining-using-distilled-sentence-representations-for-low-resource-languages">Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages<a href="#bitext-mining-using-distilled-sentence-representations-for-low-resource-languages" class="hash-link" aria-label="Direct link to Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages" title="Direct link to Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Kevin Heffernan, Onur √áelebi, Holger Schwenk</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2205.12654v1" target="_blank" rel="noopener noreferrer">2205.12654v1</a>  <strong>Published Date:</strong> 2022-05-25</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.laser.LaserEmbeddings.html#langchain_community.embeddings.laser.LaserEmbeddings" target="_blank" rel="noopener noreferrer">langchain_community...LaserEmbeddings</a></li></ul></li></ul><p><strong>Abstract:</strong> Scaling multilingual representation learning beyond the hundred most frequent
languages is challenging, in particular to cover the long tail of low-resource
languages. A promising approach has been to train one-for-all multilingual
models capable of cross-lingual transfer, but these models often suffer from
insufficient capacity and interference between unrelated languages. Instead, we
move away from this approach and focus on training multiple language (family)
specific representations, but most prominently enable all languages to still be
encoded in the same representational space. To achieve this, we focus on
teacher-student training, allowing all encoders to be mutually compatible for
bitext mining, and enabling fast learning of new languages. We introduce a new
teacher-student training scheme which combines supervised and self-supervised
training, allowing encoders to take advantage of monolingual training data,
which is valuable in the low-resource setting.
Our approach significantly outperforms the original LASER encoder. We study
very low-resource languages and handle 50 African languages, many of which are
not covered by any other model. For these languages, we train sentence
encoders, mine bitexts, and validate the bitexts by training NMT systems.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating-the-text-to-sql-capabilities-of-large-language-models">Evaluating the Text-to-SQL Capabilities of Large Language Models<a href="#evaluating-the-text-to-sql-capabilities-of-large-language-models" class="hash-link" aria-label="Direct link to Evaluating the Text-to-SQL Capabilities of Large Language Models" title="Direct link to Evaluating the Text-to-SQL Capabilities of Large Language Models">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2204.00498v1" target="_blank" rel="noopener noreferrer">2204.00498v1</a>  <strong>Published Date:</strong> 2022-03-15</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/tutorials/sql_qa" target="_blank" rel="noopener noreferrer">docs/tutorials/sql_qa</a></li><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase" target="_blank" rel="noopener noreferrer">langchain_community...SQLDatabase</a>, <a href="https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.spark_sql.SparkSQL.html#langchain_community.utilities.spark_sql.SparkSQL" target="_blank" rel="noopener noreferrer">langchain_community...SparkSQL</a></li></ul></li></ul><p><strong>Abstract:</strong> We perform an empirical evaluation of Text-to-SQL capabilities of the Codex
language model. We find that, without any finetuning, Codex is a strong
baseline on the Spider benchmark; we also analyze the failure modes of Codex in
this setting. Furthermore, we demonstrate on the GeoQuery and Scholar
benchmarks that a small number of in-domain examples provided in the prompt
enables Codex to perform better than state-of-the-art models finetuned on such
few-shot examples.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="locally-typical-sampling">Locally Typical Sampling<a href="#locally-typical-sampling" class="hash-link" aria-label="Direct link to Locally Typical Sampling" title="Direct link to Locally Typical Sampling">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Clara Meister, Tiago Pimentel, Gian Wiher,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2202.00666v5" target="_blank" rel="noopener noreferrer">2202.00666v5</a>  <strong>Published Date:</strong> 2022-02-01</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_huggingface...HuggingFaceEndpoint</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceTextGenInference</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceEndpoint</a></li></ul></li></ul><p><strong>Abstract:</strong> Today&#x27;s probabilistic language generators fall short when it comes to
producing coherent and fluent text despite the fact that the underlying models
perform well under standard metrics, e.g., perplexity. This discrepancy has
puzzled the language generation community for the last few years. In this work,
we posit that the abstraction of natural language generation as a discrete
stochastic process--which allows for an information-theoretic analysis--can
provide new insights into the behavior of probabilistic language generators,
e.g., why high-probability texts can be dull or repetitive. Humans use language
as a means of communicating information, aiming to do so in a simultaneously
efficient and error-minimizing manner; in fact, psycholinguistics research
suggests humans choose each word in a string with this subconscious goal in
mind. We formally define the set of strings that meet this criterion: those for
which each word has an information content close to the expected information
content, i.e., the conditional entropy of our model. We then propose a simple
and efficient procedure for enforcing this criterion when generating from
probabilistic models, which we call locally typical sampling. Automatic and
human evaluations show that, in comparison to nucleus and top-k sampling,
locally typical sampling offers competitive performance (in both abstractive
summarization and story generation) in terms of quality while consistently
reducing degenerate repetitions.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="colbertv2-effective-and-efficient-retrieval-via-lightweight-late-interaction">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction<a href="#colbertv2-effective-and-efficient-retrieval-via-lightweight-late-interaction" class="hash-link" aria-label="Direct link to ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction" title="Direct link to ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2112.01488v3" target="_blank" rel="noopener noreferrer">2112.01488v3</a>  <strong>Published Date:</strong> 2021-12-02</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/integrations/retrievers/ragatouille" target="_blank" rel="noopener noreferrer">docs/integrations/retrievers/ragatouille</a>, <a href="https://python.langchain.com/v0.2/docs/integrations/providers/ragatouille" target="_blank" rel="noopener noreferrer">docs/integrations/providers/ragatouille</a>, <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a>, <a href="https://python.langchain.com/v0.2/docs/integrations/providers/dspy" target="_blank" rel="noopener noreferrer">docs/integrations/providers/dspy</a></li></ul></li></ul><p><strong>Abstract:</strong> Neural information retrieval (IR) has greatly advanced search and other
knowledge-intensive language tasks. While many neural IR methods encode queries
and documents into single-vector representations, late interaction models
produce multi-vector representations at the granularity of each token and
decompose relevance modeling into scalable token-level computations. This
decomposition has been shown to make late interaction more effective, but it
inflates the space footprint of these models by an order of magnitude. In this
work, we introduce ColBERTv2, a retriever that couples an aggressive residual
compression mechanism with a denoised supervision strategy to simultaneously
improve the quality and space footprint of late interaction. We evaluate
ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art
quality within and outside the training domain while reducing the space
footprint of late interaction models by 6--10$\times$.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="learning-transferable-visual-models-from-natural-language-supervision">Learning Transferable Visual Models From Natural Language Supervision<a href="#learning-transferable-visual-models-from-natural-language-supervision" class="hash-link" aria-label="Direct link to Learning Transferable Visual Models From Natural Language Supervision" title="Direct link to Learning Transferable Visual Models From Natural Language Supervision">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Alec Radford, Jong Wook Kim, Chris Hallacy,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2103.00020v1" target="_blank" rel="noopener noreferrer">2103.00020v1</a>  <strong>Published Date:</strong> 2021-02-26</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/experimental_api_reference.html#module-langchain_experimental.open_clip" target="_blank" rel="noopener noreferrer">langchain_experimental.open_clip</a></li></ul></li></ul><p><strong>Abstract:</strong> State-of-the-art computer vision systems are trained to predict a fixed set
of predetermined object categories. This restricted form of supervision limits
their generality and usability since additional labeled data is needed to
specify any other visual concept. Learning directly from raw text about images
is a promising alternative which leverages a much broader source of
supervision. We demonstrate that the simple pre-training task of predicting
which caption goes with which image is an efficient and scalable way to learn
SOTA image representations from scratch on a dataset of 400 million (image,
text) pairs collected from the internet. After pre-training, natural language
is used to reference learned visual concepts (or describe new ones) enabling
zero-shot transfer of the model to downstream tasks. We study the performance
of this approach by benchmarking on over 30 different existing computer vision
datasets, spanning tasks such as OCR, action recognition in videos,
geo-localization, and many types of fine-grained object classification. The
model transfers non-trivially to most tasks and is often competitive with a
fully supervised baseline without the need for any dataset specific training.
For instance, we match the accuracy of the original ResNet-50 on ImageNet
zero-shot without needing to use any of the 1.28 million training examples it
was trained on. We release our code and pre-trained model weights at
<a href="https://github.com/OpenAI/CLIP" target="_blank" rel="noopener noreferrer">https://github.com/OpenAI/CLIP</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="language-models-are-few-shot-learners">Language Models are Few-Shot Learners<a href="#language-models-are-few-shot-learners" class="hash-link" aria-label="Direct link to Language Models are Few-Shot Learners" title="Direct link to Language Models are Few-Shot Learners">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Tom B. Brown, Benjamin Mann, Nick Ryder,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2005.14165v4" target="_blank" rel="noopener noreferrer">2005.14165v4</a>  <strong>Published Date:</strong> 2020-05-28</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li></ul></li></ul><p><strong>Abstract:</strong> Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3&#x27;s few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks<a href="#retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks" class="hash-link" aria-label="Direct link to Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" title="Direct link to Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/2005.11401v4" target="_blank" rel="noopener noreferrer">2005.11401v4</a>  <strong>Published Date:</strong> 2020-05-22</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>Documentation:</strong> <a href="https://python.langchain.com/v0.2/docs/concepts" target="_blank" rel="noopener noreferrer">docs/concepts</a></li></ul></li></ul><p><strong>Abstract:</strong> Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when fine-tuned on
downstream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their
performance lags behind task-specific architectures. Additionally, providing
provenance for their decisions and updating their world knowledge remain open
research problems. Pre-trained models with a differentiable access mechanism to
explicit non-parametric memory can overcome this issue, but have so far been
only investigated for extractive downstream tasks. We explore a general-purpose
fine-tuning recipe for retrieval-augmented generation (RAG) -- models which
combine pre-trained parametric and non-parametric memory for language
generation. We introduce RAG models where the parametric memory is a
pre-trained seq2seq model and the non-parametric memory is a dense vector index
of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG
formulations, one which conditions on the same retrieved passages across the
whole generated sequence, the other can use different passages per token. We
fine-tune and evaluate our models on a wide range of knowledge-intensive NLP
tasks and set the state-of-the-art on three open domain QA tasks, outperforming
parametric seq2seq models and task-specific retrieve-and-extract architectures.
For language generation tasks, we find that RAG models generate more specific,
diverse and factual language than a state-of-the-art parametric-only seq2seq
baseline.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ctrl-a-conditional-transformer-language-model-for-controllable-generation">CTRL: A Conditional Transformer Language Model for Controllable Generation<a href="#ctrl-a-conditional-transformer-language-model-for-controllable-generation" class="hash-link" aria-label="Direct link to CTRL: A Conditional Transformer Language Model for Controllable Generation" title="Direct link to CTRL: A Conditional Transformer Language Model for Controllable Generation">‚Äã</a></h2><ul><li><p><strong>Authors:</strong> Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al.</p></li><li><p><strong>arXiv id:</strong> <a href="http://arxiv.org/abs/1909.05858v2" target="_blank" rel="noopener noreferrer">1909.05858v2</a>  <strong>Published Date:</strong> 2019-09-11</p></li><li><p><strong>LangChain:</strong></p><ul><li><strong>API Reference:</strong> <a href="https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_huggingface...HuggingFaceEndpoint</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceTextGenInference</a>, <a href="https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint" target="_blank" rel="noopener noreferrer">langchain_community...HuggingFaceEndpoint</a></li></ul></li></ul><p><strong>Abstract:</strong> Large-scale language models show promising text generation capabilities, but
users cannot easily control particular aspects of the generated text. We
release CTRL, a 1.63 billion-parameter conditional transformer language model,
trained to condition on control codes that govern style, content, and
task-specific behavior. Control codes were derived from structure that
naturally co-occurs with raw text, preserving the advantages of unsupervised
learning while providing more explicit control over text generation. These
codes also allow CTRL to predict which parts of the training data are most
likely given a sequence. This provides a potential method for analyzing large
amounts of data via model-based source attribution. We have released multiple
full-sized, pretrained versions of CTRL at <a href="https://github.com/salesforce/ctrl" target="_blank" rel="noopener noreferrer">https://github.com/salesforce/ctrl</a>.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/langchain-ai/langchain/edit/master/docs/docs/additional_resources/arxiv_references.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><div style="display:flex;flex-direction:column"><hr><h4>Was this page helpful?</h4><div style="display:flex;gap:5px"><div style="display:flex;align-items:center;padding-top:10px;padding-bottom:10px;padding-left:22px;padding-right:22px;border:1px solid gray;border-radius:6px;gap:10px;cursor:pointer;font-size:16px;font-weight:600" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="#166534" style="width:24px;height:24px"><path stroke-linecap="round" stroke-linejoin="round" d="M6.633 10.25c.806 0 1.533-.446 2.031-1.08a9.041 9.041 0 0 1 2.861-2.4c.723-.384 1.35-.956 1.653-1.715a4.498 4.498 0 0 0 .322-1.672V2.75a.75.75 0 0 1 .75-.75 2.25 2.25 0 0 1 2.25 2.25c0 1.152-.26 2.243-.723 3.218-.266.558.107 1.282.725 1.282m0 0h3.126c1.026 0 1.945.694 2.054 1.715.045.422.068.85.068 1.285a11.95 11.95 0 0 1-2.649 7.521c-.388.482-.987.729-1.605.729H13.48c-.483 0-.964-.078-1.423-.23l-3.114-1.04a4.501 4.501 0 0 0-1.423-.23H5.904m10.598-9.75H14.25M5.904 18.5c.083.205.173.405.27.602.197.4-.078.898-.523.898h-.908c-.889 0-1.713-.518-1.972-1.368a12 12 0 0 1-.521-3.507c0-1.553.295-3.036.831-4.398C3.387 9.953 4.167 9.5 5 9.5h1.053c.472 0 .745.556.5.96a8.958 8.958 0 0 0-1.302 4.665c0 1.194.232 2.333.654 3.375Z"></path></svg></div><div style="display:flex;align-items:center;padding-top:10px;padding-bottom:10px;padding-left:22px;padding-right:22px;border:1px solid gray;border-radius:6px;gap:10px;cursor:pointer;font-size:16px;font-weight:600" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="#991b1b" style="width:24px;height:24px"><path stroke-linecap="round" stroke-linejoin="round" d="M7.498 15.25H4.372c-1.026 0-1.945-.694-2.054-1.715a12.137 12.137 0 0 1-.068-1.285c0-2.848.992-5.464 2.649-7.521C5.287 4.247 5.886 4 6.504 4h4.016a4.5 4.5 0 0 1 1.423.23l3.114 1.04a4.5 4.5 0 0 0 1.423.23h1.294M7.498 15.25c.618 0 .991.724.725 1.282A7.471 7.471 0 0 0 7.5 19.75 2.25 2.25 0 0 0 9.75 22a.75.75 0 0 0 .75-.75v-.633c0-.573.11-1.14.322-1.672.304-.76.93-1.33 1.653-1.715a9.04 9.04 0 0 0 2.86-2.4c.498-.634 1.226-1.08 2.032-1.08h.384m-10.253 1.5H9.7m8.075-9.75c.01.05.027.1.05.148.593 1.2.925 2.55.925 3.977 0 1.487-.36 2.89-.999 4.125m.023-8.25c-.076-.365.183-.75.575-.75h.908c.889 0 1.713.518 1.972 1.368.339 1.11.521 2.287.521 3.507 0 1.553-.295 3.036-.831 4.398-.306.774-1.086 1.227-1.918 1.227h-1.053c-.472 0-.745-.556-.5-.96a8.95 8.95 0 0 0 .303-.54"></path></svg></div></div><br><h4>You can also leave detailed feedback<!-- --> <a target="_blank" href="https://github.com/langchain-ai/langchain/issues/new?assignees=&amp;labels=03+-+Documentation&amp;projects=&amp;template=documentation.yml&amp;title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E">on GitHub</a>.</h4></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#adaptive-rag-learning-to-adapt-retrieval-augmented-large-language-models-through-question-complexity" class="table-of-contents__link toc-highlight">Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</a></li><li><a href="#self-discover-large-language-models-self-compose-reasoning-structures" class="table-of-contents__link toc-highlight">Self-Discover: Large Language Models Self-Compose Reasoning Structures</a></li><li><a href="#rag-fusion-a-new-take-on-retrieval-augmented-generation" class="table-of-contents__link toc-highlight">RAG-Fusion: a New Take on Retrieval-Augmented Generation</a></li><li><a href="#raptor-recursive-abstractive-processing-for-tree-organized-retrieval" class="table-of-contents__link toc-highlight">RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</a></li><li><a href="#corrective-retrieval-augmented-generation" class="table-of-contents__link toc-highlight">Corrective Retrieval Augmented Generation</a></li><li><a href="#code-generation-with-alphacodium-from-prompt-engineering-to-flow-engineering" class="table-of-contents__link toc-highlight">Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</a></li><li><a href="#mixtral-of-experts" class="table-of-contents__link toc-highlight">Mixtral of Experts</a></li><li><a href="#dense-x-retrieval-what-retrieval-granularity-should-we-use" class="table-of-contents__link toc-highlight">Dense X Retrieval: What Retrieval Granularity Should We Use?</a></li><li><a href="#chain-of-note-enhancing-robustness-in-retrieval-augmented-language-models" class="table-of-contents__link toc-highlight">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a></li><li><a href="#self-rag-learning-to-retrieve-generate-and-critique-through-self-reflection" class="table-of-contents__link toc-highlight">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</a></li><li><a href="#take-a-step-back-evoking-reasoning-via-abstraction-in-large-language-models" class="table-of-contents__link toc-highlight">Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</a></li><li><a href="#skeleton-of-thought-prompting-llms-for-efficient-parallel-generation" class="table-of-contents__link toc-highlight">Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation</a></li><li><a href="#llama-2-open-foundation-and-fine-tuned-chat-models" class="table-of-contents__link toc-highlight">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li><li><a href="#lost-in-the-middle-how-language-models-use-long-contexts" class="table-of-contents__link toc-highlight">Lost in the Middle: How Language Models Use Long Contexts</a></li><li><a href="#query-rewriting-for-retrieval-augmented-large-language-models" class="table-of-contents__link toc-highlight">Query Rewriting for Retrieval-Augmented Large Language Models</a></li><li><a href="#large-language-model-guided-tree-of-thought" class="table-of-contents__link toc-highlight">Large Language Model Guided Tree-of-Thought</a></li><li><a href="#plan-and-solve-prompting-improving-zero-shot-chain-of-thought-reasoning-by-large-language-models" class="table-of-contents__link toc-highlight">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</a></li><li><a href="#zero-shot-listwise-document-reranking-with-a-large-language-model" class="table-of-contents__link toc-highlight">Zero-Shot Listwise Document Reranking with a Large Language Model</a></li><li><a href="#visual-instruction-tuning" class="table-of-contents__link toc-highlight">Visual Instruction Tuning</a></li><li><a href="#generative-agents-interactive-simulacra-of-human-behavior" class="table-of-contents__link toc-highlight">Generative Agents: Interactive Simulacra of Human Behavior</a></li><li><a href="#camel-communicative-agents-for-mind-exploration-of-large-language-model-society" class="table-of-contents__link toc-highlight">CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Language Model Society</a></li><li><a href="#hugginggpt-solving-ai-tasks-with-chatgpt-and-its-friends-in-hugging-face" class="table-of-contents__link toc-highlight">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></li><li><a href="#a-watermark-for-large-language-models" class="table-of-contents__link toc-highlight">A Watermark for Large Language Models</a></li><li><a href="#precise-zero-shot-dense-retrieval-without-relevance-labels" class="table-of-contents__link toc-highlight">Precise Zero-Shot Dense Retrieval without Relevance Labels</a></li><li><a href="#constitutional-ai-harmlessness-from-ai-feedback" class="table-of-contents__link toc-highlight">Constitutional AI: Harmlessness from AI Feedback</a></li><li><a href="#robust-and-explainable-identification-of-logical-fallacies-in-natural-language-arguments" class="table-of-contents__link toc-highlight">Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments</a></li><li><a href="#complementary-explanations-for-effective-in-context-learning" class="table-of-contents__link toc-highlight">Complementary Explanations for Effective In-Context Learning</a></li><li><a href="#pal-program-aided-language-models" class="table-of-contents__link toc-highlight">PAL: Program-aided Language Models</a></li><li><a href="#an-analysis-of-fusion-functions-for-hybrid-retrieval" class="table-of-contents__link toc-highlight">An Analysis of Fusion Functions for Hybrid Retrieval</a></li><li><a href="#react-synergizing-reasoning-and-acting-in-language-models" class="table-of-contents__link toc-highlight">ReAct: Synergizing Reasoning and Acting in Language Models</a></li><li><a href="#deep-lake-a-lakehouse-for-deep-learning" class="table-of-contents__link toc-highlight">Deep Lake: a Lakehouse for Deep Learning</a></li><li><a href="#matryoshka-representation-learning" class="table-of-contents__link toc-highlight">Matryoshka Representation Learning</a></li><li><a href="#bitext-mining-using-distilled-sentence-representations-for-low-resource-languages" class="table-of-contents__link toc-highlight">Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages</a></li><li><a href="#evaluating-the-text-to-sql-capabilities-of-large-language-models" class="table-of-contents__link toc-highlight">Evaluating the Text-to-SQL Capabilities of Large Language Models</a></li><li><a href="#locally-typical-sampling" class="table-of-contents__link toc-highlight">Locally Typical Sampling</a></li><li><a href="#colbertv2-effective-and-efficient-retrieval-via-lightweight-late-interaction" class="table-of-contents__link toc-highlight">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a></li><li><a href="#learning-transferable-visual-models-from-natural-language-supervision" class="table-of-contents__link toc-highlight">Learning Transferable Visual Models From Natural Language Supervision</a></li><li><a href="#language-models-are-few-shot-learners" class="table-of-contents__link toc-highlight">Language Models are Few-Shot Learners</a></li><li><a href="#retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks" class="table-of-contents__link toc-highlight">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li><li><a href="#ctrl-a-conditional-transformer-language-model-for-controllable-generation" class="table-of-contents__link toc-highlight">CTRL: A Conditional Transformer Language Model for Controllable Generation</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/LangChainAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">GitHub</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/langchain-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">Organization<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="footer__link-item">Python<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/langchain-ai/langchainjs" target="_blank" rel="noopener noreferrer" class="footer__link-item">JS/TS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://langchain.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Homepage<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://blog.langchain.dev" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/@LangChain" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2025 LangChain, Inc.</div></div></div></footer></div>
<script src="/v0.2/assets/js/runtime~main.238fea3b.js"></script>
<script src="/v0.2/assets/js/main.34559fbe.js"></script>
</body>
</html>