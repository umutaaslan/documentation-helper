
<!DOCTYPE html>

<html data-content_root="../../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F"></script>
<script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-9B66JQQH2F');
    </script>
<title>langchain_openai.chat_models.base â€” ðŸ¦œðŸ”— LangChain  documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!--
    this give us a css class that will be invisible only if js is disabled
  -->
<noscript>
<style>
      .pst-js-only { display: none !important; }

    </style>
</noscript>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="../../../_static/pygments.css?v=8f2a1f02" rel="stylesheet" type="text/css"/>
<link href="../../../_static/autodoc_pydantic.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="../../../_static/css/custom.css?v=8e9fa5b3" rel="stylesheet" type="text/css"/>
<!-- So that users can add custom icons -->
<script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" rel="preload"/>
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" rel="preload"/>
<script src="../../../_static/documentation_options.js?v=3b5cce75"></script>
<script src="../../../_static/doctools.js?v=9bcbadda"></script>
<script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../../_static/copybutton.js?v=f281be69"></script>
<script src="../../../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = '_modules/langchain_openai/chat_models/base';</script>
<link href="../../../_static/favicon.png" rel="icon"/>
<link href="../../../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="" name="docsearch:version"/>
<meta content="Jul 10, 2025" name="docbuild:last-update"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<dialog id="pst-search-dialog">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</dialog>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../../../index.html">
<img alt="ðŸ¦œðŸ”— LangChain  documentation - Home" class="logo__image only-light" src="../../../_static/wordmark-api.svg"/>
<img alt="ðŸ¦œðŸ”— LangChain  documentation - Home" class="logo__image only-dark pst-js-only" src="../../../_static/wordmark-api-dark.svg"/>
</a></div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../reference.html">
    Reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item navbar-persistent--container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<button aria-label="Color mode" class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" data-bs-placement="bottom" data-bs-title="Color mode" data-bs-toggle="tooltip">
<i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light" title="Light"></i>
<i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark" title="Dark"></i>
<i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto" title="System Settings"></i>
</button></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="navbar-persistent--mobile">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<dialog id="pst-primary-sidebar-modal"></dialog>
<div class="bd-sidebar-primary bd-sidebar hide-on-wide" id="pst-primary-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../reference.html">
    Reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<button aria-label="Color mode" class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" data-bs-placement="bottom" data-bs-title="Color mode" data-bs-toggle="tooltip">
<i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light" title="Light"></i>
<i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark" title="Dark"></i>
<i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto" title="System Settings"></i>
</button></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
<div class="sidebar-primary-item">
<div class="flat" data-ea-manual="true" data-ea-publisher="readthedocs" data-ea-type="readthedocs-sidebar" id="ethical-ad-placement">
</div></div>
</div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../../index.html">Module code</a></li>
<li aria-current="page" class="breadcrumb-item active"><span class="ellipsis">langchain_openai.chat_models.base</span></li>
</ul>
</nav>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<h1>Source code for langchain_openai.chat_models.base</h1><div class="highlight"><pre>
<span></span><span class="sd">"""OpenAI chat wrapper."""</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">base64</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ssl</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncIterator</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">io</span><span class="w"> </span><span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">json</span><span class="w"> </span><span class="kn">import</span> <span class="n">JSONDecodeError</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">ceil</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">operator</span><span class="w"> </span><span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Literal</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">TypedDict</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">urllib.parse</span><span class="w"> </span><span class="kn">import</span> <span class="n">urlparse</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">certifi</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core._api.deprecation</span><span class="w"> </span><span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.callbacks</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">,</span>
    <span class="n">CallbackManagerForLLMRun</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.language_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">LanguageModelInput</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.language_models.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseChatModel</span><span class="p">,</span>
    <span class="n">LangSmithParams</span><span class="p">,</span>
    <span class="n">agenerate_from_stream</span><span class="p">,</span>
    <span class="n">generate_from_stream</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AIMessage</span><span class="p">,</span>
    <span class="n">AIMessageChunk</span><span class="p">,</span>
    <span class="n">BaseMessage</span><span class="p">,</span>
    <span class="n">BaseMessageChunk</span><span class="p">,</span>
    <span class="n">ChatMessage</span><span class="p">,</span>
    <span class="n">ChatMessageChunk</span><span class="p">,</span>
    <span class="n">FunctionMessage</span><span class="p">,</span>
    <span class="n">FunctionMessageChunk</span><span class="p">,</span>
    <span class="n">HumanMessage</span><span class="p">,</span>
    <span class="n">HumanMessageChunk</span><span class="p">,</span>
    <span class="n">InvalidToolCall</span><span class="p">,</span>
    <span class="n">SystemMessage</span><span class="p">,</span>
    <span class="n">SystemMessageChunk</span><span class="p">,</span>
    <span class="n">ToolCall</span><span class="p">,</span>
    <span class="n">ToolMessage</span><span class="p">,</span>
    <span class="n">ToolMessageChunk</span><span class="p">,</span>
    <span class="n">convert_to_openai_data_block</span><span class="p">,</span>
    <span class="n">is_data_content_block</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages.ai</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">InputTokenDetails</span><span class="p">,</span>
    <span class="n">OutputTokenDetails</span><span class="p">,</span>
    <span class="n">UsageMetadata</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages.tool</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool_call_chunk</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">JsonOutputParser</span><span class="p">,</span> <span class="n">PydanticOutputParser</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.output_parsers.openai_tools</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">JsonOutputKeyToolsParser</span><span class="p">,</span>
    <span class="n">PydanticToolsParser</span><span class="p">,</span>
    <span class="n">make_invalid_tool_call</span><span class="p">,</span>
    <span class="n">parse_tool_call</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatGeneration</span><span class="p">,</span> <span class="n">ChatGenerationChunk</span><span class="p">,</span> <span class="n">ChatResult</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Runnable</span><span class="p">,</span>
    <span class="n">RunnableLambda</span><span class="p">,</span>
    <span class="n">RunnableMap</span><span class="p">,</span>
    <span class="n">RunnablePassthrough</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">run_in_executor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseTool</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">_stringify</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_pydantic_field_names</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.utils.function_calling</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_to_openai_function</span><span class="p">,</span>
    <span class="n">convert_to_openai_tool</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.utils.pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">PydanticBaseModel</span><span class="p">,</span>
    <span class="n">TypeBaseModel</span><span class="p">,</span>
    <span class="n">is_basemodel_subclass</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.utils.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_build_model_kwargs</span><span class="p">,</span> <span class="n">from_env</span><span class="p">,</span> <span class="n">secret_from_env</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">ConfigDict</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">SecretStr</span><span class="p">,</span> <span class="n">model_validator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic.v1</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span> <span class="k">as</span> <span class="n">BaseModelV1</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">Self</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai.chat_models._client_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_default_async_httpx_client</span><span class="p">,</span>
    <span class="n">_get_default_httpx_client</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai.chat_models._compat</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_convert_from_v03_ai_message</span><span class="p">,</span>
    <span class="n">_convert_to_v03_ai_message</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">openai.types.responses</span><span class="w"> </span><span class="kn">import</span> <span class="n">Response</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># This SSL context is equivelent to the default `verify=True`.</span>
<span class="c1"># https://www.python-httpx.org/advanced/ssl/#configuring-client-instances</span>
<span class="n">global_ssl_context</span> <span class="o">=</span> <span class="n">ssl</span><span class="o">.</span><span class="n">create_default_context</span><span class="p">(</span><span class="n">cafile</span><span class="o">=</span><span class="n">certifi</span><span class="o">.</span><span class="n">where</span><span class="p">())</span>

<span class="n">WellKnownTools</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">"file_search"</span><span class="p">,</span>
    <span class="s2">"web_search_preview"</span><span class="p">,</span>
    <span class="s2">"computer_use_preview"</span><span class="p">,</span>
    <span class="s2">"code_interpreter"</span><span class="p">,</span>
    <span class="s2">"mcp"</span><span class="p">,</span>
    <span class="s2">"image_generation"</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_convert_dict_to_message</span><span class="p">(</span><span class="n">_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">BaseMessage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Convert a dictionary to a LangChain message.</span>

<span class="sd">    Args:</span>
<span class="sd">        _dict: The dictionary.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The LangChain message.</span>
<span class="sd">    """</span>
    <span class="n">role</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"role"</span><span class="p">)</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">)</span>
    <span class="n">id_</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"user"</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"assistant"</span><span class="p">:</span>
        <span class="c1"># Fix for azure</span>
        <span class="c1"># Also OpenAI returns None for tool invocations</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">""</span>
        <span class="n">additional_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">function_call</span> <span class="o">:=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"function_call"</span><span class="p">):</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">function_call</span><span class="p">)</span>
        <span class="n">tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">invalid_tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">raw_tool_calls</span> <span class="o">:=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tool_calls"</span><span class="p">):</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw_tool_calls</span>
            <span class="k">for</span> <span class="n">raw_tool_call</span> <span class="ow">in</span> <span class="n">raw_tool_calls</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">tool_calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parse_tool_call</span><span class="p">(</span><span class="n">raw_tool_call</span><span class="p">,</span> <span class="n">return_id</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">invalid_tool_calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">make_invalid_tool_call</span><span class="p">(</span><span class="n">raw_tool_call</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="p">)</span>
        <span class="k">if</span> <span class="n">audio</span> <span class="o">:=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"audio"</span><span class="p">):</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"audio"</span><span class="p">]</span> <span class="o">=</span> <span class="n">audio</span>
        <span class="k">return</span> <span class="n">AIMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span>
            <span class="n">tool_calls</span><span class="o">=</span><span class="n">tool_calls</span><span class="p">,</span>
            <span class="n">invalid_tool_calls</span><span class="o">=</span><span class="n">invalid_tool_calls</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"system"</span><span class="p">,</span> <span class="s2">"developer"</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"developer"</span><span class="p">:</span>
            <span class="n">additional_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"__openai_role__"</span><span class="p">:</span> <span class="n">role</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">additional_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">return</span> <span class="n">SystemMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"function"</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">FunctionMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">)),</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"tool"</span><span class="p">:</span>
        <span class="n">additional_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">"name"</span> <span class="ow">in</span> <span class="n">_dict</span><span class="p">:</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dict</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">ToolMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span>
            <span class="n">tool_call_id</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tool_call_id"</span><span class="p">)),</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_format_message_content</span><span class="p">(</span><span class="n">content</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Format message content."""</span>
    <span class="k">if</span> <span class="n">content</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">formatted_content</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">content</span><span class="p">:</span>
            <span class="c1"># Remove unexpected block types</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="ow">and</span> <span class="s2">"type"</span> <span class="ow">in</span> <span class="n">block</span>
                <span class="ow">and</span> <span class="n">block</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"tool_use"</span><span class="p">,</span> <span class="s2">"thinking"</span><span class="p">,</span> <span class="s2">"reasoning_content"</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_data_content_block</span><span class="p">(</span><span class="n">block</span><span class="p">):</span>
                <span class="n">formatted_content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">convert_to_openai_data_block</span><span class="p">(</span><span class="n">block</span><span class="p">))</span>
            <span class="c1"># Anthropic image blocks</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">block</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"image"</span>
                <span class="ow">and</span> <span class="p">(</span><span class="n">source</span> <span class="o">:=</span> <span class="n">block</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"source"</span><span class="p">))</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">source</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"base64"</span> <span class="ow">and</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">media_type</span> <span class="o">:=</span> <span class="n">source</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"media_type"</span><span class="p">))</span>
                    <span class="ow">and</span> <span class="p">(</span><span class="n">data</span> <span class="o">:=</span> <span class="n">source</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"data"</span><span class="p">))</span>
                <span class="p">):</span>
                    <span class="n">formatted_content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"image_url"</span><span class="p">,</span>
                            <span class="s2">"image_url"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"url"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"data:</span><span class="si">{</span><span class="n">media_type</span><span class="si">}</span><span class="s2">;base64,</span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">"</span><span class="p">},</span>
                        <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">source</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"url"</span> <span class="ow">and</span> <span class="p">(</span><span class="n">url</span> <span class="o">:=</span> <span class="n">source</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"url"</span><span class="p">)):</span>
                    <span class="n">formatted_content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"image_url"</span><span class="p">,</span> <span class="s2">"image_url"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"url"</span><span class="p">:</span> <span class="n">url</span><span class="p">}}</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">formatted_content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">formatted_content</span> <span class="o">=</span> <span class="n">content</span>

    <span class="k">return</span> <span class="n">formatted_content</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_convert_message_to_dict</span><span class="p">(</span><span class="n">message</span><span class="p">:</span> <span class="n">BaseMessage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Convert a LangChain message to a dictionary.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: The LangChain message.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The dictionary.</span>
<span class="sd">    """</span>
    <span class="n">message_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">_format_message_content</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="o">:=</span> <span class="n">message</span><span class="o">.</span><span class="n">name</span> <span class="ow">or</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">))</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span>

    <span class="c1"># populate role and additional message data</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">ChatMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">role</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"user"</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"assistant"</span>
        <span class="k">if</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span> <span class="ow">or</span> <span class="n">message</span><span class="o">.</span><span class="n">invalid_tool_calls</span><span class="p">:</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">_lc_tool_call_to_openai_tool_call</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span> <span class="k">for</span> <span class="n">tc</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span>
            <span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
                <span class="n">_lc_invalid_tool_call_to_openai_tool_call</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">tc</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">invalid_tool_calls</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="s2">"tool_calls"</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span>
            <span class="n">tool_call_supported_props</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"id"</span><span class="p">,</span> <span class="s2">"type"</span><span class="p">,</span> <span class="s2">"function"</span><span class="p">}</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tool_call</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">tool_call_supported_props</span><span class="p">}</span>
                <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="s2">"function_call"</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
            <span class="c1"># OpenAI raises 400 if both function_call and tool_calls are present in the</span>
            <span class="c1"># same message.</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="c1"># If tool calls present, content null value should be None not empty string.</span>
        <span class="k">if</span> <span class="s2">"function_call"</span> <span class="ow">in</span> <span class="n">message_dict</span> <span class="ow">or</span> <span class="s2">"tool_calls"</span> <span class="ow">in</span> <span class="n">message_dict</span><span class="p">:</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message_dict</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span> <span class="ow">or</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="s2">"audio"</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
            <span class="c1"># openai doesn't support passing the data back - only the id</span>
            <span class="c1"># https://platform.openai.com/docs/guides/audio/multi-turn-conversations</span>
            <span class="n">raw_audio</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"audio"</span><span class="p">]</span>
            <span class="n">audio</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">{</span><span class="s2">"id"</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"audio"</span><span class="p">][</span><span class="s2">"id"</span><span class="p">]}</span>
                <span class="k">if</span> <span class="s2">"id"</span> <span class="ow">in</span> <span class="n">raw_audio</span>
                <span class="k">else</span> <span class="n">raw_audio</span>
            <span class="p">)</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"audio"</span><span class="p">]</span> <span class="o">=</span> <span class="n">audio</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"__openai_role__"</span><span class="p">,</span> <span class="s2">"system"</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">FunctionMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"function"</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">ToolMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"tool"</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_call_id"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_call_id</span>

        <span class="n">supported_props</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">"role"</span><span class="p">,</span> <span class="s2">"tool_call_id"</span><span class="p">}</span>
        <span class="n">message_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">message_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">supported_props</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Got unknown type </span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">message_dict</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_convert_delta_to_message_chunk</span><span class="p">(</span>
    <span class="n">_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">default_class</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">BaseMessageChunk</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseMessageChunk</span><span class="p">:</span>
    <span class="n">id_</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">)</span>
    <span class="n">role</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"role"</span><span class="p">))</span>
    <span class="n">content</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">""</span><span class="p">)</span>
    <span class="n">additional_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"function_call"</span><span class="p">):</span>
        <span class="n">function_call</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_dict</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">])</span>
        <span class="k">if</span> <span class="s2">"name"</span> <span class="ow">in</span> <span class="n">function_call</span> <span class="ow">and</span> <span class="n">function_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">function_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">""</span>
        <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span> <span class="o">=</span> <span class="n">function_call</span>
    <span class="n">tool_call_chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">raw_tool_calls</span> <span class="o">:=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tool_calls"</span><span class="p">):</span>
        <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw_tool_calls</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">tool_call_chunks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">tool_call_chunk</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="n">rtc</span><span class="p">[</span><span class="s2">"function"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">),</span>
                    <span class="n">args</span><span class="o">=</span><span class="n">rtc</span><span class="p">[</span><span class="s2">"function"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"arguments"</span><span class="p">),</span>
                    <span class="nb">id</span><span class="o">=</span><span class="n">rtc</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">),</span>
                    <span class="n">index</span><span class="o">=</span><span class="n">rtc</span><span class="p">[</span><span class="s2">"index"</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">rtc</span> <span class="ow">in</span> <span class="n">raw_tool_calls</span>
            <span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"user"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">HumanMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">HumanMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"assistant"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">AIMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">AIMessageChunk</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span>
            <span class="n">tool_call_chunks</span><span class="o">=</span><span class="n">tool_call_chunks</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"system"</span><span class="p">,</span> <span class="s2">"developer"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">SystemMessageChunk</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"developer"</span><span class="p">:</span>
            <span class="n">additional_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"__openai_role__"</span><span class="p">:</span> <span class="s2">"developer"</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">additional_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">return</span> <span class="n">SystemMessageChunk</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span> <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"function"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">FunctionMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">FunctionMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">_dict</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"tool"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">ToolMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ToolMessageChunk</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">_dict</span><span class="p">[</span><span class="s2">"tool_call_id"</span><span class="p">],</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">ChatMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ChatMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_class</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>  <span class="c1"># type: ignore</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_update_token_usage</span><span class="p">(</span>
    <span class="n">overall_token_usage</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">new_usage</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
    <span class="c1"># Token usage is either ints or dictionaries</span>
    <span class="c1"># `reasoning_tokens` is nested inside `completion_tokens_details`</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_usage</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">overall_token_usage</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Got different types for token usage: "</span>
                <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">new_usage</span><span class="p">)</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">overall_token_usage</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">new_usage</span> <span class="o">+</span> <span class="n">overall_token_usage</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_usage</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">overall_token_usage</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Got different types for token usage: "</span>
                <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">new_usage</span><span class="p">)</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">overall_token_usage</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">_update_token_usage</span><span class="p">(</span><span class="n">overall_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">new_usage</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Unexpected type for token usage: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">new_usage</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_usage</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_handle_openai_bad_request</span><span class="p">(</span><span class="n">e</span><span class="p">:</span> <span class="n">openai</span><span class="o">.</span><span class="n">BadRequestError</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="s2">"'response_format' of type 'json_schema' is not supported with this model"</span>
    <span class="p">)</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">"This model does not support OpenAI's structured output feature, which "</span>
            <span class="s2">"is the default method for `with_structured_output` as of "</span>
            <span class="s2">"langchain-openai==0.3. To use `with_structured_output` with this model, "</span>
            <span class="s1">'specify `method="function_calling"`.'</span>
        <span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">e</span>
    <span class="k">elif</span> <span class="s2">"Invalid schema for response_format"</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">"Invalid schema for OpenAI's structured output feature, which is the "</span>
            <span class="s2">"default method for `with_structured_output` as of langchain-openai==0.3. "</span>
            <span class="s1">'Specify `method="function_calling"` instead or update your schema. '</span>
            <span class="s2">"See supported schemas: "</span>
            <span class="s2">"https://platform.openai.com/docs/guides/structured-outputs#supported-schemas"</span>  <span class="c1"># noqa: E501</span>
        <span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">e</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_FunctionCall</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>


<span class="n">_BM</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">"_BM"</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="n">BaseModel</span><span class="p">)</span>
<span class="n">_DictOrPydanticClass</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="nb">type</span><span class="p">[</span><span class="n">_BM</span><span class="p">],</span> <span class="nb">type</span><span class="p">]</span>
<span class="n">_DictOrPydantic</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">_BM</span><span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_AllReturnType</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">raw</span><span class="p">:</span> <span class="n">BaseMessage</span>
    <span class="n">parsed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_DictOrPydantic</span><span class="p">]</span>
    <span class="n">parsing_error</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">BaseException</span><span class="p">]</span>


<div class="viewcode-block" id="BaseChatOpenAI">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_cerebras.chat_models.BaseChatOpenAI">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">BaseChatOpenAI</span><span class="p">(</span><span class="n">BaseChatModel</span><span class="p">):</span>
    <span class="n">client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">async_client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">root_client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">root_async_client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">"gpt-3.5-turbo"</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"model"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Model name to use."""</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""What sampling temperature to use."""</span>
    <span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Holds any model parameters valid for `create` call not explicitly specified."""</span>
    <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SecretStr</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">alias</span><span class="o">=</span><span class="s2">"api_key"</span><span class="p">,</span> <span class="n">default_factory</span><span class="o">=</span><span class="n">secret_from_env</span><span class="p">(</span><span class="s2">"OPENAI_API_KEY"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">openai_api_base</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"base_url"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Base URL path for API requests, leave blank if not using a proxy or service </span>
<span class="sd">        emulator."""</span>
    <span class="n">openai_organization</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"organization"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Automatically inferred from env var `OPENAI_ORG_ID` if not provided."""</span>
    <span class="c1"># to support explicit proxy for OpenAI</span>
    <span class="n">openai_proxy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">from_env</span><span class="p">(</span><span class="s2">"OPENAI_PROXY"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">request_timeout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">Any</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"timeout"</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">"""Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or </span>
<span class="sd">        None."""</span>
    <span class="n">stream_usage</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""Whether to include usage metadata in streaming output. If True, an additional</span>
<span class="sd">    message chunk will be generated during the stream including usage metadata.</span>

<span class="sd">    .. versionadded:: 0.3.9</span>
<span class="sd">    """</span>
    <span class="n">max_retries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Maximum number of retries to make when generating."""</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Penalizes repeated tokens."""</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Penalizes repeated tokens according to frequency."""</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Seed for generation"""</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Whether to return logprobs."""</span>
    <span class="n">top_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Number of most likely tokens to return at each token position, each with</span>
<span class="sd">     an associated log probability. `logprobs` must be set to true </span>
<span class="sd">     if this parameter is used."""</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Modify the likelihood of specified tokens appearing in the completion."""</span>
    <span class="n">streaming</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""Whether to stream the results or not."""</span>
    <span class="n">n</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Number of chat completions to generate for each prompt."""</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Total probability mass of tokens to consider at each step."""</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Maximum number of tokens to generate."""</span>
    <span class="n">reasoning_effort</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Constrains effort on reasoning for reasoning models. For use with the Chat</span>
<span class="sd">    Completions API.</span>

<span class="sd">    Reasoning models only, like OpenAI o1, o3, and o4-mini.</span>

<span class="sd">    Currently supported values are low, medium, and high. Reducing reasoning effort </span>
<span class="sd">    can result in faster responses and fewer tokens used on reasoning in a response.</span>

<span class="sd">    .. versionadded:: 0.2.14</span>
<span class="sd">    """</span>
    <span class="n">reasoning</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Reasoning parameters for reasoning models, i.e., OpenAI o-series models (o1, o3,</span>
<span class="sd">    o4-mini, etc.). For use with the Responses API.</span>

<span class="sd">    Example:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        reasoning={</span>
<span class="sd">            "effort": "medium",  # can be "low", "medium", or "high"</span>
<span class="sd">            "summary": "auto",  # can be "auto", "concise", or "detailed"</span>
<span class="sd">        }</span>

<span class="sd">    .. versionadded:: 0.3.24</span>
<span class="sd">    """</span>
    <span class="n">tiktoken_model_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""The model name to pass to tiktoken when using this class. </span>
<span class="sd">    Tiktoken is used to count the number of tokens in documents to constrain </span>
<span class="sd">    them to be under a certain limit. By default, when set to None, this will </span>
<span class="sd">    be the same as the embedding model name. However, there are some cases </span>
<span class="sd">    where you may want to use this Embedding class with a model name not </span>
<span class="sd">    supported by tiktoken. This can include when using Azure embeddings or </span>
<span class="sd">    when using one of the many model providers that expose an OpenAI-like </span>
<span class="sd">    API but with different models. In those cases, in order to avoid erroring </span>
<span class="sd">    when tiktoken is called, you can specify a model name to use here."""</span>
    <span class="n">default_headers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">default_query</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Configure a custom httpx client. See the</span>
    <span class="c1"># [httpx documentation](https://www.python-httpx.org/api/#client) for more details.</span>
    <span class="n">http_client</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Optional ``httpx.Client``. Only used for sync invocations. Must specify </span>
<span class="sd">        ``http_async_client`` as well if you'd like a custom client for async</span>
<span class="sd">        invocations.</span>
<span class="sd">    """</span>
    <span class="n">http_async_client</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Optional httpx.AsyncClient. Only used for async invocations. Must specify </span>
<span class="sd">        ``http_client`` as well if you'd like a custom client for sync invocations."""</span>
    <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"stop_sequences"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Default stop sequences."""</span>
    <span class="n">extra_body</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Optional additional JSON properties to include in the request parameters when</span>
<span class="sd">    making requests to OpenAI compatible APIs, such as vLLM."""</span>
    <span class="n">include_response_headers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""Whether to include response headers in the output message response_metadata."""</span>
    <span class="n">disabled_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Parameters of the OpenAI client or chat.completions endpoint that should be </span>
<span class="sd">    disabled for the given model.</span>
<span class="sd">    </span>
<span class="sd">    Should be specified as ``{"param": None | ['val1', 'val2']}`` where the key is the </span>
<span class="sd">    parameter and the value is either None, meaning that parameter should never be</span>
<span class="sd">    used, or it's a list of disabled values for the parameter.</span>
<span class="sd">    </span>
<span class="sd">    For example, older models may not support the 'parallel_tool_calls' parameter at </span>
<span class="sd">    all, in which case ``disabled_params={"parallel_tool_calls": None}`` can be passed </span>
<span class="sd">    in.</span>
<span class="sd">    </span>
<span class="sd">    If a parameter is disabled then it will not be used by default in any methods, e.g.</span>
<span class="sd">    in :meth:`~langchain_openai.chat_models.base.ChatOpenAI.with_structured_output`.</span>
<span class="sd">    However this does not prevent a user from directly passed in the parameter during</span>
<span class="sd">    invocation. </span>
<span class="sd">    """</span>

    <span class="n">include</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Additional fields to include in generations from Responses API.</span>

<span class="sd">    Supported values:</span>

<span class="sd">    - ``"file_search_call.results"``</span>
<span class="sd">    - ``"message.input_image.image_url"``</span>
<span class="sd">    - ``"computer_call_output.output.image_url"``</span>
<span class="sd">    - ``"reasoning.encrypted_content"``</span>
<span class="sd">    - ``"code_interpreter_call.outputs"``</span>

<span class="sd">    .. versionadded:: 0.3.24</span>
<span class="sd">    """</span>

    <span class="n">service_tier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Latency tier for request. Options are ``'auto'``, ``'default'``, or ``'flex'``.</span>
<span class="sd">    Relevant for users of OpenAI's scale tier service.</span>
<span class="sd">    """</span>

    <span class="n">store</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""If True, OpenAI may store response data for future use. Defaults to True</span>
<span class="sd">    for the Responses API and False for the Chat Completions API.</span>

<span class="sd">    .. versionadded:: 0.3.24</span>
<span class="sd">    """</span>

    <span class="n">truncation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Truncation strategy (Responses API). Can be ``'auto'`` or ``'disabled'``</span>
<span class="sd">    (default). If ``'auto'``, model may drop input items from the middle of the</span>
<span class="sd">    message sequence to fit the context window.</span>

<span class="sd">    .. versionadded:: 0.3.24</span>
<span class="sd">    """</span>

    <span class="n">use_previous_response_id</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""If True, always pass ``previous_response_id`` using the ID of the most recent</span>
<span class="sd">    response. Responses API only.</span>

<span class="sd">    Input messages up to the most recent response will be dropped from request</span>
<span class="sd">    payloads.</span>

<span class="sd">    For example, the following two are equivalent:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        llm = ChatOpenAI(</span>
<span class="sd">            model="o4-mini",</span>
<span class="sd">            use_previous_response_id=True,</span>
<span class="sd">        )</span>
<span class="sd">        llm.invoke(</span>
<span class="sd">            [</span>
<span class="sd">                HumanMessage("Hello"),</span>
<span class="sd">                AIMessage("Hi there!", response_metadata={"id": "resp_123"}),</span>
<span class="sd">                HumanMessage("How are you?"),</span>
<span class="sd">            ]</span>
<span class="sd">        )</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        llm = ChatOpenAI(</span>
<span class="sd">            model="o4-mini",</span>
<span class="sd">            use_responses_api=True,</span>
<span class="sd">        )</span>
<span class="sd">        llm.invoke([HumanMessage("How are you?")], previous_response_id="resp_123")</span>

<span class="sd">    .. versionadded:: 0.3.26</span>
<span class="sd">    """</span>

    <span class="n">use_responses_api</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Whether to use the Responses API instead of the Chat API.</span>

<span class="sd">    If not specified then will be inferred based on invocation params.</span>

<span class="sd">    .. versionadded:: 0.3.9</span>
<span class="sd">    """</span>

    <span class="n">output_version</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"v0"</span><span class="p">,</span> <span class="s2">"responses/v1"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"v0"</span>
<span class="w">    </span><span class="sd">"""Version of AIMessage output format to use.</span>

<span class="sd">    This field is used to roll-out new output formats for chat model AIMessages</span>
<span class="sd">    in a backwards-compatible way.</span>

<span class="sd">    Supported values:</span>

<span class="sd">    - ``"v0"``: AIMessage format as of langchain-openai 0.3.x.</span>
<span class="sd">    - ``"responses/v1"``: Formats Responses API output</span>
<span class="sd">      items into AIMessage content blocks.</span>

<span class="sd">    Currently only impacts the Responses API. ``output_version="responses/v1"`` is</span>
<span class="sd">    recommended.</span>

<span class="sd">    .. versionadded:: 0.3.25</span>

<span class="sd">    """</span>

    <span class="n">model_config</span> <span class="o">=</span> <span class="n">ConfigDict</span><span class="p">(</span><span class="n">populate_by_name</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@model_validator</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">"before"</span><span class="p">)</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">build_extra</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Build extra kwargs from additional params that were passed in."""</span>
        <span class="n">all_required_field_names</span> <span class="o">=</span> <span class="n">get_pydantic_field_names</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">_build_model_kwargs</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">all_required_field_names</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">values</span>

    <span class="nd">@model_validator</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">"before"</span><span class="p">)</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">validate_temperature</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Currently o1 models only allow temperature=1."""</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model_name"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model"</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">""</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"o1"</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">"temperature"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">values</span><span class="p">:</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"temperature"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">values</span>

    <span class="nd">@model_validator</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">"after"</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">validate_environment</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Validate that api key and python package exists in environment."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"n must be at least 1."</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"n must be 1 when streaming."</span><span class="p">)</span>

        <span class="c1"># Check OPENAI_ORGANIZATION for backwards compatibility.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">openai_organization</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">openai_organization</span>
            <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"OPENAI_ORG_ID"</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"OPENAI_ORGANIZATION"</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"OPENAI_API_BASE"</span><span class="p">)</span>
        <span class="n">client_params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"api_key"</span><span class="p">:</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_key</span><span class="o">.</span><span class="n">get_secret_value</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_key</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">),</span>
            <span class="s2">"organization"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_organization</span><span class="p">,</span>
            <span class="s2">"base_url"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span><span class="p">,</span>
            <span class="s2">"timeout"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_timeout</span><span class="p">,</span>
            <span class="s2">"default_headers"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_headers</span><span class="p">,</span>
            <span class="s2">"default_query"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_query</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">client_params</span><span class="p">[</span><span class="s2">"max_retries"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">http_client</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">http_async_client</span><span class="p">):</span>
            <span class="n">openai_proxy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span>
            <span class="n">http_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">http_client</span>
            <span class="n">http_async_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">http_async_client</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Cannot specify 'openai_proxy' if one of "</span>
                <span class="s2">"'http_client'/'http_async_client' is already specified. Received:</span><span class="se">\n</span><span class="s2">"</span>
                <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">openai_proxy</span><span class="si">=}</span><span class="se">\n</span><span class="si">{</span><span class="n">http_client</span><span class="si">=}</span><span class="se">\n</span><span class="si">{</span><span class="n">http_async_client</span><span class="si">=}</span><span class="s2">"</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">http_client</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">import</span><span class="w"> </span><span class="nn">httpx</span>
                <span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                        <span class="s2">"Could not import httpx python package. "</span>
                        <span class="s2">"Please install it with `pip install httpx`."</span>
                    <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">http_client</span> <span class="o">=</span> <span class="n">httpx</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span>
                    <span class="n">proxy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span><span class="p">,</span> <span class="n">verify</span><span class="o">=</span><span class="n">global_ssl_context</span>
                <span class="p">)</span>
            <span class="n">sync_specific</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">"http_client"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">http_client</span>
                <span class="ow">or</span> <span class="n">_get_default_httpx_client</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_timeout</span><span class="p">)</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="o">**</span><span class="n">client_params</span><span class="p">,</span> <span class="o">**</span><span class="n">sync_specific</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">http_async_client</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">import</span><span class="w"> </span><span class="nn">httpx</span>
                <span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                        <span class="s2">"Could not import httpx python package. "</span>
                        <span class="s2">"Please install it with `pip install httpx`."</span>
                    <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">http_async_client</span> <span class="o">=</span> <span class="n">httpx</span><span class="o">.</span><span class="n">AsyncClient</span><span class="p">(</span>
                    <span class="n">proxy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span><span class="p">,</span> <span class="n">verify</span><span class="o">=</span><span class="n">global_ssl_context</span>
                <span class="p">)</span>
            <span class="n">async_specific</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">"http_client"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">http_async_client</span>
                <span class="ow">or</span> <span class="n">_get_default_async_httpx_client</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_timeout</span>
                <span class="p">)</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">AsyncOpenAI</span><span class="p">(</span>
                <span class="o">**</span><span class="n">client_params</span><span class="p">,</span>
                <span class="o">**</span><span class="n">async_specific</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_default_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the default parameters for calling OpenAI API."""</span>
        <span class="n">exclude_if_none</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"presence_penalty"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="s2">"frequency_penalty"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="s2">"seed"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
            <span class="s2">"top_p"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
            <span class="s2">"logprobs"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">logprobs</span><span class="p">,</span>
            <span class="s2">"top_logprobs"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_logprobs</span><span class="p">,</span>
            <span class="s2">"logit_bias"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">logit_bias</span><span class="p">,</span>
            <span class="s2">"stop"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># also exclude empty list for this</span>
            <span class="s2">"max_tokens"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="s2">"extra_body"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_body</span><span class="p">,</span>
            <span class="s2">"n"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
            <span class="s2">"temperature"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
            <span class="s2">"reasoning_effort"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">reasoning_effort</span><span class="p">,</span>
            <span class="s2">"reasoning"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">reasoning</span><span class="p">,</span>
            <span class="s2">"include"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">include</span><span class="p">,</span>
            <span class="s2">"service_tier"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">service_tier</span><span class="p">,</span>
            <span class="s2">"truncation"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation</span><span class="p">,</span>
            <span class="s2">"store"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">store</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"model"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="s2">"stream"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">,</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">exclude_if_none</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">},</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_combine_llm_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm_outputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">overall_token_usage</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">system_fingerprint</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">llm_outputs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Happens in streaming</span>
                <span class="k">continue</span>
            <span class="n">token_usage</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"token_usage"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">token_usage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">token_usage</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">overall_token_usage</span><span class="p">:</span>
                        <span class="n">overall_token_usage</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">_update_token_usage</span><span class="p">(</span>
                            <span class="n">overall_token_usage</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">overall_token_usage</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
            <span class="k">if</span> <span class="n">system_fingerprint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">system_fingerprint</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"system_fingerprint"</span><span class="p">)</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"token_usage"</span><span class="p">:</span> <span class="n">overall_token_usage</span><span class="p">,</span> <span class="s2">"model_name"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">system_fingerprint</span><span class="p">:</span>
            <span class="n">combined</span><span class="p">[</span><span class="s2">"system_fingerprint"</span><span class="p">]</span> <span class="o">=</span> <span class="n">system_fingerprint</span>
        <span class="k">return</span> <span class="n">combined</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_convert_chunk_to_generation_chunk</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">chunk</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">default_chunk_class</span><span class="p">:</span> <span class="nb">type</span><span class="p">,</span>
        <span class="n">base_generation_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"content.delta"</span><span class="p">:</span>  <span class="c1"># from beta.chat.completions.stream</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">token_usage</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"usage"</span><span class="p">)</span>
        <span class="n">choices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"choices"</span><span class="p">,</span> <span class="p">[])</span>
            <span class="c1"># from beta.chat.completions.stream</span>
            <span class="ow">or</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"chunk"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"choices"</span><span class="p">,</span> <span class="p">[])</span>
        <span class="p">)</span>

        <span class="n">usage_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UsageMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">_create_usage_metadata</span><span class="p">(</span><span class="n">token_usage</span><span class="p">)</span> <span class="k">if</span> <span class="n">token_usage</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">choices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># logprobs is implicitly None</span>
            <span class="n">generation_chunk</span> <span class="o">=</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="n">default_chunk_class</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span> <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage_metadata</span><span class="p">),</span>
                <span class="n">generation_info</span><span class="o">=</span><span class="n">base_generation_info</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">generation_chunk</span>

        <span class="n">choice</span> <span class="o">=</span> <span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">choice</span><span class="p">[</span><span class="s2">"delta"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">message_chunk</span> <span class="o">=</span> <span class="n">_convert_delta_to_message_chunk</span><span class="p">(</span>
            <span class="n">choice</span><span class="p">[</span><span class="s2">"delta"</span><span class="p">],</span> <span class="n">default_chunk_class</span>
        <span class="p">)</span>
        <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">base_generation_info</span><span class="p">}</span> <span class="k">if</span> <span class="n">base_generation_info</span> <span class="k">else</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">finish_reason</span> <span class="o">:=</span> <span class="n">choice</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">):</span>
            <span class="n">generation_info</span><span class="p">[</span><span class="s2">"finish_reason"</span><span class="p">]</span> <span class="o">=</span> <span class="n">finish_reason</span>
            <span class="k">if</span> <span class="n">model_name</span> <span class="o">:=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model"</span><span class="p">):</span>
                <span class="n">generation_info</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_name</span>
            <span class="k">if</span> <span class="n">system_fingerprint</span> <span class="o">:=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"system_fingerprint"</span><span class="p">):</span>
                <span class="n">generation_info</span><span class="p">[</span><span class="s2">"system_fingerprint"</span><span class="p">]</span> <span class="o">=</span> <span class="n">system_fingerprint</span>
            <span class="k">if</span> <span class="n">service_tier</span> <span class="o">:=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"service_tier"</span><span class="p">):</span>
                <span class="n">generation_info</span><span class="p">[</span><span class="s2">"service_tier"</span><span class="p">]</span> <span class="o">=</span> <span class="n">service_tier</span>

        <span class="n">logprobs</span> <span class="o">=</span> <span class="n">choice</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logprobs"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
            <span class="n">generation_info</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span> <span class="o">=</span> <span class="n">logprobs</span>

        <span class="k">if</span> <span class="n">usage_metadata</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message_chunk</span><span class="p">,</span> <span class="n">AIMessageChunk</span><span class="p">):</span>
            <span class="n">message_chunk</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">usage_metadata</span>

        <span class="n">generation_chunk</span> <span class="o">=</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message_chunk</span><span class="p">,</span> <span class="n">generation_info</span><span class="o">=</span><span class="n">generation_info</span> <span class="ow">or</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">generation_chunk</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_stream_responses</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_context_manager</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="o">**</span><span class="n">payload</span>
            <span class="p">)</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="n">raw_context_manager</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_context_manager</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">headers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">original_schema_obj</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"response_format"</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">context_manager</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
            <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">current_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">current_output_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">current_sub_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">has_reasoning</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="n">headers</span> <span class="k">if</span> <span class="n">is_first_chunk</span> <span class="k">else</span> <span class="p">{}</span>
                <span class="p">(</span>
                    <span class="n">current_index</span><span class="p">,</span>
                    <span class="n">current_output_index</span><span class="p">,</span>
                    <span class="n">current_sub_index</span><span class="p">,</span>
                    <span class="n">generation_chunk</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="n">_convert_responses_chunk_to_generation_chunk</span><span class="p">(</span>
                    <span class="n">chunk</span><span class="p">,</span>
                    <span class="n">current_index</span><span class="p">,</span>
                    <span class="n">current_output_index</span><span class="p">,</span>
                    <span class="n">current_sub_index</span><span class="p">,</span>
                    <span class="n">schema</span><span class="o">=</span><span class="n">original_schema_obj</span><span class="p">,</span>
                    <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                    <span class="n">has_reasoning</span><span class="o">=</span><span class="n">has_reasoning</span><span class="p">,</span>
                    <span class="n">output_version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_version</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_chunk</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                        <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                            <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span>
                        <span class="p">)</span>
                    <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">if</span> <span class="s2">"reasoning"</span> <span class="ow">in</span> <span class="n">generation_chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
                        <span class="n">has_reasoning</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">yield</span> <span class="n">generation_chunk</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_astream_responses</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_context_manager</span> <span class="o">=</span> <span class="p">(</span>
                <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">payload</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="n">raw_context_manager</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_context_manager</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">headers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">original_schema_obj</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"response_format"</span><span class="p">)</span>

        <span class="k">async</span> <span class="k">with</span> <span class="n">context_manager</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
            <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">current_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">current_output_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">current_sub_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">has_reasoning</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="n">headers</span> <span class="k">if</span> <span class="n">is_first_chunk</span> <span class="k">else</span> <span class="p">{}</span>
                <span class="p">(</span>
                    <span class="n">current_index</span><span class="p">,</span>
                    <span class="n">current_output_index</span><span class="p">,</span>
                    <span class="n">current_sub_index</span><span class="p">,</span>
                    <span class="n">generation_chunk</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="n">_convert_responses_chunk_to_generation_chunk</span><span class="p">(</span>
                    <span class="n">chunk</span><span class="p">,</span>
                    <span class="n">current_index</span><span class="p">,</span>
                    <span class="n">current_output_index</span><span class="p">,</span>
                    <span class="n">current_sub_index</span><span class="p">,</span>
                    <span class="n">schema</span><span class="o">=</span><span class="n">original_schema_obj</span><span class="p">,</span>
                    <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                    <span class="n">has_reasoning</span><span class="o">=</span><span class="n">has_reasoning</span><span class="p">,</span>
                    <span class="n">output_version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_version</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_chunk</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                        <span class="k">await</span> <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                            <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span>
                        <span class="p">)</span>
                    <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">if</span> <span class="s2">"reasoning"</span> <span class="ow">in</span> <span class="n">generation_chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
                        <span class="n">has_reasoning</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">yield</span> <span class="n">generation_chunk</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_should_stream_usage</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">stream_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Determine whether to include usage metadata in streaming output.</span>

<span class="sd">        For backwards compatibility, we check for `stream_options` passed</span>
<span class="sd">        explicitly to kwargs or in the model_kwargs and override self.stream_usage.</span>
<span class="sd">        """</span>
        <span class="n">stream_usage_sources</span> <span class="o">=</span> <span class="p">[</span>  <span class="c1"># order of precedence</span>
            <span class="n">stream_usage</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stream_options"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"include_usage"</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stream_options"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"include_usage"</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stream_usage</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">source</span> <span class="ow">in</span> <span class="n">stream_usage_sources</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">source</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream_usage</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">stream_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">stream_usage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_stream_usage</span><span class="p">(</span><span class="n">stream_usage</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stream_usage</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream_options"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"include_usage"</span><span class="p">:</span> <span class="n">stream_usage</span><span class="p">}</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">default_chunk_class</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">BaseMessageChunk</span><span class="p">]</span> <span class="o">=</span> <span class="n">AIMessageChunk</span>
        <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">"Cannot currently include response headers when response_format is "</span>
                    <span class="s2">"specified."</span>
                <span class="p">)</span>
            <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span>
            <span class="n">response_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="n">response_stream</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">raw_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
                <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">context_manager</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
                <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                        <span class="n">chunk</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
                    <span class="n">generation_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_chunk_to_generation_chunk</span><span class="p">(</span>
                        <span class="n">chunk</span><span class="p">,</span>
                        <span class="n">default_chunk_class</span><span class="p">,</span>
                        <span class="n">base_generation_info</span> <span class="k">if</span> <span class="n">is_first_chunk</span> <span class="k">else</span> <span class="p">{},</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">generation_chunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">default_chunk_class</span> <span class="o">=</span> <span class="n">generation_chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="vm">__class__</span>
                    <span class="n">logprobs</span> <span class="o">=</span> <span class="p">(</span><span class="n">generation_chunk</span><span class="o">.</span><span class="n">generation_info</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logprobs"</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                        <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                            <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
                            <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span><span class="p">,</span>
                            <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">yield</span> <span class="n">generation_chunk</span>
        <span class="k">except</span> <span class="n">openai</span><span class="o">.</span><span class="n">BadRequestError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">_handle_openai_bad_request</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="s2">"get_final_completion"</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="n">final_completion</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">get_final_completion</span><span class="p">()</span>
            <span class="n">generation_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_generation_chunk_from_completion</span><span class="p">(</span>
                <span class="n">final_completion</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                    <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span>
                <span class="p">)</span>
            <span class="k">yield</span> <span class="n">generation_chunk</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">:</span>
            <span class="n">stream_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stream</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">generate_from_stream</span><span class="p">(</span><span class="n">stream_iter</span><span class="p">)</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">generation_info</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">"Cannot currently include response headers when response_format is "</span>
                    <span class="s2">"specified."</span>
                <span class="p">)</span>
            <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">openai</span><span class="o">.</span><span class="n">BadRequestError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">_handle_openai_bad_request</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_responses_api</span><span class="p">(</span><span class="n">payload</span><span class="p">):</span>
            <span class="n">original_schema_obj</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"response_format"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">original_schema_obj</span> <span class="ow">and</span> <span class="n">_is_pydantic_class</span><span class="p">(</span><span class="n">original_schema_obj</span><span class="p">):</span>
                <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                    <span class="n">raw_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                        <span class="o">**</span><span class="n">payload</span>
                    <span class="p">)</span>
                    <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
                    <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_construct_lc_result_from_responses_api</span><span class="p">(</span>
                <span class="n">response</span><span class="p">,</span>
                <span class="n">schema</span><span class="o">=</span><span class="n">original_schema_obj</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">generation_info</span><span class="p">,</span>
                <span class="n">output_version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_version</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chat_result</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">generation_info</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_use_responses_api</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">payload</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_responses_api</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_responses_api</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_version</span> <span class="o">==</span> <span class="s2">"responses/v1"</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">include</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reasoning</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_previous_response_id</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_use_responses_api</span><span class="p">(</span><span class="n">payload</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_request_payload</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_</span><span class="p">:</span> <span class="n">LanguageModelInput</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_input</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span><span class="o">.</span><span class="n">to_messages</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">stop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stop"</span><span class="p">]</span> <span class="o">=</span> <span class="n">stop</span>

        <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_responses_api</span><span class="p">(</span><span class="n">payload</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_previous_response_id</span><span class="p">:</span>
                <span class="n">last_messages</span><span class="p">,</span> <span class="n">previous_response_id</span> <span class="o">=</span> <span class="n">_get_last_messages</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
                <span class="n">payload_to_use</span> <span class="o">=</span> <span class="n">last_messages</span> <span class="k">if</span> <span class="n">previous_response_id</span> <span class="k">else</span> <span class="n">messages</span>
                <span class="k">if</span> <span class="n">previous_response_id</span><span class="p">:</span>
                    <span class="n">payload</span><span class="p">[</span><span class="s2">"previous_response_id"</span><span class="p">]</span> <span class="o">=</span> <span class="n">previous_response_id</span>
                <span class="n">payload</span> <span class="o">=</span> <span class="n">_construct_responses_api_payload</span><span class="p">(</span><span class="n">payload_to_use</span><span class="p">,</span> <span class="n">payload</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">payload</span> <span class="o">=</span> <span class="n">_construct_responses_api_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">payload</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">_convert_message_to_dict</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">payload</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_chat_result</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">response</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">openai</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">],</span>
        <span class="n">generation_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="n">generations</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">response_dict</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">response</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">response</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="c1"># Sometimes the AI Model calling will get error, we should raise it (this is</span>
        <span class="c1"># typically followed by a null value for `choices`, which we raise for</span>
        <span class="c1"># separately below).</span>
        <span class="k">if</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"error"</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"error"</span><span class="p">))</span>

        <span class="c1"># Raise informative error messages for non-OpenAI chat completions APIs</span>
        <span class="c1"># that return malformed responses.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">choices</span> <span class="o">=</span> <span class="n">response_dict</span><span class="p">[</span><span class="s2">"choices"</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Response missing `choices` key: </span><span class="si">{</span><span class="n">response_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
            <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>

        <span class="k">if</span> <span class="n">choices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">"Received response with null value for `choices`."</span><span class="p">)</span>

        <span class="n">token_usage</span> <span class="o">=</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"usage"</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">choices</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">_convert_dict_to_message</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">"message"</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">token_usage</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
                <span class="n">message</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">_create_usage_metadata</span><span class="p">(</span><span class="n">token_usage</span><span class="p">)</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="n">generation_info</span> <span class="ow">or</span> <span class="p">{}</span>
            <span class="n">generation_info</span><span class="p">[</span><span class="s2">"finish_reason"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">generation_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="s2">"logprobs"</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
                <span class="n">generation_info</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span>
            <span class="n">gen</span> <span class="o">=</span> <span class="n">ChatGeneration</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">generation_info</span><span class="o">=</span><span class="n">generation_info</span><span class="p">)</span>
            <span class="n">generations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
        <span class="n">llm_output</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"token_usage"</span><span class="p">:</span> <span class="n">token_usage</span><span class="p">,</span>
            <span class="s2">"model_name"</span><span class="p">:</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">),</span>
            <span class="s2">"system_fingerprint"</span><span class="p">:</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"system_fingerprint"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="s2">"id"</span> <span class="ow">in</span> <span class="n">response_dict</span><span class="p">:</span>
            <span class="n">llm_output</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]</span> <span class="o">=</span> <span class="n">response_dict</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]</span>
        <span class="k">if</span> <span class="s2">"service_tier"</span> <span class="ow">in</span> <span class="n">response_dict</span><span class="p">:</span>
            <span class="n">llm_output</span><span class="p">[</span><span class="s2">"service_tier"</span><span class="p">]</span> <span class="o">=</span> <span class="n">response_dict</span><span class="p">[</span><span class="s2">"service_tier"</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">openai</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span> <span class="s2">"choices"</span><span class="p">,</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="s2">"parsed"</span><span class="p">):</span>
                <span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"parsed"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">parsed</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="s2">"refusal"</span><span class="p">):</span>
                <span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"refusal"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">refusal</span>

        <span class="k">return</span> <span class="n">ChatResult</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="n">generations</span><span class="p">,</span> <span class="n">llm_output</span><span class="o">=</span><span class="n">llm_output</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_astream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">stream_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">stream_usage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_stream_usage</span><span class="p">(</span><span class="n">stream_usage</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stream_usage</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream_options"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"include_usage"</span><span class="p">:</span> <span class="n">stream_usage</span><span class="p">}</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">default_chunk_class</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">BaseMessageChunk</span><span class="p">]</span> <span class="o">=</span> <span class="n">AIMessageChunk</span>
        <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">"Cannot currently include response headers when response_format is "</span>
                    <span class="s2">"specified."</span>
                <span class="p">)</span>
            <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span>
            <span class="n">response_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
                <span class="o">**</span><span class="n">payload</span>
            <span class="p">)</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="n">response_stream</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">raw_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">payload</span>
                <span class="p">)</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
                <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">context_manager</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">async</span> <span class="k">with</span> <span class="n">context_manager</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
                <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                        <span class="n">chunk</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
                    <span class="n">generation_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_chunk_to_generation_chunk</span><span class="p">(</span>
                        <span class="n">chunk</span><span class="p">,</span>
                        <span class="n">default_chunk_class</span><span class="p">,</span>
                        <span class="n">base_generation_info</span> <span class="k">if</span> <span class="n">is_first_chunk</span> <span class="k">else</span> <span class="p">{},</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">generation_chunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">default_chunk_class</span> <span class="o">=</span> <span class="n">generation_chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="vm">__class__</span>
                    <span class="n">logprobs</span> <span class="o">=</span> <span class="p">(</span><span class="n">generation_chunk</span><span class="o">.</span><span class="n">generation_info</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logprobs"</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                        <span class="k">await</span> <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                            <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
                            <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span><span class="p">,</span>
                            <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">yield</span> <span class="n">generation_chunk</span>
        <span class="k">except</span> <span class="n">openai</span><span class="o">.</span><span class="n">BadRequestError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">_handle_openai_bad_request</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="s2">"get_final_completion"</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="n">final_completion</span> <span class="o">=</span> <span class="k">await</span> <span class="n">response</span><span class="o">.</span><span class="n">get_final_completion</span><span class="p">()</span>
            <span class="n">generation_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_generation_chunk_from_completion</span><span class="p">(</span>
                <span class="n">final_completion</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                <span class="k">await</span> <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                    <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span>
                <span class="p">)</span>
            <span class="k">yield</span> <span class="n">generation_chunk</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_agenerate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">:</span>
            <span class="n">stream_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_astream</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="k">await</span> <span class="n">agenerate_from_stream</span><span class="p">(</span><span class="n">stream_iter</span><span class="p">)</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">generation_info</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">"Cannot currently include response headers when response_format is "</span>
                    <span class="s2">"specified."</span>
                <span class="p">)</span>
            <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">payload</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="n">openai</span><span class="o">.</span><span class="n">BadRequestError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">_handle_openai_bad_request</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_responses_api</span><span class="p">(</span><span class="n">payload</span><span class="p">):</span>
            <span class="n">original_schema_obj</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"response_format"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">original_schema_obj</span> <span class="ow">and</span> <span class="n">_is_pydantic_class</span><span class="p">(</span><span class="n">original_schema_obj</span><span class="p">):</span>
                <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                    <span class="n">raw_response</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                            <span class="o">**</span><span class="n">payload</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
                    <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">responses</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_construct_lc_result_from_responses_api</span><span class="p">(</span>
                <span class="n">response</span><span class="p">,</span>
                <span class="n">schema</span><span class="o">=</span><span class="n">original_schema_obj</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">generation_info</span><span class="p">,</span>
                <span class="n">output_version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_version</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">return</span> <span class="k">await</span> <span class="n">run_in_executor</span><span class="p">(</span>
            <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chat_result</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">generation_info</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_identifying_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the identifying parameters."""</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"model_name"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_params</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_invocation_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the parameters used to invoke the model."""</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"model"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="o">**</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_get_invocation_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">),</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># Redact headers from built-in remote MCP tool invocations</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">tools</span> <span class="o">:=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tools</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">params</span><span class="p">[</span><span class="s2">"tools"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">({</span><span class="o">**</span><span class="n">tool</span><span class="p">,</span> <span class="s2">"headers"</span><span class="p">:</span> <span class="s2">"**REDACTED**"</span><span class="p">}</span> <span class="k">if</span> <span class="s2">"headers"</span> <span class="ow">in</span> <span class="n">tool</span> <span class="k">else</span> <span class="n">tool</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tool</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"mcp"</span>
                <span class="k">else</span> <span class="n">tool</span>
                <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span>
            <span class="p">]</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_ls_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LangSmithParams</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Get standard params for tracing."""</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_invocation_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">ls_params</span> <span class="o">=</span> <span class="n">LangSmithParams</span><span class="p">(</span>
            <span class="n">ls_provider</span><span class="o">=</span><span class="s2">"openai"</span><span class="p">,</span>
            <span class="n">ls_model_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">ls_model_type</span><span class="o">=</span><span class="s2">"chat"</span><span class="p">,</span>
            <span class="n">ls_temperature</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">ls_max_tokens</span> <span class="o">:=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"max_tokens"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span><span class="p">)</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"max_completion_tokens"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span>
        <span class="p">):</span>
            <span class="n">ls_params</span><span class="p">[</span><span class="s2">"ls_max_tokens"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ls_max_tokens</span>
        <span class="k">if</span> <span class="n">ls_stop</span> <span class="o">:=</span> <span class="n">stop</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stop"</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ls_params</span><span class="p">[</span><span class="s2">"ls_stop"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ls_stop</span>
        <span class="k">return</span> <span class="n">ls_params</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_llm_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Return type of chat model."""</span>
        <span class="k">return</span> <span class="s2">"openai-chat"</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_encoding_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">Encoding</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tiktoken_model_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tiktoken_model_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">encoder</span> <span class="o">=</span> <span class="s2">"cl100k_base"</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-4o"</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span>
                <span class="s2">"gpt-4.1"</span>
            <span class="p">):</span>
                <span class="n">encoder</span> <span class="o">=</span> <span class="s2">"o200k_base"</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="n">encoder</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">encoding</span>

<div class="viewcode-block" id="BaseChatOpenAI.get_token_ids">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_cerebras.chat_models.BaseChatOpenAI.get_token_ids">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_token_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the tokens present in the text with tiktoken package."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_get_token_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_get_token_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="c1"># tiktoken NOT supported for Python 3.7 or below</span>
        <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">7</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_token_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">encoding_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_encoding_model</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">encoding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span></div>


<div class="viewcode-block" id="BaseChatOpenAI.get_num_tokens_from_messages">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_cerebras.chat_models.BaseChatOpenAI.get_num_tokens_from_messages">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_num_tokens_from_messages</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="nb">type</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">BaseTool</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Calculate num tokens for ``gpt-3.5-turbo`` and ``gpt-4`` with ``tiktoken`` package.</span>

<span class="sd">        **Requirements**: You must have the ``pillow`` installed if you want to count</span>
<span class="sd">        image tokens if you are specifying the image as a base64 string, and you must</span>
<span class="sd">        have both ``pillow`` and ``httpx`` installed if you are specifying the image</span>
<span class="sd">        as a URL. If these aren't installed image inputs will be ignored in token</span>
<span class="sd">        counting.</span>

<span class="sd">        `OpenAI reference &lt;https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb&gt;`__</span>

<span class="sd">        Args:</span>
<span class="sd">            messages: The message inputs to tokenize.</span>
<span class="sd">            tools: If provided, sequence of dict, BaseModel, function, or BaseTools</span>
<span class="sd">                to be converted to tool schemas.</span>
<span class="sd">        """</span>  <span class="c1"># noqa: E501</span>
        <span class="c1"># TODO: Count bound tools as part of input.</span>
        <span class="k">if</span> <span class="n">tools</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">"Counting tokens in tool schemas is not yet supported. Ignoring tools."</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">7</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_num_tokens_from_messages</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_encoding_model</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-3.5-turbo-0301"</span><span class="p">):</span>
            <span class="c1"># every message follows &lt;im_start&gt;{role/name}\n{content}&lt;im_end&gt;\n</span>
            <span class="n">tokens_per_message</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="c1"># if there's a name, the role is omitted</span>
            <span class="n">tokens_per_name</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-3.5-turbo"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-4"</span><span class="p">):</span>
            <span class="n">tokens_per_message</span> <span class="o">=</span> <span class="mi">3</span>
            <span class="n">tokens_per_name</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"get_num_tokens_from_messages() is not presently implemented "</span>
                <span class="sa">f</span><span class="s2">"for model </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">. See "</span>
                <span class="s2">"https://platform.openai.com/docs/guides/text-generation/managing-tokens"</span>  <span class="c1"># noqa: E501</span>
                <span class="s2">" for information on how messages are converted to tokens."</span>
            <span class="p">)</span>
        <span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">messages_dict</span> <span class="o">=</span> <span class="p">[</span><span class="n">_convert_message_to_dict</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages_dict</span><span class="p">:</span>
            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="n">tokens_per_message</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># This is an inferred approximation. OpenAI does not document how to</span>
                <span class="c1"># count tool message tokens.</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">"tool_call_id"</span><span class="p">:</span>
                    <span class="n">num_tokens</span> <span class="o">+=</span> <span class="mi">3</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="c1"># content or tool calls</span>
                    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">val</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"text"</span><span class="p">:</span>
                            <span class="n">text</span> <span class="o">=</span> <span class="n">val</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">val</span>
                            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
                        <span class="k">elif</span> <span class="n">val</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"image_url"</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"detail"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"low"</span><span class="p">:</span>
                                <span class="n">num_tokens</span> <span class="o">+=</span> <span class="mi">85</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">image_size</span> <span class="o">=</span> <span class="n">_url_to_size</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">][</span><span class="s2">"url"</span><span class="p">])</span>
                                <span class="k">if</span> <span class="ow">not</span> <span class="n">image_size</span><span class="p">:</span>
                                    <span class="k">continue</span>
                                <span class="n">num_tokens</span> <span class="o">+=</span> <span class="n">_count_image_tokens</span><span class="p">(</span><span class="o">*</span><span class="n">image_size</span><span class="p">)</span>
                        <span class="c1"># Tool/function call token counting is not documented by OpenAI.</span>
                        <span class="c1"># This is an approximation.</span>
                        <span class="k">elif</span> <span class="n">val</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"function"</span><span class="p">:</span>
                            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span>
                                <span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"arguments"</span><span class="p">])</span>
                            <span class="p">)</span>
                            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]))</span>
                        <span class="k">elif</span> <span class="n">val</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"file"</span><span class="p">:</span>
                            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                                <span class="s2">"Token counts for file inputs are not supported. "</span>
                                <span class="s2">"Ignoring file inputs."</span>
                            <span class="p">)</span>
                            <span class="k">pass</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">"Unrecognized content block type</span><span class="se">\n\n</span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">"</span>
                            <span class="p">)</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">value</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Cast str(value) in case the message value is not a string</span>
                    <span class="c1"># This occurs with function messages</span>
                    <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">"name"</span><span class="p">:</span>
                    <span class="n">num_tokens</span> <span class="o">+=</span> <span class="n">tokens_per_name</span>
        <span class="c1"># every reply is primed with &lt;im_start&gt;assistant</span>
        <span class="n">num_tokens</span> <span class="o">+=</span> <span class="mi">3</span>
        <span class="k">return</span> <span class="n">num_tokens</span></div>


<div class="viewcode-block" id="BaseChatOpenAI.bind_functions">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_cerebras.chat_models.BaseChatOpenAI.bind_functions">[docs]</a>
    <span class="nd">@deprecated</span><span class="p">(</span>
        <span class="n">since</span><span class="o">=</span><span class="s2">"0.2.1"</span><span class="p">,</span>
        <span class="n">alternative</span><span class="o">=</span><span class="s2">"langchain_openai.chat_models.base.ChatOpenAI.bind_tools"</span><span class="p">,</span>
        <span class="n">removal</span><span class="o">=</span><span class="s2">"1.0.0"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">bind_functions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">functions</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="nb">type</span><span class="p">[</span><span class="n">BaseModel</span><span class="p">],</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">BaseTool</span><span class="p">]],</span>
        <span class="n">function_call</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">_FunctionCall</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"auto"</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">BaseMessage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Bind functions (and other objects) to this chat model.</span>

<span class="sd">        Assumes model is compatible with OpenAI function-calling API.</span>

<span class="sd">        NOTE: Using bind_tools is recommended instead, as the `functions` and</span>
<span class="sd">            `function_call` request parameters are officially marked as deprecated by</span>
<span class="sd">            OpenAI.</span>

<span class="sd">        Args:</span>
<span class="sd">            functions: A list of function definitions to bind to this chat model.</span>
<span class="sd">                Can be  a dictionary, pydantic model, or callable. Pydantic</span>
<span class="sd">                models and callables will be automatically converted to</span>
<span class="sd">                their schema dictionary representation.</span>
<span class="sd">            function_call: Which function to require the model to call.</span>
<span class="sd">                Must be the name of the single provided function or</span>
<span class="sd">                "auto" to automatically determine which function to call</span>
<span class="sd">                (if any).</span>
<span class="sd">            **kwargs: Any additional parameters to pass to the</span>
<span class="sd">                :class:`~langchain.runnable.Runnable` constructor.</span>
<span class="sd">        """</span>

        <span class="n">formatted_functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">convert_to_openai_function</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">functions</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">function_call</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">function_call</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">{</span><span class="s2">"name"</span><span class="p">:</span> <span class="n">function_call</span><span class="p">}</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">function_call</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">function_call</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"auto"</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">function_call</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">function_call</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">formatted_functions</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">"When specifying `function_call`, you must provide exactly one "</span>
                    <span class="s2">"function."</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">function_call</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">formatted_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">!=</span> <span class="n">function_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Function call </span><span class="si">{</span><span class="n">function_call</span><span class="si">}</span><span class="s2"> was specified, but the only "</span>
                    <span class="sa">f</span><span class="s2">"provided function was </span><span class="si">{</span><span class="n">formatted_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'name'</span><span class="p">]</span><span class="si">}</span><span class="s2">."</span>
                <span class="p">)</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="s2">"function_call"</span><span class="p">:</span> <span class="n">function_call</span><span class="p">}</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">functions</span><span class="o">=</span><span class="n">formatted_functions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="BaseChatOpenAI.bind_tools">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_cerebras.chat_models.BaseChatOpenAI.bind_tools">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">bind_tools</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="nb">type</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">BaseTool</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tool_choice</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"auto"</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">,</span> <span class="s2">"required"</span><span class="p">,</span> <span class="s2">"any"</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parallel_tool_calls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">BaseMessage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Bind tool-like objects to this chat model.</span>

<span class="sd">        Assumes model is compatible with OpenAI tool-calling API.</span>

<span class="sd">        Args:</span>
<span class="sd">            tools: A list of tool definitions to bind to this chat model.</span>
<span class="sd">                Supports any tool definition handled by</span>
<span class="sd">                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.</span>
<span class="sd">            tool_choice: Which tool to require the model to call. Options are:</span>

<span class="sd">                - str of the form ``"&lt;&lt;tool_name&gt;&gt;"``: calls &lt;&lt;tool_name&gt;&gt; tool.</span>
<span class="sd">                - ``"auto"``: automatically selects a tool (including no tool).</span>
<span class="sd">                - ``"none"``: does not call a tool.</span>
<span class="sd">                - ``"any"`` or ``"required"`` or ``True``: force at least one tool to be called.</span>
<span class="sd">                - dict of the form ``{"type": "function", "function": {"name": &lt;&lt;tool_name&gt;&gt;}}``: calls &lt;&lt;tool_name&gt;&gt; tool.</span>
<span class="sd">                - ``False`` or ``None``: no effect, default OpenAI behavior.</span>
<span class="sd">            strict: If True, model output is guaranteed to exactly match the JSON Schema</span>
<span class="sd">                provided in the tool definition. If True, the input schema will be</span>
<span class="sd">                validated according to</span>
<span class="sd">                https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.</span>
<span class="sd">                If False, input schema will not be validated and model output will not</span>
<span class="sd">                be validated.</span>
<span class="sd">                If None, ``strict`` argument will not be passed to the model.</span>
<span class="sd">            parallel_tool_calls: Set to ``False`` to disable parallel tool use.</span>
<span class="sd">                Defaults to ``None`` (no specification, which allows parallel tool use).</span>
<span class="sd">            kwargs: Any additional parameters are passed directly to</span>
<span class="sd">                :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind`.</span>

<span class="sd">        .. versionchanged:: 0.1.21</span>

<span class="sd">            Support for ``strict`` argument added.</span>

<span class="sd">        """</span>  <span class="c1"># noqa: E501</span>

        <span class="k">if</span> <span class="n">parallel_tool_calls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"parallel_tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="n">parallel_tool_calls</span>
        <span class="n">formatted_tools</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">tool</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span>
        <span class="p">]</span>
        <span class="n">tool_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">formatted_tools</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">"function"</span> <span class="ow">in</span> <span class="n">tool</span><span class="p">:</span>
                <span class="n">tool_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">])</span>
            <span class="k">elif</span> <span class="s2">"name"</span> <span class="ow">in</span> <span class="n">tool</span><span class="p">:</span>
                <span class="n">tool_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool</span><span class="p">[</span><span class="s2">"name"</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="k">if</span> <span class="n">tool_choice</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="c1"># tool_choice is a tool/function name</span>
                <span class="k">if</span> <span class="n">tool_choice</span> <span class="ow">in</span> <span class="n">tool_names</span><span class="p">:</span>
                    <span class="n">tool_choice</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span>
                        <span class="s2">"function"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"name"</span><span class="p">:</span> <span class="n">tool_choice</span><span class="p">},</span>
                    <span class="p">}</span>
                <span class="k">elif</span> <span class="n">tool_choice</span> <span class="ow">in</span> <span class="n">WellKnownTools</span><span class="p">:</span>
                    <span class="n">tool_choice</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="n">tool_choice</span><span class="p">}</span>
                <span class="c1"># 'any' is not natively supported by OpenAI API.</span>
                <span class="c1"># We support 'any' since other models use this instead of 'required'.</span>
                <span class="k">elif</span> <span class="n">tool_choice</span> <span class="o">==</span> <span class="s2">"any"</span><span class="p">:</span>
                    <span class="n">tool_choice</span> <span class="o">=</span> <span class="s2">"required"</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">pass</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
                <span class="n">tool_choice</span> <span class="o">=</span> <span class="s2">"required"</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Unrecognized tool_choice type. Expected str, bool or dict. "</span>
                    <span class="sa">f</span><span class="s2">"Received: </span><span class="si">{</span><span class="n">tool_choice</span><span class="si">}</span><span class="s2">"</span>
                <span class="p">)</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"tool_choice"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tool_choice</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">formatted_tools</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="BaseChatOpenAI.with_structured_output">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_cerebras.chat_models.BaseChatOpenAI.with_structured_output">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">with_structured_output</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_DictOrPydanticClass</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span>
            <span class="s2">"function_calling"</span><span class="p">,</span> <span class="s2">"json_mode"</span><span class="p">,</span> <span class="s2">"json_schema"</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">"function_calling"</span><span class="p">,</span>
        <span class="n">include_raw</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">_DictOrPydantic</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Model wrapper that returns outputs formatted to match the given schema.</span>

<span class="sd">        Args:</span>
<span class="sd">            schema:</span>
<span class="sd">                The output schema. Can be passed in as:</span>

<span class="sd">                - an OpenAI function/tool schema,</span>
<span class="sd">                - a JSON Schema,</span>
<span class="sd">                - a TypedDict class (support added in 0.1.20),</span>
<span class="sd">                - or a Pydantic class.</span>

<span class="sd">                If ``schema`` is a Pydantic class then the model output will be a</span>
<span class="sd">                Pydantic instance of that class, and the model-generated fields will be</span>
<span class="sd">                validated by the Pydantic class. Otherwise the model output will be a</span>
<span class="sd">                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`</span>
<span class="sd">                for more on how to properly specify types and descriptions of</span>
<span class="sd">                schema fields when specifying a Pydantic or TypedDict class.</span>

<span class="sd">            method: The method for steering model generation, one of:</span>

<span class="sd">                - "function_calling":</span>
<span class="sd">                    Uses OpenAI's tool-calling (formerly called function calling)</span>
<span class="sd">                    API: https://platform.openai.com/docs/guides/function-calling</span>
<span class="sd">                - "json_schema":</span>
<span class="sd">                    Uses OpenAI's Structured Output API: https://platform.openai.com/docs/guides/structured-outputs</span>
<span class="sd">                    Supported for "gpt-4o-mini", "gpt-4o-2024-08-06", "o1", and later</span>
<span class="sd">                    models.</span>
<span class="sd">                - "json_mode":</span>
<span class="sd">                    Uses OpenAI's JSON mode. Note that if using JSON mode then you</span>
<span class="sd">                    must include instructions for formatting the output into the</span>
<span class="sd">                    desired schema into the model call:</span>
<span class="sd">                    https://platform.openai.com/docs/guides/structured-outputs/json-mode</span>

<span class="sd">                Learn more about the differences between the methods and which models</span>
<span class="sd">                support which methods here:</span>

<span class="sd">                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode</span>
<span class="sd">                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format</span>

<span class="sd">            include_raw:</span>
<span class="sd">                If False then only the parsed structured output is returned. If</span>
<span class="sd">                an error occurs during model output parsing it will be raised. If True</span>
<span class="sd">                then both the raw model response (a BaseMessage) and the parsed model</span>
<span class="sd">                response will be returned. If an error occurs during output parsing it</span>
<span class="sd">                will be caught and returned as well. The final output is always a dict</span>
<span class="sd">                with keys "raw", "parsed", and "parsing_error".</span>
<span class="sd">            strict:</span>

<span class="sd">                - True:</span>
<span class="sd">                    Model output is guaranteed to exactly match the schema.</span>
<span class="sd">                    The input schema will also be validated according to</span>
<span class="sd">                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</span>
<span class="sd">                - False:</span>
<span class="sd">                    Input schema will not be validated and model output will not be</span>
<span class="sd">                    validated.</span>
<span class="sd">                - None:</span>
<span class="sd">                    ``strict`` argument will not be passed to the model.</span>

<span class="sd">            tools:</span>
<span class="sd">                A list of tool-like objects to bind to the chat model. Requires that:</span>

<span class="sd">                - ``method`` is ``"json_schema"`` (default).</span>
<span class="sd">                - ``strict=True``</span>
<span class="sd">                - ``include_raw=True``</span>

<span class="sd">                If a model elects to call a</span>
<span class="sd">                tool, the resulting ``AIMessage`` in ``"raw"`` will include tool calls.</span>

<span class="sd">                .. dropdown:: Example</span>

<span class="sd">                    .. code-block:: python</span>

<span class="sd">                        from langchain.chat_models import init_chat_model</span>
<span class="sd">                        from pydantic import BaseModel</span>


<span class="sd">                        class ResponseSchema(BaseModel):</span>
<span class="sd">                            response: str</span>


<span class="sd">                        def get_weather(location: str) -&gt; str:</span>
<span class="sd">                            \"\"\"Get weather at a location.\"\"\"</span>
<span class="sd">                            pass</span>

<span class="sd">                        llm = init_chat_model("openai:gpt-4o-mini")</span>

<span class="sd">                        structured_llm = llm.with_structured_output(</span>
<span class="sd">                            ResponseSchema,</span>
<span class="sd">                            tools=[get_weather],</span>
<span class="sd">                            strict=True,</span>
<span class="sd">                            include_raw=True,</span>
<span class="sd">                        )</span>

<span class="sd">                        structured_llm.invoke("What's the weather in Boston?")</span>

<span class="sd">                    .. code-block:: python</span>

<span class="sd">                        {</span>
<span class="sd">                            "raw": AIMessage(content="", tool_calls=[...], ...),</span>
<span class="sd">                            "parsing_error": None,</span>
<span class="sd">                            "parsed": None,</span>
<span class="sd">                        }</span>

<span class="sd">            kwargs: Additional keyword args are passed through to the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.</span>

<span class="sd">            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.</span>

<span class="sd">            | If ``include_raw`` is True, then Runnable outputs a dict with keys:</span>

<span class="sd">            - "raw": BaseMessage</span>
<span class="sd">            - "parsed": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.</span>
<span class="sd">            - "parsing_error": Optional[BaseException]</span>

<span class="sd">        .. versionchanged:: 0.1.20</span>

<span class="sd">            Added support for TypedDict class ``schema``.</span>

<span class="sd">        .. versionchanged:: 0.1.21</span>

<span class="sd">            Support for ``strict`` argument added.</span>
<span class="sd">            Support for ``method`` = "json_schema" added.</span>

<span class="sd">        .. versionchanged:: 0.3.12</span>
<span class="sd">            Support for ``tools`` added.</span>

<span class="sd">        .. versionchanged:: 0.3.21</span>
<span class="sd">            Pass ``kwargs`` through to the model.</span>
<span class="sd">        """</span>  <span class="c1"># noqa: E501</span>
        <span class="k">if</span> <span class="n">strict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"json_mode"</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Argument `strict` is not supported with `method`='json_mode'"</span>
            <span class="p">)</span>
        <span class="n">is_pydantic_schema</span> <span class="o">=</span> <span class="n">_is_pydantic_class</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"json_schema"</span><span class="p">:</span>
            <span class="c1"># Check for Pydantic BaseModel V1</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">is_pydantic_schema</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">BaseModelV1</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">"Received a Pydantic BaseModel V1 schema. This is not supported by "</span>
                    <span class="s1">'method="json_schema". Please use method="function_calling" '</span>
                    <span class="s2">"or specify schema via JSON Schema or Pydantic V2 BaseModel. "</span>
                    <span class="s1">'Overriding to method="function_calling".'</span>
                <span class="p">)</span>
                <span class="n">method</span> <span class="o">=</span> <span class="s2">"function_calling"</span>
            <span class="c1"># Check for incompatible model</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-3"</span><span class="p">)</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-4-"</span><span class="p">)</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="s2">"gpt-4"</span>
            <span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Cannot use method='json_schema' with model </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> "</span>
                    <span class="sa">f</span><span class="s2">"since it doesn't support OpenAI's Structured Output API. You can "</span>
                    <span class="sa">f</span><span class="s2">"see supported models here: "</span>
                    <span class="sa">f</span><span class="s2">"https://platform.openai.com/docs/guides/structured-outputs#supported-models. "</span>  <span class="c1"># noqa: E501</span>
                    <span class="s2">"To fix this warning, set `method='function_calling'. "</span>
                    <span class="s2">"Overriding to method='function_calling'."</span>
                <span class="p">)</span>
                <span class="n">method</span> <span class="o">=</span> <span class="s2">"function_calling"</span>

        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"function_calling"</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">"schema must be specified when method is not 'json_mode'. "</span>
                    <span class="s2">"Received None."</span>
                <span class="p">)</span>
            <span class="n">tool_name</span> <span class="o">=</span> <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">schema</span><span class="p">)[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]</span>
            <span class="n">bind_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filter_disabled_params</span><span class="p">(</span>
                <span class="o">**</span><span class="p">{</span>
                    <span class="o">**</span><span class="nb">dict</span><span class="p">(</span>
                        <span class="n">tool_choice</span><span class="o">=</span><span class="n">tool_name</span><span class="p">,</span>
                        <span class="n">parallel_tool_calls</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
                        <span class="n">ls_structured_output_format</span><span class="o">=</span><span class="p">{</span>
                            <span class="s2">"kwargs"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"method"</span><span class="p">:</span> <span class="n">method</span><span class="p">,</span> <span class="s2">"strict"</span><span class="p">:</span> <span class="n">strict</span><span class="p">},</span>
                            <span class="s2">"schema"</span><span class="p">:</span> <span class="n">schema</span><span class="p">,</span>
                        <span class="p">},</span>
                    <span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>

            <span class="n">llm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">schema</span><span class="p">],</span> <span class="o">**</span><span class="n">bind_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_pydantic_schema</span><span class="p">:</span>
                <span class="n">output_parser</span><span class="p">:</span> <span class="n">Runnable</span> <span class="o">=</span> <span class="n">PydanticToolsParser</span><span class="p">(</span>
                    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">schema</span><span class="p">],</span>  <span class="c1"># type: ignore[list-item]</span>
                    <span class="n">first_tool_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># type: ignore[list-item]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_parser</span> <span class="o">=</span> <span class="n">JsonOutputKeyToolsParser</span><span class="p">(</span>
                    <span class="n">key_name</span><span class="o">=</span><span class="n">tool_name</span><span class="p">,</span> <span class="n">first_tool_only</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"json_mode"</span><span class="p">:</span>
            <span class="n">llm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
                <span class="o">**</span><span class="p">{</span>
                    <span class="o">**</span><span class="nb">dict</span><span class="p">(</span>
                        <span class="n">response_format</span><span class="o">=</span><span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_object"</span><span class="p">},</span>
                        <span class="n">ls_structured_output_format</span><span class="o">=</span><span class="p">{</span>
                            <span class="s2">"kwargs"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"method"</span><span class="p">:</span> <span class="n">method</span><span class="p">},</span>
                            <span class="s2">"schema"</span><span class="p">:</span> <span class="n">schema</span><span class="p">,</span>
                        <span class="p">},</span>
                    <span class="p">),</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="n">output_parser</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">schema</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="k">if</span> <span class="n">is_pydantic_schema</span>
                <span class="k">else</span> <span class="n">JsonOutputParser</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"json_schema"</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">"schema must be specified when method is not 'json_mode'. "</span>
                    <span class="s2">"Received None."</span>
                <span class="p">)</span>
            <span class="n">response_format</span> <span class="o">=</span> <span class="n">_convert_to_openai_response_format</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>
            <span class="n">bind_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="o">**</span><span class="nb">dict</span><span class="p">(</span>
                    <span class="n">response_format</span><span class="o">=</span><span class="n">response_format</span><span class="p">,</span>
                    <span class="n">ls_structured_output_format</span><span class="o">=</span><span class="p">{</span>
                        <span class="s2">"kwargs"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"method"</span><span class="p">:</span> <span class="n">method</span><span class="p">,</span> <span class="s2">"strict"</span><span class="p">:</span> <span class="n">strict</span><span class="p">},</span>
                        <span class="s2">"schema"</span><span class="p">:</span> <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">schema</span><span class="p">),</span>
                    <span class="p">},</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="n">tools</span><span class="p">:</span>
                <span class="n">bind_kwargs</span><span class="p">[</span><span class="s2">"tools"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tools</span>
                <span class="p">]</span>
            <span class="n">llm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">**</span><span class="n">bind_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_pydantic_schema</span><span class="p">:</span>
                <span class="n">output_parser</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span>
                    <span class="n">partial</span><span class="p">(</span><span class="n">_oai_structured_outputs_parser</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="nb">type</span><span class="p">,</span> <span class="n">schema</span><span class="p">))</span>
                <span class="p">)</span><span class="o">.</span><span class="n">with_types</span><span class="p">(</span><span class="n">output_type</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="nb">type</span><span class="p">,</span> <span class="n">schema</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_parser</span> <span class="o">=</span> <span class="n">JsonOutputParser</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Unrecognized method argument. Expected one of 'function_calling' or "</span>
                <span class="sa">f</span><span class="s2">"'json_mode'. Received: '</span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">'"</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">include_raw</span><span class="p">:</span>
            <span class="n">parser_assign</span> <span class="o">=</span> <span class="n">RunnablePassthrough</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
                <span class="n">parsed</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="s2">"raw"</span><span class="p">)</span> <span class="o">|</span> <span class="n">output_parser</span><span class="p">,</span> <span class="n">parsing_error</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="n">parser_none</span> <span class="o">=</span> <span class="n">RunnablePassthrough</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">parsed</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">parser_with_fallback</span> <span class="o">=</span> <span class="n">parser_assign</span><span class="o">.</span><span class="n">with_fallbacks</span><span class="p">(</span>
                <span class="p">[</span><span class="n">parser_none</span><span class="p">],</span> <span class="n">exception_key</span><span class="o">=</span><span class="s2">"parsing_error"</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">RunnableMap</span><span class="p">(</span><span class="n">raw</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span> <span class="o">|</span> <span class="n">parser_with_fallback</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">output_parser</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_filter_disabled_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">disabled_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kwargs</span>
        <span class="n">filtered</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Skip param</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">disabled_params</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">disabled_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">disabled_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">continue</span>
            <span class="c1"># Keep param</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">filtered</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="k">return</span> <span class="n">filtered</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_generation_chunk_from_completion</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">completion</span><span class="p">:</span> <span class="n">openai</span><span class="o">.</span><span class="n">BaseModel</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatGenerationChunk</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Get chunk from completion (e.g., from final completion of a stream)."""</span>
        <span class="n">chat_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chat_result</span><span class="p">(</span><span class="n">completion</span><span class="p">)</span>
        <span class="n">chat_message</span> <span class="o">=</span> <span class="n">chat_result</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chat_message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">chat_message</span><span class="o">.</span><span class="n">usage_metadata</span>
            <span class="c1"># Skip tool_calls, already sent as chunks</span>
            <span class="k">if</span> <span class="s2">"tool_calls"</span> <span class="ow">in</span> <span class="n">chat_message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
                <span class="n">chat_message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"tool_calls"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">usage_metadata</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">message</span> <span class="o">=</span> <span class="n">AIMessageChunk</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">chat_message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage_metadata</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">generation_info</span><span class="o">=</span><span class="n">chat_result</span><span class="o">.</span><span class="n">llm_output</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="ChatOpenAI">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_cerebras.chat_models.ChatOpenAI">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">BaseChatOpenAI</span><span class="p">):</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">    </span><span class="sd">"""OpenAI chat model integration.</span>

<span class="sd">    .. dropdown:: Setup</span>
<span class="sd">        :open:</span>

<span class="sd">        Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.</span>

<span class="sd">        .. code-block:: bash</span>

<span class="sd">            pip install -U langchain-openai</span>
<span class="sd">            export OPENAI_API_KEY="your-api-key"</span>

<span class="sd">    .. dropdown:: Key init args â€” completion params</span>

<span class="sd">        model: str</span>
<span class="sd">            Name of OpenAI model to use.</span>
<span class="sd">        temperature: float</span>
<span class="sd">            Sampling temperature.</span>
<span class="sd">        max_tokens: Optional[int]</span>
<span class="sd">            Max number of tokens to generate.</span>
<span class="sd">        logprobs: Optional[bool]</span>
<span class="sd">            Whether to return logprobs.</span>
<span class="sd">        stream_options: Dict</span>
<span class="sd">            Configure streaming outputs, like whether to return token usage when</span>
<span class="sd">            streaming (``{"include_usage": True}``).</span>
<span class="sd">        use_responses_api: Optional[bool]</span>
<span class="sd">            Whether to use the responses API.</span>

<span class="sd">        See full list of supported init args and their descriptions in the params section.</span>

<span class="sd">    .. dropdown:: Key init args â€” client params</span>

<span class="sd">        timeout: Union[float, Tuple[float, float], Any, None]</span>
<span class="sd">            Timeout for requests.</span>
<span class="sd">        max_retries: Optional[int]</span>
<span class="sd">            Max number of retries.</span>
<span class="sd">        api_key: Optional[str]</span>
<span class="sd">            OpenAI API key. If not passed in will be read from env var ``OPENAI_API_KEY``.</span>
<span class="sd">        base_url: Optional[str]</span>
<span class="sd">            Base URL for API requests. Only specify if using a proxy or service</span>
<span class="sd">            emulator.</span>
<span class="sd">        organization: Optional[str]</span>
<span class="sd">            OpenAI organization ID. If not passed in will be read from env</span>
<span class="sd">            var ``OPENAI_ORG_ID``.</span>

<span class="sd">        See full list of supported init args and their descriptions in the params section.</span>

<span class="sd">    .. dropdown:: Instantiate</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>

<span class="sd">            llm = ChatOpenAI(</span>
<span class="sd">                model="gpt-4o",</span>
<span class="sd">                temperature=0,</span>
<span class="sd">                max_tokens=None,</span>
<span class="sd">                timeout=None,</span>
<span class="sd">                max_retries=2,</span>
<span class="sd">                # api_key="...",</span>
<span class="sd">                # base_url="...",</span>
<span class="sd">                # organization="...",</span>
<span class="sd">                # other params...</span>
<span class="sd">            )</span>

<span class="sd">        **NOTE**: Any param which is not explicitly supported will be passed directly to the</span>
<span class="sd">        ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is</span>
<span class="sd">        invoked. For example:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>
<span class="sd">            import openai</span>

<span class="sd">            ChatOpenAI(..., frequency_penalty=0.2).invoke(...)</span>

<span class="sd">            # results in underlying API call of:</span>

<span class="sd">            openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)</span>

<span class="sd">            # which is also equivalent to:</span>

<span class="sd">            ChatOpenAI(...).invoke(..., frequency_penalty=0.2)</span>

<span class="sd">    .. dropdown:: Invoke</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            messages = [</span>
<span class="sd">                (</span>
<span class="sd">                    "system",</span>
<span class="sd">                    "You are a helpful translator. Translate the user sentence to French.",</span>
<span class="sd">                ),</span>
<span class="sd">                ("human", "I love programming."),</span>
<span class="sd">            ]</span>
<span class="sd">            llm.invoke(messages)</span>

<span class="sd">        .. code-block:: pycon</span>

<span class="sd">            AIMessage(</span>
<span class="sd">                content="J'adore la programmation.",</span>
<span class="sd">                response_metadata={</span>
<span class="sd">                    "token_usage": {</span>
<span class="sd">                        "completion_tokens": 5,</span>
<span class="sd">                        "prompt_tokens": 31,</span>
<span class="sd">                        "total_tokens": 36,</span>
<span class="sd">                    },</span>
<span class="sd">                    "model_name": "gpt-4o",</span>
<span class="sd">                    "system_fingerprint": "fp_43dfabdef1",</span>
<span class="sd">                    "finish_reason": "stop",</span>
<span class="sd">                    "logprobs": None,</span>
<span class="sd">                },</span>
<span class="sd">                id="run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0",</span>
<span class="sd">                usage_metadata={"input_tokens": 31, "output_tokens": 5, "total_tokens": 36},</span>
<span class="sd">            )</span>

<span class="sd">    .. dropdown:: Stream</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            for chunk in llm.stream(messages):</span>
<span class="sd">                print(chunk.text(), end="")</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessageChunk(content="", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(content="J", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(content="'adore", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(content=" la", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(</span>
<span class="sd">                content=" programmation", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span>
<span class="sd">            )</span>
<span class="sd">            AIMessageChunk(content=".", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(</span>
<span class="sd">                content="",</span>
<span class="sd">                response_metadata={"finish_reason": "stop"},</span>
<span class="sd">                id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0",</span>
<span class="sd">            )</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            stream = llm.stream(messages)</span>
<span class="sd">            full = next(stream)</span>
<span class="sd">            for chunk in stream:</span>
<span class="sd">                full += chunk</span>
<span class="sd">            full</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessageChunk(</span>
<span class="sd">                content="J'adore la programmation.",</span>
<span class="sd">                response_metadata={"finish_reason": "stop"},</span>
<span class="sd">                id="run-bf917526-7f58-4683-84f7-36a6b671d140",</span>
<span class="sd">            )</span>

<span class="sd">    .. dropdown:: Async</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            await llm.ainvoke(messages)</span>

<span class="sd">            # stream:</span>
<span class="sd">            # async for chunk in (await llm.astream(messages))</span>

<span class="sd">            # batch:</span>
<span class="sd">            # await llm.abatch([messages])</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessage(</span>
<span class="sd">                content="J'adore la programmation.",</span>
<span class="sd">                response_metadata={</span>
<span class="sd">                    "token_usage": {</span>
<span class="sd">                        "completion_tokens": 5,</span>
<span class="sd">                        "prompt_tokens": 31,</span>
<span class="sd">                        "total_tokens": 36,</span>
<span class="sd">                    },</span>
<span class="sd">                    "model_name": "gpt-4o",</span>
<span class="sd">                    "system_fingerprint": "fp_43dfabdef1",</span>
<span class="sd">                    "finish_reason": "stop",</span>
<span class="sd">                    "logprobs": None,</span>
<span class="sd">                },</span>
<span class="sd">                id="run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0",</span>
<span class="sd">                usage_metadata={"input_tokens": 31, "output_tokens": 5, "total_tokens": 36},</span>
<span class="sd">            )</span>

<span class="sd">    .. dropdown:: Tool calling</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from pydantic import BaseModel, Field</span>


<span class="sd">            class GetWeather(BaseModel):</span>
<span class="sd">                '''Get the current weather in a given location'''</span>

<span class="sd">                location: str = Field(</span>
<span class="sd">                    ..., description="The city and state, e.g. San Francisco, CA"</span>
<span class="sd">                )</span>


<span class="sd">            class GetPopulation(BaseModel):</span>
<span class="sd">                '''Get the current population in a given location'''</span>

<span class="sd">                location: str = Field(</span>
<span class="sd">                    ..., description="The city and state, e.g. San Francisco, CA"</span>
<span class="sd">                )</span>


<span class="sd">            llm_with_tools = llm.bind_tools(</span>
<span class="sd">                [GetWeather, GetPopulation]</span>
<span class="sd">                # strict = True  # enforce tool args schema is respected</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg = llm_with_tools.invoke(</span>
<span class="sd">                "Which city is hotter today and which is bigger: LA or NY?"</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg.tool_calls</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            [</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetWeather",</span>
<span class="sd">                    "args": {"location": "Los Angeles, CA"},</span>
<span class="sd">                    "id": "call_6XswGD5Pqk8Tt5atYr7tfenU",</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetWeather",</span>
<span class="sd">                    "args": {"location": "New York, NY"},</span>
<span class="sd">                    "id": "call_ZVL15vA8Y7kXqOy3dtmQgeCi",</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetPopulation",</span>
<span class="sd">                    "args": {"location": "Los Angeles, CA"},</span>
<span class="sd">                    "id": "call_49CFW8zqC9W7mh7hbMLSIrXw",</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetPopulation",</span>
<span class="sd">                    "args": {"location": "New York, NY"},</span>
<span class="sd">                    "id": "call_6ghfKxV264jEfe1mRIkS3PE7",</span>
<span class="sd">                },</span>
<span class="sd">            ]</span>

<span class="sd">        Note that ``openai &gt;= 1.32`` supports a ``parallel_tool_calls`` parameter</span>
<span class="sd">        that defaults to ``True``. This parameter can be set to ``False`` to</span>
<span class="sd">        disable parallel tool calls:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm_with_tools.invoke(</span>
<span class="sd">                "What is the weather in LA and NY?", parallel_tool_calls=False</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg.tool_calls</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            [</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetWeather",</span>
<span class="sd">                    "args": {"location": "Los Angeles, CA"},</span>
<span class="sd">                    "id": "call_4OoY0ZR99iEvC7fevsH8Uhtz",</span>
<span class="sd">                }</span>
<span class="sd">            ]</span>

<span class="sd">        Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model</span>
<span class="sd">        using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by</span>
<span class="sd">        setting ``model_kwargs``.</span>

<span class="sd">        See ``ChatOpenAI.bind_tools()`` method for more.</span>

<span class="sd">    .. dropdown:: Built-in tools</span>

<span class="sd">        .. versionadded:: 0.3.9</span>

<span class="sd">        You can access `built-in tools &lt;https://platform.openai.com/docs/guides/tools?api-mode=responses&gt;`_</span>
<span class="sd">        supported by the OpenAI Responses API. See LangChain</span>
<span class="sd">        `docs &lt;https://python.langchain.com/docs/integrations/chat/openai/&gt;`_ for more</span>
<span class="sd">        detail.</span>

<span class="sd">        .. note::</span>
<span class="sd">            ``langchain-openai &gt;= 0.3.26`` allows users to opt-in to an updated</span>
<span class="sd">            AIMessage format when using the Responses API. Setting</span>

<span class="sd">            ..  code-block:: python</span>

<span class="sd">                llm = ChatOpenAI(model="...", output_version="responses/v1")</span>

<span class="sd">            will format output from reasoning summaries, built-in tool invocations, and</span>
<span class="sd">            other response items into the message's ``content`` field, rather than</span>
<span class="sd">            ``additional_kwargs``. We recommend this format for new applications.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>

<span class="sd">            llm = ChatOpenAI(model="gpt-4.1-mini", output_version="responses/v1")</span>

<span class="sd">            tool = {"type": "web_search_preview"}</span>
<span class="sd">            llm_with_tools = llm.bind_tools([tool])</span>

<span class="sd">            response = llm_with_tools.invoke("What was a positive news story from today?")</span>
<span class="sd">            response.content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            [</span>
<span class="sd">                {</span>
<span class="sd">                    "type": "text",</span>
<span class="sd">                    "text": "Today, a heartwarming story emerged from ...",</span>
<span class="sd">                    "annotations": [</span>
<span class="sd">                        {</span>
<span class="sd">                            "end_index": 778,</span>
<span class="sd">                            "start_index": 682,</span>
<span class="sd">                            "title": "Title of story",</span>
<span class="sd">                            "type": "url_citation",</span>
<span class="sd">                            "url": "&lt;url of story&gt;",</span>
<span class="sd">                        }</span>
<span class="sd">                    ],</span>
<span class="sd">                }</span>
<span class="sd">            ]</span>

<span class="sd">    .. dropdown:: Managing conversation state</span>

<span class="sd">        .. versionadded:: 0.3.9</span>

<span class="sd">        OpenAI's Responses API supports management of</span>
<span class="sd">        `conversation state &lt;https://platform.openai.com/docs/guides/conversation-state?api-mode=responses&gt;`_.</span>
<span class="sd">        Passing in response IDs from previous messages will continue a conversational</span>
<span class="sd">        thread. See LangChain</span>
<span class="sd">        `docs &lt;https://python.langchain.com/docs/integrations/chat/openai/&gt;`_ for more</span>
<span class="sd">        detail.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>

<span class="sd">            llm = ChatOpenAI(model="gpt-4.1-mini", use_responses_api=True)</span>
<span class="sd">            response = llm.invoke("Hi, I'm Bob.")</span>
<span class="sd">            response.text()</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            "Hi Bob! How can I assist you today?"</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            second_response = llm.invoke(</span>
<span class="sd">                "What is my name?", previous_response_id=response.response_metadata["id"]</span>
<span class="sd">            )</span>
<span class="sd">            second_response.text()</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            "Your name is Bob. How can I help you today, Bob?"</span>

<span class="sd">        .. versionadded:: 0.3.26</span>

<span class="sd">        You can also initialize ChatOpenAI with :attr:`use_previous_response_id`.</span>
<span class="sd">        Input messages up to the most recent response will then be dropped from request</span>
<span class="sd">        payloads, and ``previous_response_id`` will be set using the ID of the most</span>
<span class="sd">        recent response.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            llm = ChatOpenAI(model="gpt-4.1-mini", use_previous_response_id=True)</span>

<span class="sd">    .. dropdown:: Reasoning output</span>

<span class="sd">        OpenAI's Responses API supports `reasoning models &lt;https://platform.openai.com/docs/guides/reasoning?api-mode=responses&gt;`_</span>
<span class="sd">        that expose a summary of internal reasoning processes.</span>

<span class="sd">        .. note::</span>
<span class="sd">            ``langchain-openai &gt;= 0.3.26`` allows users to opt-in to an updated</span>
<span class="sd">            AIMessage format when using the Responses API. Setting</span>

<span class="sd">            ..  code-block:: python</span>

<span class="sd">                llm = ChatOpenAI(model="...", output_version="responses/v1")</span>

<span class="sd">            will format output from reasoning summaries, built-in tool invocations, and</span>
<span class="sd">            other response items into the message's ``content`` field, rather than</span>
<span class="sd">            ``additional_kwargs``. We recommend this format for new applications.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>

<span class="sd">            reasoning = {</span>
<span class="sd">                "effort": "medium",  # 'low', 'medium', or 'high'</span>
<span class="sd">                "summary": "auto",  # 'detailed', 'auto', or None</span>
<span class="sd">            }</span>

<span class="sd">            llm = ChatOpenAI(</span>
<span class="sd">                model="o4-mini", reasoning=reasoning, output_version="responses/v1"</span>
<span class="sd">            )</span>
<span class="sd">            response = llm.invoke("What is 3^3?")</span>

<span class="sd">            # Response text</span>
<span class="sd">            print(f"Output: {response.text()}")</span>

<span class="sd">            # Reasoning summaries</span>
<span class="sd">            for block in response.content:</span>
<span class="sd">                if block["type"] == "reasoning":</span>
<span class="sd">                    for summary in block["summary"]:</span>
<span class="sd">                        print(summary["text"])</span>

<span class="sd">        .. code-block:: none</span>

<span class="sd">            Output: 3Â³ = 27</span>
<span class="sd">            Reasoning: The user wants to know...</span>

<span class="sd">    .. dropdown:: Structured output</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from typing import Optional</span>

<span class="sd">            from pydantic import BaseModel, Field</span>


<span class="sd">            class Joke(BaseModel):</span>
<span class="sd">                '''Joke to tell user.'''</span>

<span class="sd">                setup: str = Field(description="The setup of the joke")</span>
<span class="sd">                punchline: str = Field(description="The punchline to the joke")</span>
<span class="sd">                rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")</span>


<span class="sd">            structured_llm = llm.with_structured_output(Joke)</span>
<span class="sd">            structured_llm.invoke("Tell me a joke about cats")</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            Joke(</span>
<span class="sd">                setup="Why was the cat sitting on the computer?",</span>
<span class="sd">                punchline="To keep an eye on the mouse!",</span>
<span class="sd">                rating=None,</span>
<span class="sd">            )</span>

<span class="sd">        See ``ChatOpenAI.with_structured_output()`` for more.</span>

<span class="sd">    .. dropdown:: JSON mode</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            json_llm = llm.bind(response_format={"type": "json_object"})</span>
<span class="sd">            ai_msg = json_llm.invoke(</span>
<span class="sd">                "Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]"</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg.content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            '\\n{\\n  "random_ints": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\n}'</span>

<span class="sd">    .. dropdown:: Image input</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import base64</span>
<span class="sd">            import httpx</span>
<span class="sd">            from langchain_core.messages import HumanMessage</span>

<span class="sd">            image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"</span>
<span class="sd">            image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")</span>
<span class="sd">            message = HumanMessage(</span>
<span class="sd">                content=[</span>
<span class="sd">                    {"type": "text", "text": "describe the weather in this image"},</span>
<span class="sd">                    {</span>
<span class="sd">                        "type": "image_url",</span>
<span class="sd">                        "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},</span>
<span class="sd">                    },</span>
<span class="sd">                ]</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg = llm.invoke([message])</span>
<span class="sd">            ai_msg.content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            "The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions."</span>

<span class="sd">    .. dropdown:: Token usage</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm.invoke(messages)</span>
<span class="sd">            ai_msg.usage_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {"input_tokens": 28, "output_tokens": 5, "total_tokens": 33}</span>

<span class="sd">        When streaming, set the ``stream_usage`` kwarg:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            stream = llm.stream(messages, stream_usage=True)</span>
<span class="sd">            full = next(stream)</span>
<span class="sd">            for chunk in stream:</span>
<span class="sd">                full += chunk</span>
<span class="sd">            full.usage_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {"input_tokens": 28, "output_tokens": 5, "total_tokens": 33}</span>

<span class="sd">        Alternatively, setting ``stream_usage`` when instantiating the model can be</span>
<span class="sd">        useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using</span>
<span class="sd">        methods like ``.with_structured_output``, which generate chains under the</span>
<span class="sd">        hood.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            llm = ChatOpenAI(model="gpt-4o", stream_usage=True)</span>
<span class="sd">            structured_llm = llm.with_structured_output(...)</span>

<span class="sd">    .. dropdown:: Logprobs</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            logprobs_llm = llm.bind(logprobs=True)</span>
<span class="sd">            ai_msg = logprobs_llm.invoke(messages)</span>
<span class="sd">            ai_msg.response_metadata["logprobs"]</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {</span>
<span class="sd">                "content": [</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": "J",</span>
<span class="sd">                        "bytes": [74],</span>
<span class="sd">                        "logprob": -4.9617593e-06,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": "'adore",</span>
<span class="sd">                        "bytes": [39, 97, 100, 111, 114, 101],</span>
<span class="sd">                        "logprob": -0.25202933,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": " la",</span>
<span class="sd">                        "bytes": [32, 108, 97],</span>
<span class="sd">                        "logprob": -0.20141791,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": " programmation",</span>
<span class="sd">                        "bytes": [</span>
<span class="sd">                            32,</span>
<span class="sd">                            112,</span>
<span class="sd">                            114,</span>
<span class="sd">                            111,</span>
<span class="sd">                            103,</span>
<span class="sd">                            114,</span>
<span class="sd">                            97,</span>
<span class="sd">                            109,</span>
<span class="sd">                            109,</span>
<span class="sd">                            97,</span>
<span class="sd">                            116,</span>
<span class="sd">                            105,</span>
<span class="sd">                            111,</span>
<span class="sd">                            110,</span>
<span class="sd">                        ],</span>
<span class="sd">                        "logprob": -1.9361265e-07,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": ".",</span>
<span class="sd">                        "bytes": [46],</span>
<span class="sd">                        "logprob": -1.2233183e-05,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                ]</span>
<span class="sd">            }</span>

<span class="sd">    .. dropdown:: Response metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm.invoke(messages)</span>
<span class="sd">            ai_msg.response_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {</span>
<span class="sd">                "token_usage": {</span>
<span class="sd">                    "completion_tokens": 5,</span>
<span class="sd">                    "prompt_tokens": 28,</span>
<span class="sd">                    "total_tokens": 33,</span>
<span class="sd">                },</span>
<span class="sd">                "model_name": "gpt-4o",</span>
<span class="sd">                "system_fingerprint": "fp_319be4768e",</span>
<span class="sd">                "finish_reason": "stop",</span>
<span class="sd">                "logprobs": None,</span>
<span class="sd">            }</span>

<span class="sd">    .. dropdown:: Flex processing</span>

<span class="sd">        OpenAI offers a variety of</span>
<span class="sd">        `service tiers &lt;https://platform.openai.com/docs/guides/flex-processing&gt;`_.</span>
<span class="sd">        The "flex" tier offers cheaper pricing for requests, with the trade-off that</span>
<span class="sd">        responses may take longer and resources might not always be available.</span>
<span class="sd">        This approach is best suited for non-critical tasks, including model testing,</span>
<span class="sd">        data enhancement, or jobs that can be run asynchronously.</span>

<span class="sd">        To use it, initialize the model with ``service_tier="flex"``:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>

<span class="sd">            llm = ChatOpenAI(model="o4-mini", service_tier="flex")</span>

<span class="sd">        Note that this is a beta feature that is only available for a subset of models.</span>
<span class="sd">        See OpenAI `docs &lt;https://platform.openai.com/docs/guides/flex-processing&gt;`_</span>
<span class="sd">        for more detail.</span>

<span class="sd">    """</span>  <span class="c1"># noqa: E501</span>

    <span class="n">max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"max_completion_tokens"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Maximum number of tokens to generate."""</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lc_secrets</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"openai_api_key"</span><span class="p">:</span> <span class="s2">"OPENAI_API_KEY"</span><span class="p">}</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lc_namespace</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the namespace of the langchain object."""</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">"langchain"</span><span class="p">,</span> <span class="s2">"chat_models"</span><span class="p">,</span> <span class="s2">"openai"</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lc_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">attributes</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_organization</span><span class="p">:</span>
            <span class="n">attributes</span><span class="p">[</span><span class="s2">"openai_organization"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_organization</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span><span class="p">:</span>
            <span class="n">attributes</span><span class="p">[</span><span class="s2">"openai_api_base"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span><span class="p">:</span>
            <span class="n">attributes</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span>

        <span class="k">return</span> <span class="n">attributes</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_lc_serializable</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Return whether this model can be serialized by Langchain."""</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_default_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the default parameters for calling OpenAI API."""</span>
        <span class="n">params</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_default_params</span>
        <span class="k">if</span> <span class="s2">"max_tokens"</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">params</span><span class="p">[</span><span class="s2">"max_completion_tokens"</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"max_tokens"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_request_payload</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_</span><span class="p">:</span> <span class="n">LanguageModelInput</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># max_tokens was deprecated in favor of max_completion_tokens</span>
        <span class="c1"># in September 2024 release</span>
        <span class="k">if</span> <span class="s2">"max_tokens"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"max_completion_tokens"</span><span class="p">]</span> <span class="o">=</span> <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"max_tokens"</span><span class="p">)</span>

        <span class="c1"># Mutate system message role to "developer" for o-series models</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="ow">and</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s2">"^o\d"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">payload</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"messages"</span><span class="p">,</span> <span class="p">[]):</span>
                <span class="k">if</span> <span class="n">message</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"system"</span><span class="p">:</span>
                    <span class="n">message</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"developer"</span>
        <span class="k">return</span> <span class="n">payload</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Route to Chat Completions or Responses API."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_responses_api</span><span class="p">({</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">}):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_stream_responses</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_stream</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_astream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Route to Chat Completions or Responses API."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_responses_api</span><span class="p">({</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">}):</span>
            <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_astream_responses</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">chunk</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_astream</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">chunk</span>

<div class="viewcode-block" id="ChatOpenAI.with_structured_output">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_cerebras.chat_models.ChatOpenAI.with_structured_output">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">with_structured_output</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_DictOrPydanticClass</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"function_calling"</span><span class="p">,</span> <span class="s2">"json_mode"</span><span class="p">,</span> <span class="s2">"json_schema"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"json_schema"</span><span class="p">,</span>
        <span class="n">include_raw</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">_DictOrPydantic</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Model wrapper that returns outputs formatted to match the given schema.</span>

<span class="sd">        Args:</span>
<span class="sd">            schema:</span>
<span class="sd">                The output schema. Can be passed in as:</span>

<span class="sd">                - a JSON Schema,</span>
<span class="sd">                - a TypedDict class,</span>
<span class="sd">                - or a Pydantic class,</span>
<span class="sd">                - an OpenAI function/tool schema.</span>

<span class="sd">                If ``schema`` is a Pydantic class then the model output will be a</span>
<span class="sd">                Pydantic instance of that class, and the model-generated fields will be</span>
<span class="sd">                validated by the Pydantic class. Otherwise the model output will be a</span>
<span class="sd">                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`</span>
<span class="sd">                for more on how to properly specify types and descriptions of</span>
<span class="sd">                schema fields when specifying a Pydantic or TypedDict class.</span>

<span class="sd">            method: The method for steering model generation, one of:</span>

<span class="sd">                - "json_schema":</span>
<span class="sd">                    Uses OpenAI's Structured Output API:</span>
<span class="sd">                    https://platform.openai.com/docs/guides/structured-outputs</span>
<span class="sd">                    Supported for "gpt-4o-mini", "gpt-4o-2024-08-06", "o1", and later</span>
<span class="sd">                    models.</span>
<span class="sd">                - "function_calling":</span>
<span class="sd">                    Uses OpenAI's tool-calling (formerly called function calling)</span>
<span class="sd">                    API: https://platform.openai.com/docs/guides/function-calling</span>
<span class="sd">                - "json_mode":</span>
<span class="sd">                    Uses OpenAI's JSON mode. Note that if using JSON mode then you</span>
<span class="sd">                    must include instructions for formatting the output into the</span>
<span class="sd">                    desired schema into the model call:</span>
<span class="sd">                    https://platform.openai.com/docs/guides/structured-outputs/json-mode</span>

<span class="sd">                Learn more about the differences between the methods and which models</span>
<span class="sd">                support which methods here:</span>

<span class="sd">                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode</span>
<span class="sd">                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format</span>

<span class="sd">            include_raw:</span>
<span class="sd">                If False then only the parsed structured output is returned. If</span>
<span class="sd">                an error occurs during model output parsing it will be raised. If True</span>
<span class="sd">                then both the raw model response (a BaseMessage) and the parsed model</span>
<span class="sd">                response will be returned. If an error occurs during output parsing it</span>
<span class="sd">                will be caught and returned as well. The final output is always a dict</span>
<span class="sd">                with keys "raw", "parsed", and "parsing_error".</span>
<span class="sd">            strict:</span>

<span class="sd">                - True:</span>
<span class="sd">                    Model output is guaranteed to exactly match the schema.</span>
<span class="sd">                    The input schema will also be validated according to</span>
<span class="sd">                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</span>
<span class="sd">                - False:</span>
<span class="sd">                    Input schema will not be validated and model output will not be</span>
<span class="sd">                    validated.</span>
<span class="sd">                - None:</span>
<span class="sd">                    ``strict`` argument will not be passed to the model.</span>

<span class="sd">                If schema is specified via TypedDict or JSON schema, ``strict`` is not</span>
<span class="sd">                enabled by default. Pass ``strict=True`` to enable it.</span>

<span class="sd">                Note: ``strict`` can only be non-null if ``method`` is</span>
<span class="sd">                ``"json_schema"`` or ``"function_calling"``.</span>
<span class="sd">            tools:</span>
<span class="sd">                A list of tool-like objects to bind to the chat model. Requires that:</span>

<span class="sd">                - ``method`` is ``"json_schema"`` (default).</span>
<span class="sd">                - ``strict=True``</span>
<span class="sd">                - ``include_raw=True``</span>

<span class="sd">                If a model elects to call a</span>
<span class="sd">                tool, the resulting ``AIMessage`` in ``"raw"`` will include tool calls.</span>

<span class="sd">                .. dropdown:: Example</span>

<span class="sd">                    .. code-block:: python</span>

<span class="sd">                        from langchain.chat_models import init_chat_model</span>
<span class="sd">                        from pydantic import BaseModel</span>


<span class="sd">                        class ResponseSchema(BaseModel):</span>
<span class="sd">                            response: str</span>


<span class="sd">                        def get_weather(location: str) -&gt; str:</span>
<span class="sd">                            \"\"\"Get weather at a location.\"\"\"</span>
<span class="sd">                            pass</span>

<span class="sd">                        llm = init_chat_model("openai:gpt-4o-mini")</span>

<span class="sd">                        structured_llm = llm.with_structured_output(</span>
<span class="sd">                            ResponseSchema,</span>
<span class="sd">                            tools=[get_weather],</span>
<span class="sd">                            strict=True,</span>
<span class="sd">                            include_raw=True,</span>
<span class="sd">                        )</span>

<span class="sd">                        structured_llm.invoke("What's the weather in Boston?")</span>

<span class="sd">                    .. code-block:: python</span>

<span class="sd">                        {</span>
<span class="sd">                            "raw": AIMessage(content="", tool_calls=[...], ...),</span>
<span class="sd">                            "parsing_error": None,</span>
<span class="sd">                            "parsed": None,</span>
<span class="sd">                        }</span>

<span class="sd">            kwargs: Additional keyword args are passed through to the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.</span>

<span class="sd">            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.</span>

<span class="sd">            | If ``include_raw`` is True, then Runnable outputs a dict with keys:</span>

<span class="sd">            - "raw": BaseMessage</span>
<span class="sd">            - "parsed": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.</span>
<span class="sd">            - "parsing_error": Optional[BaseException]</span>

<span class="sd">        .. versionchanged:: 0.1.20</span>

<span class="sd">            Added support for TypedDict class ``schema``.</span>

<span class="sd">        .. versionchanged:: 0.1.21</span>

<span class="sd">            Support for ``strict`` argument added.</span>
<span class="sd">            Support for ``method="json_schema"`` added.</span>

<span class="sd">        .. versionchanged:: 0.3.0</span>

<span class="sd">            ``method`` default changed from "function_calling" to "json_schema".</span>

<span class="sd">        .. versionchanged:: 0.3.12</span>
<span class="sd">            Support for ``tools`` added.</span>

<span class="sd">        .. versionchanged:: 0.3.21</span>
<span class="sd">            Pass ``kwargs`` through to the model.</span>

<span class="sd">        .. dropdown:: Example: schema=Pydantic class, method="json_schema", include_raw=False, strict=True</span>

<span class="sd">            Note, OpenAI has a number of restrictions on what types of schemas can be</span>
<span class="sd">            provided if ``strict`` = True. When using Pydantic, our model cannot</span>
<span class="sd">            specify any Field metadata (like min/max constraints) and fields cannot</span>
<span class="sd">            have default values.</span>

<span class="sd">            See all constraints here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from typing import Optional</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>
<span class="sd">                from pydantic import BaseModel, Field</span>


<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>

<span class="sd">                    answer: str</span>
<span class="sd">                    justification: Optional[str] = Field(</span>
<span class="sd">                        default=..., description="A justification for the answer."</span>
<span class="sd">                    )</span>


<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(AnswerWithJustification)</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>

<span class="sd">                # -&gt; AnswerWithJustification(</span>
<span class="sd">                #     answer='They weigh the same',</span>
<span class="sd">                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'</span>
<span class="sd">                # )</span>

<span class="sd">        .. dropdown:: Example: schema=Pydantic class, method="function_calling", include_raw=False, strict=False</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from typing import Optional</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>
<span class="sd">                from pydantic import BaseModel, Field</span>


<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>

<span class="sd">                    answer: str</span>
<span class="sd">                    justification: Optional[str] = Field(</span>
<span class="sd">                        default=..., description="A justification for the answer."</span>
<span class="sd">                    )</span>


<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(</span>
<span class="sd">                    AnswerWithJustification, method="function_calling"</span>
<span class="sd">                )</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>

<span class="sd">                # -&gt; AnswerWithJustification(</span>
<span class="sd">                #     answer='They weigh the same',</span>
<span class="sd">                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'</span>
<span class="sd">                # )</span>

<span class="sd">        .. dropdown:: Example: schema=Pydantic class, method="json_schema", include_raw=True</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>
<span class="sd">                from pydantic import BaseModel</span>


<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>

<span class="sd">                    answer: str</span>
<span class="sd">                    justification: str</span>


<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(</span>
<span class="sd">                    AnswerWithJustification, include_raw=True</span>
<span class="sd">                )</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{"answer":"They weigh the same.","justification":"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ."}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),</span>
<span class="sd">                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),</span>
<span class="sd">                #     'parsing_error': None</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=TypedDict class, method="json_schema", include_raw=False, strict=False</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                # IMPORTANT: If you are using Python &lt;=3.8, you need to import Annotated</span>
<span class="sd">                # from typing_extensions, not from typing.</span>
<span class="sd">                from typing_extensions import Annotated, TypedDict</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>


<span class="sd">                class AnswerWithJustification(TypedDict):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>

<span class="sd">                    answer: str</span>
<span class="sd">                    justification: Annotated[</span>
<span class="sd">                        Optional[str], None, "A justification for the answer."</span>
<span class="sd">                    ]</span>


<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(AnswerWithJustification)</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'answer': 'They weigh the same',</span>
<span class="sd">                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=OpenAI function schema, method="json_schema", include_raw=False</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>

<span class="sd">                oai_schema = {</span>
<span class="sd">                    'name': 'AnswerWithJustification',</span>
<span class="sd">                    'description': 'An answer to the user question along with justification for the answer.',</span>
<span class="sd">                    'parameters': {</span>
<span class="sd">                        'type': 'object',</span>
<span class="sd">                        'properties': {</span>
<span class="sd">                            'answer': {'type': 'string'},</span>
<span class="sd">                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}</span>
<span class="sd">                        },</span>
<span class="sd">                       'required': ['answer']</span>
<span class="sd">                   }</span>
<span class="sd">               }</span>

<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(oai_schema)</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'answer': 'They weigh the same',</span>
<span class="sd">                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=Pydantic class, method="json_mode", include_raw=True</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>
<span class="sd">                from pydantic import BaseModel</span>

<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    answer: str</span>
<span class="sd">                    justification: str</span>

<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(</span>
<span class="sd">                    AnswerWithJustification,</span>
<span class="sd">                    method="json_mode",</span>
<span class="sd">                    include_raw=True</span>
<span class="sd">                )</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "Answer the following question. "</span>
<span class="sd">                    "Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n"</span>
<span class="sd">                    "What's heavier a pound of bricks or a pound of feathers?"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'raw': AIMessage(content='{\\n    "answer": "They are both the same weight.",\\n    "justification": "Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight." \\n}'),</span>
<span class="sd">                #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),</span>
<span class="sd">                #     'parsing_error': None</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=None, method="json_mode", include_raw=True</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                structured_llm = llm.with_structured_output(method="json_mode", include_raw=True)</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "Answer the following question. "</span>
<span class="sd">                    "Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n"</span>
<span class="sd">                    "What's heavier a pound of bricks or a pound of feathers?"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'raw': AIMessage(content='{\\n    "answer": "They are both the same weight.",\\n    "justification": "Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight." \\n}'),</span>
<span class="sd">                #     'parsed': {</span>
<span class="sd">                #         'answer': 'They are both the same weight.',</span>
<span class="sd">                #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'</span>
<span class="sd">                #     },</span>
<span class="sd">                #     'parsing_error': None</span>
<span class="sd">                # }</span>
<span class="sd">        """</span>  <span class="c1"># noqa: E501</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span>
            <span class="n">schema</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">include_raw</span><span class="o">=</span><span class="n">include_raw</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span></div>
</div>



<span class="k">def</span><span class="w"> </span><span class="nf">_is_pydantic_class</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_basemodel_subclass</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_lc_tool_call_to_openai_tool_call</span><span class="p">(</span><span class="n">tool_call</span><span class="p">:</span> <span class="n">ToolCall</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">],</span>
        <span class="s2">"function"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span>
            <span class="s2">"arguments"</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"args"</span><span class="p">]),</span>
        <span class="p">},</span>
    <span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_lc_invalid_tool_call_to_openai_tool_call</span><span class="p">(</span>
    <span class="n">invalid_tool_call</span><span class="p">:</span> <span class="n">InvalidToolCall</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="n">invalid_tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">],</span>
        <span class="s2">"function"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span> <span class="n">invalid_tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span>
            <span class="s2">"arguments"</span><span class="p">:</span> <span class="n">invalid_tool_call</span><span class="p">[</span><span class="s2">"args"</span><span class="p">],</span>
        <span class="p">},</span>
    <span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_url_to_size</span><span class="p">(</span><span class="n">image_source</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>  <span class="c1"># type: ignore[import]</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">"Unable to count image tokens. To count image tokens please install "</span>
            <span class="s2">"`pip install -U pillow httpx`."</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">_is_url</span><span class="p">(</span><span class="n">image_source</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">httpx</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">"Unable to count image tokens. To count image tokens please install "</span>
                <span class="s2">"`pip install -U httpx`."</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">httpx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">image_source</span><span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">))</span><span class="o">.</span><span class="n">size</span>
        <span class="k">return</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span>
    <span class="k">elif</span> <span class="n">_is_b64</span><span class="p">(</span><span class="n">image_source</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">encoded</span> <span class="o">=</span> <span class="n">image_source</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">size</span>
        <span class="k">return</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_count_image_tokens</span><span class="p">(</span><span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="c1"># Reference: https://platform.openai.com/docs/guides/vision/calculating-costs</span>
    <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">_resize</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">height</span> <span class="o">/</span> <span class="mi">512</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">width</span> <span class="o">/</span> <span class="mi">512</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">170</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="mi">85</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_url</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">([</span><span class="n">result</span><span class="o">.</span><span class="n">scheme</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">netloc</span><span class="p">])</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Unable to parse URL: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_b64</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"data:image"</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_resize</span><span class="p">(</span><span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="c1"># larger side must be &lt;= 2048</span>
    <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="mi">2048</span> <span class="ow">or</span> <span class="n">height</span> <span class="o">&gt;</span> <span class="mi">2048</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="n">height</span><span class="p">:</span>
            <span class="n">height</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">*</span> <span class="mi">2048</span><span class="p">)</span> <span class="o">//</span> <span class="n">width</span>
            <span class="n">width</span> <span class="o">=</span> <span class="mi">2048</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">width</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="mi">2048</span><span class="p">)</span> <span class="o">//</span> <span class="n">height</span>
            <span class="n">height</span> <span class="o">=</span> <span class="mi">2048</span>
    <span class="c1"># smaller side must be &lt;= 768</span>
    <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="mi">768</span> <span class="ow">and</span> <span class="n">height</span> <span class="o">&gt;</span> <span class="mi">768</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="n">height</span><span class="p">:</span>
            <span class="n">width</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="mi">768</span><span class="p">)</span> <span class="o">//</span> <span class="n">height</span>
            <span class="n">height</span> <span class="o">=</span> <span class="mi">768</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">height</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="mi">768</span><span class="p">)</span> <span class="o">//</span> <span class="n">height</span>
            <span class="n">width</span> <span class="o">=</span> <span class="mi">768</span>
    <span class="k">return</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_convert_to_openai_response_format</span><span class="p">(</span>
    <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="nb">type</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">strict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">TypeBaseModel</span><span class="p">]:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_basemodel_subclass</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">schema</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
        <span class="ow">and</span> <span class="s2">"json_schema"</span> <span class="ow">in</span> <span class="n">schema</span>
        <span class="ow">and</span> <span class="n">schema</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"json_schema"</span>
    <span class="p">):</span>
        <span class="n">response_format</span> <span class="o">=</span> <span class="n">schema</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">"name"</span> <span class="ow">in</span> <span class="n">schema</span> <span class="ow">and</span> <span class="s2">"schema"</span> <span class="ow">in</span> <span class="n">schema</span><span class="p">:</span>
        <span class="n">response_format</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_schema"</span><span class="p">,</span> <span class="s2">"json_schema"</span><span class="p">:</span> <span class="n">schema</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">strict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"strict"</span><span class="p">),</span> <span class="nb">bool</span><span class="p">):</span>
                <span class="n">strict</span> <span class="o">=</span> <span class="n">schema</span><span class="p">[</span><span class="s2">"strict"</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">strict</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">function</span> <span class="o">=</span> <span class="n">convert_to_openai_function</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>
        <span class="n">function</span><span class="p">[</span><span class="s2">"schema"</span><span class="p">]</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"parameters"</span><span class="p">)</span>
        <span class="n">response_format</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_schema"</span><span class="p">,</span> <span class="s2">"json_schema"</span><span class="p">:</span> <span class="n">function</span><span class="p">}</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">strict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">strict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">response_format</span><span class="p">[</span><span class="s2">"json_schema"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"strict"</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">"Output schema already has 'strict' value set to "</span>
            <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">schema</span><span class="p">[</span><span class="s1">'json_schema'</span><span class="p">][</span><span class="s1">'strict'</span><span class="p">]</span><span class="si">}</span><span class="s2"> but 'strict' also passed in to "</span>
            <span class="sa">f</span><span class="s2">"with_structured_output as </span><span class="si">{</span><span class="n">strict</span><span class="si">}</span><span class="s2">. Please make sure that "</span>
            <span class="sa">f</span><span class="s2">"'strict' is only specified in one place."</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response_format</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_oai_structured_outputs_parser</span><span class="p">(</span>
    <span class="n">ai_msg</span><span class="p">:</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">_BM</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PydanticBaseModel</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">parsed</span> <span class="o">:=</span> <span class="n">ai_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"parsed"</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parsed</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">schema</span><span class="p">(</span><span class="o">**</span><span class="n">parsed</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">parsed</span>
    <span class="k">elif</span> <span class="n">ai_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"refusal"</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">OpenAIRefusalError</span><span class="p">(</span><span class="n">ai_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"refusal"</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">ai_msg</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Structured Output response does not have a 'parsed' field nor a 'refusal' "</span>
            <span class="sa">f</span><span class="s2">"field. Received message:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">ai_msg</span><span class="si">}</span><span class="s2">"</span>
        <span class="p">)</span>


<div class="viewcode-block" id="OpenAIRefusalError">
<a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.OpenAIRefusalError.html#langchain_cerebras.chat_models.OpenAIRefusalError">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">OpenAIRefusalError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Error raised when OpenAI Structured Outputs API returns a refusal.</span>

<span class="sd">    When using OpenAI's Structured Outputs API with user-generated input, the model</span>
<span class="sd">    may occasionally refuse to fulfill the request for safety reasons.</span>

<span class="sd">    See here for more on refusals:</span>
<span class="sd">    https://platform.openai.com/docs/guides/structured-outputs/refusals</span>

<span class="sd">    .. versionadded:: 0.1.21</span>
<span class="sd">    """</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_create_usage_metadata</span><span class="p">(</span><span class="n">oai_token_usage</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">UsageMetadata</span><span class="p">:</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"prompt_tokens"</span><span class="p">)</span> <span class="ow">or</span> <span class="mi">0</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"completion_tokens"</span><span class="p">)</span> <span class="ow">or</span> <span class="mi">0</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"total_tokens"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">input_tokens</span> <span class="o">+</span> <span class="n">output_tokens</span>
    <span class="n">input_token_details</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"audio"</span><span class="p">:</span> <span class="p">(</span><span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"prompt_tokens_details"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"audio_tokens"</span>
        <span class="p">),</span>
        <span class="s2">"cache_read"</span><span class="p">:</span> <span class="p">(</span><span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"prompt_tokens_details"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"cached_tokens"</span>
        <span class="p">),</span>
    <span class="p">}</span>
    <span class="n">output_token_details</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"audio"</span><span class="p">:</span> <span class="p">(</span><span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"completion_tokens_details"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"audio_tokens"</span>
        <span class="p">),</span>
        <span class="s2">"reasoning"</span><span class="p">:</span> <span class="p">(</span><span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"completion_tokens_details"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"reasoning_tokens"</span>
        <span class="p">),</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">UsageMetadata</span><span class="p">(</span>
        <span class="n">input_tokens</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">,</span>
        <span class="n">output_tokens</span><span class="o">=</span><span class="n">output_tokens</span><span class="p">,</span>
        <span class="n">total_tokens</span><span class="o">=</span><span class="n">total_tokens</span><span class="p">,</span>
        <span class="n">input_token_details</span><span class="o">=</span><span class="n">InputTokenDetails</span><span class="p">(</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">input_token_details</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>
        <span class="p">),</span>
        <span class="n">output_token_details</span><span class="o">=</span><span class="n">OutputTokenDetails</span><span class="p">(</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">output_token_details</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>
        <span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_create_usage_metadata_responses</span><span class="p">(</span><span class="n">oai_token_usage</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">UsageMetadata</span><span class="p">:</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"input_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"output_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"total_tokens"</span><span class="p">,</span> <span class="n">input_tokens</span> <span class="o">+</span> <span class="n">output_tokens</span><span class="p">)</span>
    <span class="n">output_token_details</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"reasoning"</span><span class="p">:</span> <span class="p">(</span><span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"output_tokens_details"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"reasoning_tokens"</span>
        <span class="p">)</span>
    <span class="p">}</span>
    <span class="n">input_token_details</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"cache_read"</span><span class="p">:</span> <span class="p">(</span><span class="n">oai_token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"input_tokens_details"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">"cached_tokens"</span>
        <span class="p">)</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">UsageMetadata</span><span class="p">(</span>
        <span class="n">input_tokens</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">,</span>
        <span class="n">output_tokens</span><span class="o">=</span><span class="n">output_tokens</span><span class="p">,</span>
        <span class="n">total_tokens</span><span class="o">=</span><span class="n">total_tokens</span><span class="p">,</span>
        <span class="n">input_token_details</span><span class="o">=</span><span class="n">InputTokenDetails</span><span class="p">(</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">input_token_details</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>
        <span class="p">),</span>
        <span class="n">output_token_details</span><span class="o">=</span><span class="n">OutputTokenDetails</span><span class="p">(</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">output_token_details</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>
        <span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_builtin_tool</span><span class="p">(</span><span class="n">tool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="s2">"type"</span> <span class="ow">in</span> <span class="n">tool</span> <span class="ow">and</span> <span class="n">tool</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">"function"</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_use_responses_api</span><span class="p">(</span><span class="n">payload</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">uses_builtin_tools</span> <span class="o">=</span> <span class="s2">"tools"</span> <span class="ow">in</span> <span class="n">payload</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">_is_builtin_tool</span><span class="p">(</span><span class="n">tool</span><span class="p">)</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">[</span><span class="s2">"tools"</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">responses_only_args</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"include"</span><span class="p">,</span>
        <span class="s2">"previous_response_id"</span><span class="p">,</span>
        <span class="s2">"reasoning"</span><span class="p">,</span>
        <span class="s2">"text"</span><span class="p">,</span>
        <span class="s2">"truncation"</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">uses_builtin_tools</span> <span class="ow">or</span> <span class="n">responses_only_args</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_last_messages</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Return</span>
<span class="sd">        1. Every message after the most-recent AIMessage that has a non-empty</span>
<span class="sd">           ``response_metadata["id"]`` (may be an empty list),</span>
<span class="sd">        2. That id.</span>

<span class="sd">    If the most-recent AIMessage does not have an id (or there is no</span>
<span class="sd">    AIMessage at all) the entire conversation is returned together with ``None``.</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">response_id</span> <span class="o">=</span> <span class="n">msg</span><span class="o">.</span><span class="n">response_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">response_id</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">messages</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:],</span> <span class="n">response_id</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">messages</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">messages</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_construct_responses_api_payload</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span> <span class="n">payload</span><span class="p">:</span> <span class="nb">dict</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="c1"># Rename legacy parameters</span>
    <span class="k">for</span> <span class="n">legacy_token_param</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"max_tokens"</span><span class="p">,</span> <span class="s2">"max_completion_tokens"</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">legacy_token_param</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"max_output_tokens"</span><span class="p">]</span> <span class="o">=</span> <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">legacy_token_param</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">"reasoning_effort"</span> <span class="ow">in</span> <span class="n">payload</span> <span class="ow">and</span> <span class="s2">"reasoning"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
        <span class="n">payload</span><span class="p">[</span><span class="s2">"reasoning"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"effort"</span><span class="p">:</span> <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"reasoning_effort"</span><span class="p">)}</span>

    <span class="n">payload</span><span class="p">[</span><span class="s2">"input"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_construct_responses_api_input</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tools</span> <span class="o">:=</span> <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">new_tools</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">:</span>
            <span class="c1"># chat api: {"type": "function", "function": {"name": "...", "description": "...", "parameters": {...}, "strict": ...}}  # noqa: E501</span>
            <span class="c1"># responses api: {"type": "function", "name": "...", "description": "...", "parameters": {...}, "strict": ...}  # noqa: E501</span>
            <span class="k">if</span> <span class="n">tool</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"function"</span> <span class="ow">and</span> <span class="s2">"function"</span> <span class="ow">in</span> <span class="n">tool</span><span class="p">:</span>
                <span class="n">new_tools</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span> <span class="o">**</span><span class="n">tool</span><span class="p">[</span><span class="s2">"function"</span><span class="p">]})</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">tool</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"image_generation"</span><span class="p">:</span>
                    <span class="c1"># Handle partial images (not yet supported)</span>
                    <span class="k">if</span> <span class="s2">"partial_images"</span> <span class="ow">in</span> <span class="n">tool</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                            <span class="s2">"Partial image generation is not yet supported "</span>
                            <span class="s2">"via the LangChain ChatOpenAI client. Please "</span>
                            <span class="s2">"drop the 'partial_images' key from the image_generation "</span>
                            <span class="s2">"tool."</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="n">payload</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">"partial_images"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tool</span><span class="p">:</span>
                        <span class="c1"># OpenAI requires this parameter be set; we ignore it during</span>
                        <span class="c1"># streaming.</span>
                        <span class="n">tool</span><span class="p">[</span><span class="s2">"partial_images"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>

                <span class="n">new_tools</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool</span><span class="p">)</span>

        <span class="n">payload</span><span class="p">[</span><span class="s2">"tools"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_tools</span>
    <span class="k">if</span> <span class="n">tool_choice</span> <span class="o">:=</span> <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"tool_choice"</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1"># chat api: {"type": "function", "function": {"name": "..."}}</span>
        <span class="c1"># responses api: {"type": "function", "name": "..."}</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">tool_choice</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"function"</span>
            <span class="ow">and</span> <span class="s2">"function"</span> <span class="ow">in</span> <span class="n">tool_choice</span>
        <span class="p">):</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"tool_choice"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span> <span class="o">**</span><span class="n">tool_choice</span><span class="p">[</span><span class="s2">"function"</span><span class="p">]}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"tool_choice"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tool_choice</span>

    <span class="c1"># Structured output</span>
    <span class="k">if</span> <span class="n">schema</span> <span class="o">:=</span> <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"response_format"</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">payload</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"text"</span><span class="p">):</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">payload</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Can specify at most one of 'response_format' or 'text', received both:"</span>
                <span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="si">{</span><span class="n">schema</span><span class="si">=}</span><span class="se">\n</span><span class="si">{</span><span class="n">text</span><span class="si">=}</span><span class="s2">"</span>
            <span class="p">)</span>

        <span class="c1"># For pydantic + non-streaming case, we use responses.parse.</span>
        <span class="c1"># Otherwise, we use responses.create.</span>
        <span class="n">strict</span> <span class="o">=</span> <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"strict"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">payload</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span> <span class="ow">and</span> <span class="n">_is_pydantic_class</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"text_format"</span><span class="p">]</span> <span class="o">=</span> <span class="n">schema</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_is_pydantic_class</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
                <span class="n">schema_dict</span> <span class="o">=</span> <span class="n">schema</span><span class="o">.</span><span class="n">model_json_schema</span><span class="p">()</span>
                <span class="n">strict</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">schema_dict</span> <span class="o">=</span> <span class="n">schema</span>
            <span class="k">if</span> <span class="n">schema_dict</span> <span class="o">==</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_object"</span><span class="p">}:</span>  <span class="c1"># JSON mode</span>
                <span class="n">payload</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"format"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_object"</span><span class="p">}}</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="p">(</span>
                    <span class="n">response_format</span> <span class="o">:=</span> <span class="n">_convert_to_openai_response_format</span><span class="p">(</span>
                        <span class="n">schema_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="ow">and</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">response_format</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span>
                <span class="ow">and</span> <span class="p">(</span><span class="n">response_format</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"json_schema"</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">payload</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">"format"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_schema"</span><span class="p">,</span> <span class="o">**</span><span class="n">response_format</span><span class="p">[</span><span class="s2">"json_schema"</span><span class="p">]}</span>
                <span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span>
    <span class="k">return</span> <span class="n">payload</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_make_computer_call_output_from_message</span><span class="p">(</span><span class="n">message</span><span class="p">:</span> <span class="n">ToolMessage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="n">computer_call_output</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"call_id"</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_call_id</span><span class="p">,</span>
        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"computer_call_output"</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="c1"># Use first input_image block</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
            <span class="n">block</span>
            <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span>
            <span class="k">if</span> <span class="n">cast</span><span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="n">block</span><span class="p">)[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"input_image"</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># string, assume image_url</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"input_image"</span><span class="p">,</span> <span class="s2">"image_url"</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">}</span>
    <span class="n">computer_call_output</span><span class="p">[</span><span class="s2">"output"</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">computer_call_output</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_pop_index_and_sub_index</span><span class="p">(</span><span class="n">block</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""When streaming, langchain-core uses the ``index`` key to aggregate</span>
<span class="sd">    text blocks. OpenAI API does not support this key, so we need to remove it.</span>
<span class="sd">    """</span>
    <span class="n">new_block</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">block</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">"index"</span><span class="p">}</span>
    <span class="k">if</span> <span class="s2">"summary"</span> <span class="ow">in</span> <span class="n">new_block</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_block</span><span class="p">[</span><span class="s2">"summary"</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">new_summary</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sub_block</span> <span class="ow">in</span> <span class="n">new_block</span><span class="p">[</span><span class="s2">"summary"</span><span class="p">]:</span>
            <span class="n">new_sub_block</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sub_block</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">"index"</span><span class="p">}</span>
            <span class="n">new_summary</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_sub_block</span><span class="p">)</span>
        <span class="n">new_block</span><span class="p">[</span><span class="s2">"summary"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_summary</span>
    <span class="k">return</span> <span class="n">new_block</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_construct_responses_api_input</span><span class="p">(</span><span class="n">messages</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Construct the input for the OpenAI Responses API."""</span>
    <span class="n">input_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">lc_msg</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lc_msg</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">lc_msg</span> <span class="o">=</span> <span class="n">_convert_from_v03_ai_message</span><span class="p">(</span><span class="n">lc_msg</span><span class="p">)</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">_convert_message_to_dict</span><span class="p">(</span><span class="n">lc_msg</span><span class="p">)</span>
        <span class="c1"># "name" parameter unsupported</span>
        <span class="k">if</span> <span class="s2">"name"</span> <span class="ow">in</span> <span class="n">msg</span><span class="p">:</span>
            <span class="n">msg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"name"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"tool"</span><span class="p">:</span>
            <span class="n">tool_output</span> <span class="o">=</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">lc_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"computer_call_output"</span><span class="p">:</span>
                <span class="n">computer_call_output</span> <span class="o">=</span> <span class="n">_make_computer_call_output_from_message</span><span class="p">(</span>
                    <span class="n">cast</span><span class="p">(</span><span class="n">ToolMessage</span><span class="p">,</span> <span class="n">lc_msg</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">computer_call_output</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_output</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="n">tool_output</span> <span class="o">=</span> <span class="n">_stringify</span><span class="p">(</span><span class="n">tool_output</span><span class="p">)</span>
                <span class="n">function_call_output</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function_call_output"</span><span class="p">,</span>
                    <span class="s2">"output"</span><span class="p">:</span> <span class="n">tool_output</span><span class="p">,</span>
                    <span class="s2">"call_id"</span><span class="p">:</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"tool_call_id"</span><span class="p">],</span>
                <span class="p">}</span>
                <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">function_call_output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"assistant"</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">),</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">block_type</span> <span class="o">:=</span> <span class="n">block</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)):</span>
                        <span class="c1"># Aggregate content blocks for a single message</span>
                        <span class="k">if</span> <span class="n">block_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"text"</span><span class="p">,</span> <span class="s2">"output_text"</span><span class="p">,</span> <span class="s2">"refusal"</span><span class="p">):</span>
                            <span class="n">msg_id</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">block_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"text"</span><span class="p">,</span> <span class="s2">"output_text"</span><span class="p">):</span>
                                <span class="n">new_block</span> <span class="o">=</span> <span class="p">{</span>
                                    <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"output_text"</span><span class="p">,</span>
                                    <span class="s2">"text"</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span>
                                    <span class="s2">"annotations"</span><span class="p">:</span> <span class="n">block</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"annotations"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">[],</span>
                                <span class="p">}</span>
                            <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">"refusal"</span><span class="p">:</span>
                                <span class="n">new_block</span> <span class="o">=</span> <span class="p">{</span>
                                    <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"refusal"</span><span class="p">,</span>
                                    <span class="s2">"refusal"</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="s2">"refusal"</span><span class="p">],</span>
                                <span class="p">}</span>
                            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">input_</span><span class="p">:</span>
                                <span class="k">if</span> <span class="p">(</span><span class="n">item_id</span> <span class="o">:=</span> <span class="n">item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">))</span> <span class="ow">and</span> <span class="n">item_id</span> <span class="o">==</span> <span class="n">msg_id</span><span class="p">:</span>
                                    <span class="c1"># If existing block with this ID, append to it</span>
                                    <span class="k">if</span> <span class="s2">"content"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">item</span><span class="p">:</span>
                                        <span class="n">item</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                                    <span class="n">item</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_block</span><span class="p">)</span>
                                    <span class="k">break</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="c1"># If no block with this ID, create a new one</span>
                                <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                                    <span class="p">{</span>
                                        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"message"</span><span class="p">,</span>
                                        <span class="s2">"content"</span><span class="p">:</span> <span class="p">[</span><span class="n">new_block</span><span class="p">],</span>
                                        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span>
                                        <span class="s2">"id"</span><span class="p">:</span> <span class="n">msg_id</span><span class="p">,</span>
                                    <span class="p">}</span>
                                <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">block_type</span> <span class="ow">in</span> <span class="p">(</span>
                            <span class="s2">"reasoning"</span><span class="p">,</span>
                            <span class="s2">"web_search_call"</span><span class="p">,</span>
                            <span class="s2">"file_search_call"</span><span class="p">,</span>
                            <span class="s2">"function_call"</span><span class="p">,</span>
                            <span class="s2">"computer_call"</span><span class="p">,</span>
                            <span class="s2">"code_interpreter_call"</span><span class="p">,</span>
                            <span class="s2">"mcp_call"</span><span class="p">,</span>
                            <span class="s2">"mcp_list_tools"</span><span class="p">,</span>
                            <span class="s2">"mcp_approval_request"</span><span class="p">,</span>
                        <span class="p">):</span>
                            <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_pop_index_and_sub_index</span><span class="p">(</span><span class="n">block</span><span class="p">))</span>
                        <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">"image_generation_call"</span><span class="p">:</span>
                            <span class="c1"># A previous image generation call can be referenced by ID</span>
                            <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                                <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"image_generation_call"</span><span class="p">,</span> <span class="s2">"id"</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]}</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">pass</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">),</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"message"</span><span class="p">,</span>
                        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span>
                        <span class="s2">"content"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"output_text"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]}],</span>
                    <span class="p">}</span>
                <span class="p">)</span>

            <span class="c1"># Add function calls from tool calls if not already present</span>
            <span class="k">if</span> <span class="n">tool_calls</span> <span class="o">:=</span> <span class="n">msg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"tool_calls"</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
                <span class="n">content_call_ids</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">block</span><span class="p">[</span><span class="s2">"call_id"</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">input_</span>
                    <span class="k">if</span> <span class="n">block</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"function_call"</span> <span class="ow">and</span> <span class="s2">"call_id"</span> <span class="ow">in</span> <span class="n">block</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">tool_calls</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">content_call_ids</span><span class="p">:</span>
                        <span class="n">function_call</span> <span class="o">=</span> <span class="p">{</span>
                            <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function_call"</span><span class="p">,</span>
                            <span class="s2">"name"</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">],</span>
                            <span class="s2">"arguments"</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"arguments"</span><span class="p">],</span>
                            <span class="s2">"call_id"</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">],</span>
                        <span class="p">}</span>
                        <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">function_call</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"user"</span><span class="p">,</span> <span class="s2">"system"</span><span class="p">,</span> <span class="s2">"developer"</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">[</span><span class="s2">"content"</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">new_blocks</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">non_message_item_types</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"mcp_approval_response"</span><span class="p">,)</span>
                <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]:</span>
                    <span class="c1"># chat api: {"type": "text", "text": "..."}</span>
                    <span class="c1"># responses api: {"type": "input_text", "text": "..."}</span>
                    <span class="k">if</span> <span class="n">block</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"text"</span><span class="p">:</span>
                        <span class="n">new_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"input_text"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]})</span>
                    <span class="c1"># chat api: {"type": "image_url", "image_url": {"url": "...", "detail": "..."}}  # noqa: E501</span>
                    <span class="c1"># responses api: {"type": "image_url", "image_url": "...", "detail": "...", "file_id": "..."}  # noqa: E501</span>
                    <span class="k">elif</span> <span class="n">block</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"image_url"</span><span class="p">:</span>
                        <span class="n">new_block</span> <span class="o">=</span> <span class="p">{</span>
                            <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"input_image"</span><span class="p">,</span>
                            <span class="s2">"image_url"</span><span class="p">:</span> <span class="n">block</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">][</span><span class="s2">"url"</span><span class="p">],</span>
                        <span class="p">}</span>
                        <span class="k">if</span> <span class="n">block</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"detail"</span><span class="p">):</span>
                            <span class="n">new_block</span><span class="p">[</span><span class="s2">"detail"</span><span class="p">]</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">][</span><span class="s2">"detail"</span><span class="p">]</span>
                        <span class="n">new_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_block</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">block</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"file"</span><span class="p">:</span>
                        <span class="n">new_block</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"input_file"</span><span class="p">,</span> <span class="o">**</span><span class="n">block</span><span class="p">[</span><span class="s2">"file"</span><span class="p">]}</span>
                        <span class="n">new_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_block</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">block</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"input_text"</span><span class="p">,</span> <span class="s2">"input_image"</span><span class="p">,</span> <span class="s2">"input_file"</span><span class="p">):</span>
                        <span class="n">new_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">block</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="ow">in</span> <span class="n">non_message_item_types</span><span class="p">:</span>
                        <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="n">msg</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_blocks</span>
                <span class="k">if</span> <span class="n">msg</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]:</span>
                    <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_construct_lc_result_from_responses_api</span><span class="p">(</span>
    <span class="n">response</span><span class="p">:</span> <span class="n">Response</span><span class="p">,</span>
    <span class="n">schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">type</span><span class="p">[</span><span class="n">_BM</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_version</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"v0"</span><span class="p">,</span> <span class="s2">"responses/v1"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"v0"</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Construct ChatResponse from OpenAI Response API response."""</span>
    <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">error</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">error</span><span class="p">)</span>

    <span class="n">response_metadata</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">v</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">exclude_none</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"json"</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">k</span>
        <span class="ow">in</span> <span class="p">(</span>
            <span class="s2">"created_at"</span><span class="p">,</span>
            <span class="c1"># backwards compatibility: keep response ID in response_metadata as well as</span>
            <span class="c1"># top-level-id</span>
            <span class="s2">"id"</span><span class="p">,</span>
            <span class="s2">"incomplete_details"</span><span class="p">,</span>
            <span class="s2">"metadata"</span><span class="p">,</span>
            <span class="s2">"object"</span><span class="p">,</span>
            <span class="s2">"status"</span><span class="p">,</span>
            <span class="s2">"user"</span><span class="p">,</span>
            <span class="s2">"model"</span><span class="p">,</span>
            <span class="s2">"service_tier"</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">metadata</span><span class="p">:</span>
        <span class="n">response_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>
    <span class="c1"># for compatibility with chat completion calls.</span>
    <span class="n">response_metadata</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">response_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">usage</span><span class="p">:</span>
        <span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">_create_usage_metadata_responses</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">usage</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">usage_metadata</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">content_blocks</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">invalid_tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">additional_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"message"</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">content</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">content</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">content</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"output_text"</span><span class="p">:</span>
                    <span class="n">block</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"text"</span><span class="p">,</span>
                        <span class="s2">"text"</span><span class="p">:</span> <span class="n">content</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
                        <span class="s2">"annotations"</span><span class="p">:</span> <span class="p">[</span>
                            <span class="n">annotation</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
                            <span class="k">for</span> <span class="n">annotation</span> <span class="ow">in</span> <span class="n">content</span><span class="o">.</span><span class="n">annotations</span>
                        <span class="p">],</span>
                        <span class="s2">"id"</span><span class="p">:</span> <span class="n">output</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
                    <span class="p">}</span>
                    <span class="n">content_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="s2">"parsed"</span><span class="p">):</span>
                        <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"parsed"</span><span class="p">]</span> <span class="o">=</span> <span class="n">content</span><span class="o">.</span><span class="n">parsed</span>
                <span class="k">if</span> <span class="n">content</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"refusal"</span><span class="p">:</span>
                    <span class="n">content_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"refusal"</span><span class="p">,</span> <span class="s2">"refusal"</span><span class="p">:</span> <span class="n">content</span><span class="o">.</span><span class="n">refusal</span><span class="p">,</span> <span class="s2">"id"</span><span class="p">:</span> <span class="n">output</span><span class="o">.</span><span class="n">id</span><span class="p">}</span>
                    <span class="p">)</span>
        <span class="k">elif</span> <span class="n">output</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"function_call"</span><span class="p">:</span>
            <span class="n">content_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">exclude_none</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"json"</span><span class="p">))</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">arguments</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">error</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">except</span> <span class="n">JSONDecodeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">arguments</span>
                <span class="n">error</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">error</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tool_call</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"tool_call"</span><span class="p">,</span>
                    <span class="s2">"name"</span><span class="p">:</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                    <span class="s2">"args"</span><span class="p">:</span> <span class="n">args</span><span class="p">,</span>
                    <span class="s2">"id"</span><span class="p">:</span> <span class="n">output</span><span class="o">.</span><span class="n">call_id</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="n">tool_calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool_call</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tool_call</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"invalid_tool_call"</span><span class="p">,</span>
                    <span class="s2">"name"</span><span class="p">:</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                    <span class="s2">"args"</span><span class="p">:</span> <span class="n">args</span><span class="p">,</span>
                    <span class="s2">"id"</span><span class="p">:</span> <span class="n">output</span><span class="o">.</span><span class="n">call_id</span><span class="p">,</span>
                    <span class="s2">"error"</span><span class="p">:</span> <span class="n">error</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="n">invalid_tool_calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool_call</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">output</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="s2">"reasoning"</span><span class="p">,</span>
            <span class="s2">"web_search_call"</span><span class="p">,</span>
            <span class="s2">"file_search_call"</span><span class="p">,</span>
            <span class="s2">"computer_call"</span><span class="p">,</span>
            <span class="s2">"code_interpreter_call"</span><span class="p">,</span>
            <span class="s2">"mcp_call"</span><span class="p">,</span>
            <span class="s2">"mcp_list_tools"</span><span class="p">,</span>
            <span class="s2">"mcp_approval_request"</span><span class="p">,</span>
            <span class="s2">"image_generation_call"</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">content_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">exclude_none</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"json"</span><span class="p">))</span>

    <span class="c1"># Workaround for parsing structured output in the streaming case.</span>
    <span class="c1">#    from openai import OpenAI</span>
    <span class="c1">#    from pydantic import BaseModel</span>

    <span class="c1">#    class Foo(BaseModel):</span>
    <span class="c1">#        response: str</span>

    <span class="c1">#    client = OpenAI()</span>

    <span class="c1">#    client.responses.parse(</span>
    <span class="c1">#        model="gpt-4o-mini",</span>
    <span class="c1">#        input=[{"content": "how are ya", "role": "user"}],</span>
    <span class="c1">#        text_format=Foo,</span>
    <span class="c1">#        stream=True,  # &lt;-- errors</span>
    <span class="c1">#    )</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">schema</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="s2">"parsed"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">additional_kwargs</span>
        <span class="ow">and</span> <span class="n">response</span><span class="o">.</span><span class="n">output_text</span>  <span class="c1"># tool calls can generate empty output text</span>
        <span class="ow">and</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">text_config</span> <span class="o">:=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">format_</span> <span class="o">:=</span> <span class="n">text_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"format"</span><span class="p">,</span> <span class="p">{}))</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">format_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"type"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"json_schema"</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">parsed_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">output_text</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">schema</span> <span class="ow">and</span> <span class="n">_is_pydantic_class</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
                <span class="n">parsed</span> <span class="o">=</span> <span class="n">schema</span><span class="p">(</span><span class="o">**</span><span class="n">parsed_dict</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">parsed</span> <span class="o">=</span> <span class="n">parsed_dict</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"parsed"</span><span class="p">]</span> <span class="o">=</span> <span class="n">parsed</span>
        <span class="k">except</span> <span class="n">json</span><span class="o">.</span><span class="n">JSONDecodeError</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="n">message</span> <span class="o">=</span> <span class="n">AIMessage</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="n">content_blocks</span><span class="p">,</span>
        <span class="nb">id</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
        <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage_metadata</span><span class="p">,</span>
        <span class="n">response_metadata</span><span class="o">=</span><span class="n">response_metadata</span><span class="p">,</span>
        <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
        <span class="n">tool_calls</span><span class="o">=</span><span class="n">tool_calls</span><span class="p">,</span>
        <span class="n">invalid_tool_calls</span><span class="o">=</span><span class="n">invalid_tool_calls</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">output_version</span> <span class="o">==</span> <span class="s2">"v0"</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="n">_convert_to_v03_ai_message</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="n">ChatResult</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="p">[</span><span class="n">ChatGeneration</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">)])</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_convert_responses_chunk_to_generation_chunk</span><span class="p">(</span>
    <span class="n">chunk</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">current_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>  <span class="c1"># index in content</span>
    <span class="n">current_output_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>  <span class="c1"># index in Response output</span>
    <span class="n">current_sub_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>  <span class="c1"># index of content block in output item</span>
    <span class="n">schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">type</span><span class="p">[</span><span class="n">_BM</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">has_reasoning</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">output_version</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"v0"</span><span class="p">,</span> <span class="s2">"responses/v1"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"v0"</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]]:</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_advance</span><span class="p">(</span><span class="n">output_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">sub_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Advance indexes tracked during streaming.</span>

<span class="sd">        Example: we stream a response item of the form:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {</span>
<span class="sd">                "type": "message",  # output_index 0</span>
<span class="sd">                "role": "assistant",</span>
<span class="sd">                "id": "msg_123",</span>
<span class="sd">                "content": [</span>
<span class="sd">                    {"type": "output_text", "text": "foo"},  # sub_index 0</span>
<span class="sd">                    {"type": "output_text", "text": "bar"},  # sub_index 1</span>
<span class="sd">                ],</span>
<span class="sd">            }</span>

<span class="sd">        This is a single item with a shared ``output_index`` and two sub-indexes, one</span>
<span class="sd">        for each content block.</span>

<span class="sd">        This will be processed into an AIMessage with two text blocks:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessage(</span>
<span class="sd">                [</span>
<span class="sd">                    {"type": "text", "text": "foo", "id": "msg_123"},  # index 0</span>
<span class="sd">                    {"type": "text", "text": "bar", "id": "msg_123"},  # index 1</span>
<span class="sd">                ]</span>
<span class="sd">            )</span>

<span class="sd">        This function just identifies updates in output or sub-indexes and increments</span>
<span class="sd">        the current index accordingly.</span>
<span class="sd">        """</span>
        <span class="k">nonlocal</span> <span class="n">current_index</span><span class="p">,</span> <span class="n">current_output_index</span><span class="p">,</span> <span class="n">current_sub_index</span>
        <span class="k">if</span> <span class="n">sub_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">current_output_index</span> <span class="o">!=</span> <span class="n">output_idx</span><span class="p">:</span>
                <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">current_output_index</span> <span class="o">!=</span> <span class="n">output_idx</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">current_sub_index</span> <span class="o">!=</span> <span class="n">sub_idx</span><span class="p">):</span>
                <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">current_sub_index</span> <span class="o">=</span> <span class="n">sub_idx</span>
        <span class="n">current_output_index</span> <span class="o">=</span> <span class="n">output_idx</span>

    <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tool_call_chunks</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">additional_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">metadata</span><span class="p">:</span>
        <span class="n">response_metadata</span> <span class="o">=</span> <span class="n">metadata</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">response_metadata</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">usage_metadata</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.output_text.delta"</span><span class="p">:</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">,</span> <span class="n">chunk</span><span class="o">.</span><span class="n">content_index</span><span class="p">)</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"text"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">delta</span><span class="p">,</span> <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">})</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.output_text.annotation.added"</span><span class="p">:</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">,</span> <span class="n">chunk</span><span class="o">.</span><span class="n">content_index</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">annotation</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="c1"># Appears to be a breaking change in openai==1.82.0</span>
            <span class="n">annotation</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">annotation</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">annotation</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">annotation</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">exclude_none</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"json"</span><span class="p">)</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"annotations"</span><span class="p">:</span> <span class="p">[</span><span class="n">annotation</span><span class="p">],</span> <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">})</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.output_text.done"</span><span class="p">:</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"id"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item_id</span><span class="p">,</span> <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">})</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.created"</span><span class="p">:</span>
        <span class="nb">id</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">id</span>
        <span class="n">response_metadata</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">id</span>  <span class="c1"># Backwards compatibility</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.completed"</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
            <span class="n">AIMessage</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">_construct_lc_result_from_responses_api</span><span class="p">(</span>
                    <span class="n">chunk</span><span class="o">.</span><span class="n">response</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">output_version</span><span class="o">=</span><span class="n">output_version</span>
                <span class="p">)</span>
                <span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="o">.</span><span class="n">message</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">parsed</span> <span class="o">:=</span> <span class="n">msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"parsed"</span><span class="p">):</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"parsed"</span><span class="p">]</span> <span class="o">=</span> <span class="n">parsed</span>
        <span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">msg</span><span class="o">.</span><span class="n">usage_metadata</span>
        <span class="n">response_metadata</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">msg</span><span class="o">.</span><span class="n">response_metadata</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">"id"</span>
        <span class="p">}</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.output_item.added"</span> <span class="ow">and</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"message"</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">output_version</span> <span class="o">==</span> <span class="s2">"v0"</span><span class="p">:</span>
            <span class="nb">id</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">id</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="k">elif</span> <span class="p">(</span>
        <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.output_item.added"</span>
        <span class="ow">and</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"function_call"</span>
    <span class="p">):</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">)</span>
        <span class="n">tool_call_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"tool_call_chunk"</span><span class="p">,</span>
                <span class="s2">"name"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="s2">"args"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">arguments</span><span class="p">,</span>
                <span class="s2">"id"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">call_id</span><span class="p">,</span>
                <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function_call"</span><span class="p">,</span>
                <span class="s2">"name"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="s2">"arguments"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">arguments</span><span class="p">,</span>
                <span class="s2">"call_id"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">call_id</span><span class="p">,</span>
                <span class="s2">"id"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
                <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.output_item.done"</span> <span class="ow">and</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">(</span>
        <span class="s2">"web_search_call"</span><span class="p">,</span>
        <span class="s2">"file_search_call"</span><span class="p">,</span>
        <span class="s2">"computer_call"</span><span class="p">,</span>
        <span class="s2">"code_interpreter_call"</span><span class="p">,</span>
        <span class="s2">"mcp_call"</span><span class="p">,</span>
        <span class="s2">"mcp_list_tools"</span><span class="p">,</span>
        <span class="s2">"mcp_approval_request"</span><span class="p">,</span>
        <span class="s2">"image_generation_call"</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">)</span>
        <span class="n">tool_output</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">exclude_none</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"json"</span><span class="p">)</span>
        <span class="n">tool_output</span><span class="p">[</span><span class="s2">"index"</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_index</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool_output</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.function_call_arguments.delta"</span><span class="p">:</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">)</span>
        <span class="n">tool_call_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"tool_call_chunk"</span><span class="p">,</span> <span class="s2">"args"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">delta</span><span class="p">,</span> <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function_call"</span><span class="p">,</span> <span class="s2">"arguments"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">delta</span><span class="p">,</span> <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">}</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.refusal.done"</span><span class="p">:</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"refusal"</span><span class="p">,</span> <span class="s2">"refusal"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">refusal</span><span class="p">})</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.output_item.added"</span> <span class="ow">and</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"reasoning"</span><span class="p">:</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">)</span>
        <span class="n">reasoning</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">item</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">exclude_none</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"json"</span><span class="p">)</span>
        <span class="n">reasoning</span><span class="p">[</span><span class="s2">"index"</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_index</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reasoning</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.reasoning_summary_part.added"</span><span class="p">:</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">)</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="c1"># langchain-core uses the `index` key to aggregate text blocks.</span>
                <span class="s2">"summary"</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">"index"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">summary_index</span><span class="p">,</span> <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"summary_text"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="s2">""</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">,</span>
                <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"reasoning"</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.image_generation_call.partial_image"</span><span class="p">:</span>
        <span class="c1"># Partial images are not supported yet.</span>
        <span class="k">pass</span>
    <span class="k">elif</span> <span class="n">chunk</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"response.reasoning_summary_text.delta"</span><span class="p">:</span>
        <span class="n">_advance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">output_index</span><span class="p">)</span>
        <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">"summary"</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">"index"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">summary_index</span><span class="p">,</span>
                        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"summary_text"</span><span class="p">,</span>
                        <span class="s2">"text"</span><span class="p">:</span> <span class="n">chunk</span><span class="o">.</span><span class="n">delta</span><span class="p">,</span>
                    <span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">"index"</span><span class="p">:</span> <span class="n">current_index</span><span class="p">,</span>
                <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"reasoning"</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">current_index</span><span class="p">,</span> <span class="n">current_output_index</span><span class="p">,</span> <span class="n">current_sub_index</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">message</span> <span class="o">=</span> <span class="n">AIMessageChunk</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">tool_call_chunks</span><span class="o">=</span><span class="n">tool_call_chunks</span><span class="p">,</span>
        <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage_metadata</span><span class="p">,</span>
        <span class="n">response_metadata</span><span class="o">=</span><span class="n">response_metadata</span><span class="p">,</span>
        <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
        <span class="nb">id</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">output_version</span> <span class="o">==</span> <span class="s2">"v0"</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
            <span class="n">AIMessageChunk</span><span class="p">,</span>
            <span class="n">_convert_to_v03_ai_message</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">has_reasoning</span><span class="o">=</span><span class="n">has_reasoning</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">current_index</span><span class="p">,</span>
        <span class="n">current_output_index</span><span class="p">,</span>
        <span class="n">current_sub_index</span><span class="p">,</span>
        <span class="n">ChatGenerationChunk</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">),</span>
    <span class="p">)</span>
</pre></div>
</article>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script defer="" src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer="" src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      Â© Copyright 2025, LangChain Inc.
      <br/>
</p>
</div>
</div>
</div>
</footer>
</body>
</html>