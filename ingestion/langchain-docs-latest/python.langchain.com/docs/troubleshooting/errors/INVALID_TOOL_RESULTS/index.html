<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-troubleshooting/errors/INVALID_TOOL_RESULTS" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">INVALID_TOOL_RESULTS | ü¶úÔ∏èüîó LangChain</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://python.langchain.com/img/brand/theme-image.png"><meta data-rh="true" name="twitter:image" content="https://python.langchain.com/img/brand/theme-image.png"><meta data-rh="true" property="og:url" content="https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="INVALID_TOOL_RESULTS | ü¶úÔ∏èüîó LangChain"><meta data-rh="true" name="description" content="You are passing too many, too few, or mismatched ToolMessages to a model."><meta data-rh="true" property="og:description" content="You are passing too many, too few, or mismatched ToolMessages to a model."><link data-rh="true" rel="icon" href="/img/brand/favicon.png"><link data-rh="true" rel="canonical" href="https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/"><link data-rh="true" rel="alternate" href="https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/" hreflang="en"><link data-rh="true" rel="alternate" href="https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://VAU016LAWS-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="ü¶úÔ∏èüîó LangChain" href="/opensearch.xml">


<script src="/js/google_analytics.js"></script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F" async></script><link rel="stylesheet" href="/assets/css/styles.12df7c44.css">
<script src="/assets/js/runtime~main.9df51359.js" defer="defer"></script>
<script src="/assets/js/main.232f3f22.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#d0c9fe" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY"><strong>Our <a href="https://academy.langchain.com/courses/ambient-agents/?utm_medium=internal&utm_source=docs&utm_campaign=q2-2025_ambient-agents_co" target="_blank">Building Ambient Agents with LangGraph</a> course is now available on LangChain Academy!</strong></div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/brand/wordmark.png" alt="ü¶úÔ∏èüîó LangChain" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/brand/wordmark-dark.png" alt="ü¶úÔ∏èüîó LangChain" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/docs/integrations/providers/">Integrations</a><a href="https://python.langchain.com/api_reference/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API Reference</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">More</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/contributing/">Contributing</a></li><li><a class="dropdown__link" href="/docs/people/">People</a></li><li><a class="dropdown__link" href="/docs/troubleshooting/errors/">Error reference</a></li><li><hr class="dropdown-separator" style="margin-top: 0.5rem; margin-bottom: 0.5rem"></li><li><a href="https://docs.smith.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangSmith<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://langchain-ai.github.io/langgraph/" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangGraph<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://smith.langchain.com/hub" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangChain Hub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://js.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangChain JS/TS<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">v0.3</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/introduction/">v0.3</a></li><li><a href="https://python.langchain.com/v0.2/docs/introduction" target="_blank" rel="noopener noreferrer" class="dropdown__link">v0.2<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://python.langchain.com/v0.1/docs/get_started/introduction" target="_blank" rel="noopener noreferrer" class="dropdown__link">v0.1<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a href="https://chat.langchain.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">üí¨</a><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_z5aJ"><div class="docItemContainer_c0TR"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div style="display:flex;flex-direction:column;align-items:flex-end;float:right;margin-left:12px"><a target="_blank" href="https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a><a href="https://github.com/langchain-ai/langchain/blob/master/docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb" target="_blank"><img src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&amp;logoColor=white" alt="Open on GitHub"></a></div><div class="theme-doc-markdown markdown"><header><h1>INVALID_TOOL_RESULTS</h1></header>
<p>You are passing too many, too few, or mismatched <a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html#toolmessage" target="_blank" rel="noopener noreferrer"><code>ToolMessages</code></a> to a model.</p>
<p>When <a href="/docs/concepts/tool_calling/">using a model to call tools</a>, the <a href="https://api.js.langchain.com/classes/_langchain_core.messages.AIMessage.html" target="_blank" rel="noopener noreferrer"><code>AIMessage</code></a>
the model responds with will contain a <code>tool_calls</code> array. To continue the flow, the next messages you pass back to the model must
be exactly one <code>ToolMessage</code> for each item in that array containing the result of that tool call. Each <code>ToolMessage</code> must have a <code>tool_call_id</code> field
that matches one of the <code>tool_calls</code> on the <code>AIMessage</code>.</p>
<p>For example, given the following response from a model:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> typing </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> List</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">messages </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> BaseMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> HumanMessage</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tools </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> tool</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_openai </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> ChatOpenAI</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> ChatOpenAI</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">model</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;gpt-4o-mini&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token decorator annotation punctuation" style="color:rgb(4, 81, 165)">@tool</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(0, 0, 255)">foo_tool</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"> </span><span class="token operator" style="color:rgb(0, 0, 0)">-</span><span class="token operator" style="color:rgb(0, 0, 0)">&gt;</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(0, 112, 193)">str</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">    A dummy tool that returns &#x27;action complete!&#x27;</span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token keyword" style="color:rgb(0, 0, 255)">return</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;action complete!&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model_with_tools </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">bind_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">foo_tool</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> List</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">BaseMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    HumanMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&#x27;Call tool &quot;foo&quot; twice with no arguments&#x27;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">response_message </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> model_with_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">print</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">response_message</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tool_calls</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html">BaseMessage</a> | </span><span><a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html">HumanMessage</a> | </span><span><a href="https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html">tool</a> | </span><span><a href="https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html">ChatOpenAI</a></span></div>
<div class="language-output codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-output codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">[{&#x27;name&#x27;: &#x27;foo_tool&#x27;, &#x27;args&#x27;: {}, &#x27;id&#x27;: &#x27;call_dq9O0eGHrryBwDRCnk0deHK4&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;}, {&#x27;name&#x27;: &#x27;foo_tool&#x27;, &#x27;args&#x27;: {}, &#x27;id&#x27;: &#x27;call_mjLuNyXNHoUIXHiBtXhaWdxN&#x27;, &#x27;type&#x27;: &#x27;tool_call&#x27;}]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Calling the model with only one tool response would result in an error:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain_core</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">messages </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> AIMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> ToolMessage</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tool_call </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> response_message</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tool_calls</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">tool_response </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> foo_tool</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">append</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    AIMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">        content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">response_message</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">content</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">        additional_kwargs</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">response_message</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">additional_kwargs</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">append</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ToolMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token builtin" style="color:rgb(0, 112, 193)">str</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">tool_response</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> tool_call_id</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">tool_call</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">get</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;id&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">final_response </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> model_with_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">print</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">final_response</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><b style="padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem">API Reference:</b><span><a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html">AIMessage</a> | </span><span><a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html">ToolMessage</a></span></div>
<div class="language-output codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-output codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">---------------------------------------------------------------------------</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">BadRequestError                           Traceback (most recent call last)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">Cell In[3], line 9</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">      6 chat_history.append(AIMessage(content=response_message.content, additional_kwargs=response_message.additional_kwargs))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">      7 chat_history.append(ToolMessage(content=str(tool_response), tool_call_id=tool_call.get(&#x27;id&#x27;)))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">----&gt; 9 final_response = model_with_tools.invoke(chat_history)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">     10 print(final_response)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/runnables/base.py:5354, in RunnableBindingBase.invoke(self, input, config, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5348 def invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5349     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5350     input: Input,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5351     config: Optional[RunnableConfig] = None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5352     **kwargs: Optional[Any],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5353 ) -&gt; Output:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 5354     return self.bound.invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5355         input,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5356         self._merge_configs(config),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5357         **{**self.kwargs, **kwargs},</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5358     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:286, in BaseChatModel.invoke(self, input, config, stop, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    275 def invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    276     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    277     input: LanguageModelInput,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    281     **kwargs: Any,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    282 ) -&gt; BaseMessage:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    283     config = ensure_config(config)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    284     return cast(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    285         ChatGeneration,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 286         self.generate_prompt(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    287             [self._convert_input(input)],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    288             stop=stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    289             callbacks=config.get(&quot;callbacks&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    290             tags=config.get(&quot;tags&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    291             metadata=config.get(&quot;metadata&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    292             run_name=config.get(&quot;run_name&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    293             run_id=config.pop(&quot;run_id&quot;, None),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    294             **kwargs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    295         ).generations[0][0],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    296     ).message</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:786, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    778 def generate_prompt(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    779     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    780     prompts: list[PromptValue],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    783     **kwargs: Any,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    784 ) -&gt; LLMResult:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    785     prompt_messages = [p.to_messages() for p in prompts]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 786     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:643, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    641         if run_managers:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    642             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 643         raise e</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    644 flattened_outputs = [</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    645     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    646     for res in results</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    647 ]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    648 llm_output = self._combine_llm_outputs([res.llm_output for res in results])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:633, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    630 for i, m in enumerate(messages):</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    631     try:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    632         results.append(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 633             self._generate_with_cache(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    634                 m,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    635                 stop=stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    636                 run_manager=run_managers[i] if run_managers else None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    637                 **kwargs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    638             )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    639         )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    640     except BaseException as e:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    641         if run_managers:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:851, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    849 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    850     if inspect.signature(self._generate).parameters.get(&quot;run_manager&quot;):</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 851         result = self._generate(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    852             messages, stop=stop, run_manager=run_manager, **kwargs</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    853         )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    854     else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    855         result = self._generate(messages, stop=stop, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:686, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    684     generation_info = {&quot;headers&quot;: dict(raw_response.headers)}</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    685 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 686     response = self.client.create(**payload)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    687 return self._create_chat_result(response, generation_info)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:274, in required_args.&lt;locals&gt;.inner.&lt;locals&gt;.wrapper(*args, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    272             msg = f&quot;Missing required argument: {quote(missing[0])}&quot;</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    273     raise TypeError(msg)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 274 return func(*args, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:742, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    704 @required_args([&quot;messages&quot;, &quot;model&quot;], [&quot;messages&quot;, &quot;model&quot;, &quot;stream&quot;])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    705 def create(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    706     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    739     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    740 ) -&gt; ChatCompletion | Stream[ChatCompletionChunk]:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    741     validate_response_format(response_format)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 742     return self._post(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    743         &quot;/chat/completions&quot;,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    744         body=maybe_transform(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    745             {</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    746                 &quot;messages&quot;: messages,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    747                 &quot;model&quot;: model,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    748                 &quot;frequency_penalty&quot;: frequency_penalty,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    749                 &quot;function_call&quot;: function_call,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    750                 &quot;functions&quot;: functions,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    751                 &quot;logit_bias&quot;: logit_bias,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    752                 &quot;logprobs&quot;: logprobs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    753                 &quot;max_completion_tokens&quot;: max_completion_tokens,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    754                 &quot;max_tokens&quot;: max_tokens,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    755                 &quot;metadata&quot;: metadata,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    756                 &quot;n&quot;: n,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    757                 &quot;parallel_tool_calls&quot;: parallel_tool_calls,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    758                 &quot;presence_penalty&quot;: presence_penalty,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    759                 &quot;response_format&quot;: response_format,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    760                 &quot;seed&quot;: seed,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    761                 &quot;service_tier&quot;: service_tier,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    762                 &quot;stop&quot;: stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    763                 &quot;store&quot;: store,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    764                 &quot;stream&quot;: stream,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    765                 &quot;stream_options&quot;: stream_options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    766                 &quot;temperature&quot;: temperature,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    767                 &quot;tool_choice&quot;: tool_choice,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    768                 &quot;tools&quot;: tools,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    769                 &quot;top_logprobs&quot;: top_logprobs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    770                 &quot;top_p&quot;: top_p,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    771                 &quot;user&quot;: user,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    772             },</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    773             completion_create_params.CompletionCreateParams,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    774         ),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    775         options=make_request_options(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    776             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    777         ),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    778         cast_to=ChatCompletion,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    779         stream=stream or False,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    780         stream_cls=Stream[ChatCompletionChunk],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    781     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1270, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1256 def post(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1257     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1258     path: str,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1265     stream_cls: type[_StreamT] | None = None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1266 ) -&gt; ResponseT | _StreamT:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1267     opts = FinalRequestOptions.construct(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1268         method=&quot;post&quot;, url=path, json_data=body, files=to_httpx_files(files), **options</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1269     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    944 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    945     retries_taken = 0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 947 return self._request(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    948     cast_to=cast_to,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    949     options=options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    950     stream=stream,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    951     stream_cls=stream_cls,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    952     retries_taken=retries_taken,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    953 )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1048         err.response.read()</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1050     log.debug(&quot;Re-raising status error&quot;)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 1051     raise self._make_status_error_from_response(err.response) from None</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1053 return self._process_response(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1054     cast_to=cast_to,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1055     options=options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1059     retries_taken=retries_taken,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1060 )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">BadRequestError: Error code: 400 - {&#x27;error&#x27;: {&#x27;message&#x27;: &quot;An assistant message with &#x27;tool_calls&#x27; must be followed by tool messages responding to each &#x27;tool_call_id&#x27;. The following tool_call_ids did not have response messages: call_mjLuNyXNHoUIXHiBtXhaWdxN&quot;, &#x27;type&#x27;: &#x27;invalid_request_error&#x27;, &#x27;param&#x27;: &#x27;messages&#x27;, &#x27;code&#x27;: None}}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If we add a second response, the call will succeed as expected because we now have one tool response per tool call:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">tool_response_2 </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> foo_tool</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">response_message</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tool_calls</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">1</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">append</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">tool_response_2</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">model_with_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-output codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-output codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">AIMessage(content=&#x27;Both calls to the tool &quot;foo&quot; have been completed successfully. The output for each call is &quot;action complete!&quot;.&#x27;, additional_kwargs={&#x27;refusal&#x27;: None}, response_metadata={&#x27;token_usage&#x27;: {&#x27;completion_tokens&#x27;: 24, &#x27;prompt_tokens&#x27;: 137, &#x27;total_tokens&#x27;: 161, &#x27;completion_tokens_details&#x27;: {&#x27;audio_tokens&#x27;: None, &#x27;reasoning_tokens&#x27;: 0}, &#x27;prompt_tokens_details&#x27;: {&#x27;audio_tokens&#x27;: None, &#x27;cached_tokens&#x27;: 0}}, &#x27;model_name&#x27;: &#x27;gpt-4o-mini-2024-07-18&#x27;, &#x27;system_fingerprint&#x27;: &#x27;fp_e2bde53e6e&#x27;, &#x27;finish_reason&#x27;: &#x27;stop&#x27;, &#x27;logprobs&#x27;: None}, id=&#x27;run-b5ac3c54-4e26-4da4-853a-d0ab1cba90e0-0&#x27;, usage_metadata={&#x27;input_tokens&#x27;: 137, &#x27;output_tokens&#x27;: 24, &#x27;total_tokens&#x27;: 161, &#x27;input_token_details&#x27;: {&#x27;cache_read&#x27;: 0}, &#x27;output_token_details&#x27;: {&#x27;reasoning&#x27;: 0}})</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>But if we add a duplicate, extra tool response, the call will fail again:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">duplicate_tool_response_2 </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> foo_tool</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">response_message</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">tool_calls</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">1</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">append</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">duplicate_tool_response_2</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">await</span><span class="token plain"> model_with_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">chat_history</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-output codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-output codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">---------------------------------------------------------------------------</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">BadRequestError                           Traceback (most recent call last)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">Cell In[7], line 5</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">      1 duplicate_tool_response_2 = foo_tool.invoke(response_message.tool_calls[1])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">      3 chat_history.append(duplicate_tool_response_2)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">----&gt; 5 await model_with_tools.invoke(chat_history)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/runnables/base.py:5354, in RunnableBindingBase.invoke(self, input, config, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5348 def invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5349     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5350     input: Input,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5351     config: Optional[RunnableConfig] = None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5352     **kwargs: Optional[Any],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5353 ) -&gt; Output:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 5354     return self.bound.invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5355         input,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5356         self._merge_configs(config),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5357         **{**self.kwargs, **kwargs},</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5358     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:286, in BaseChatModel.invoke(self, input, config, stop, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    275 def invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    276     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    277     input: LanguageModelInput,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    281     **kwargs: Any,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    282 ) -&gt; BaseMessage:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    283     config = ensure_config(config)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    284     return cast(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    285         ChatGeneration,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 286         self.generate_prompt(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    287             [self._convert_input(input)],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    288             stop=stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    289             callbacks=config.get(&quot;callbacks&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    290             tags=config.get(&quot;tags&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    291             metadata=config.get(&quot;metadata&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    292             run_name=config.get(&quot;run_name&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    293             run_id=config.pop(&quot;run_id&quot;, None),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    294             **kwargs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    295         ).generations[0][0],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    296     ).message</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:786, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    778 def generate_prompt(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    779     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    780     prompts: list[PromptValue],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    783     **kwargs: Any,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    784 ) -&gt; LLMResult:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    785     prompt_messages = [p.to_messages() for p in prompts]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 786     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:643, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    641         if run_managers:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    642             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 643         raise e</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    644 flattened_outputs = [</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    645     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    646     for res in results</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    647 ]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    648 llm_output = self._combine_llm_outputs([res.llm_output for res in results])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:633, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    630 for i, m in enumerate(messages):</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    631     try:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    632         results.append(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 633             self._generate_with_cache(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    634                 m,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    635                 stop=stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    636                 run_manager=run_managers[i] if run_managers else None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    637                 **kwargs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    638             )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    639         )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    640     except BaseException as e:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    641         if run_managers:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:851, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    849 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    850     if inspect.signature(self._generate).parameters.get(&quot;run_manager&quot;):</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 851         result = self._generate(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    852             messages, stop=stop, run_manager=run_manager, **kwargs</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    853         )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    854     else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    855         result = self._generate(messages, stop=stop, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:686, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    684     generation_info = {&quot;headers&quot;: dict(raw_response.headers)}</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    685 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 686     response = self.client.create(**payload)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    687 return self._create_chat_result(response, generation_info)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:274, in required_args.&lt;locals&gt;.inner.&lt;locals&gt;.wrapper(*args, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    272             msg = f&quot;Missing required argument: {quote(missing[0])}&quot;</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    273     raise TypeError(msg)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 274 return func(*args, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:742, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    704 @required_args([&quot;messages&quot;, &quot;model&quot;], [&quot;messages&quot;, &quot;model&quot;, &quot;stream&quot;])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    705 def create(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    706     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    739     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    740 ) -&gt; ChatCompletion | Stream[ChatCompletionChunk]:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    741     validate_response_format(response_format)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 742     return self._post(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    743         &quot;/chat/completions&quot;,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    744         body=maybe_transform(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    745             {</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    746                 &quot;messages&quot;: messages,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    747                 &quot;model&quot;: model,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    748                 &quot;frequency_penalty&quot;: frequency_penalty,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    749                 &quot;function_call&quot;: function_call,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    750                 &quot;functions&quot;: functions,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    751                 &quot;logit_bias&quot;: logit_bias,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    752                 &quot;logprobs&quot;: logprobs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    753                 &quot;max_completion_tokens&quot;: max_completion_tokens,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    754                 &quot;max_tokens&quot;: max_tokens,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    755                 &quot;metadata&quot;: metadata,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    756                 &quot;n&quot;: n,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    757                 &quot;parallel_tool_calls&quot;: parallel_tool_calls,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    758                 &quot;presence_penalty&quot;: presence_penalty,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    759                 &quot;response_format&quot;: response_format,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    760                 &quot;seed&quot;: seed,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    761                 &quot;service_tier&quot;: service_tier,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    762                 &quot;stop&quot;: stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    763                 &quot;store&quot;: store,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    764                 &quot;stream&quot;: stream,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    765                 &quot;stream_options&quot;: stream_options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    766                 &quot;temperature&quot;: temperature,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    767                 &quot;tool_choice&quot;: tool_choice,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    768                 &quot;tools&quot;: tools,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    769                 &quot;top_logprobs&quot;: top_logprobs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    770                 &quot;top_p&quot;: top_p,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    771                 &quot;user&quot;: user,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    772             },</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    773             completion_create_params.CompletionCreateParams,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    774         ),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    775         options=make_request_options(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    776             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    777         ),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    778         cast_to=ChatCompletion,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    779         stream=stream or False,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    780         stream_cls=Stream[ChatCompletionChunk],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    781     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1270, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1256 def post(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1257     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1258     path: str,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1265     stream_cls: type[_StreamT] | None = None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1266 ) -&gt; ResponseT | _StreamT:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1267     opts = FinalRequestOptions.construct(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1268         method=&quot;post&quot;, url=path, json_data=body, files=to_httpx_files(files), **options</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1269     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    944 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    945     retries_taken = 0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 947 return self._request(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    948     cast_to=cast_to,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    949     options=options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    950     stream=stream,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    951     stream_cls=stream_cls,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    952     retries_taken=retries_taken,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    953 )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1048         err.response.read()</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1050     log.debug(&quot;Re-raising status error&quot;)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 1051     raise self._make_status_error_from_response(err.response) from None</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1053 return self._process_response(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1054     cast_to=cast_to,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1055     options=options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1059     retries_taken=retries_taken,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1060 )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">BadRequestError: Error code: 400 - {&#x27;error&#x27;: {&#x27;message&#x27;: &quot;Invalid parameter: messages with role &#x27;tool&#x27; must be a response to a preceeding message with &#x27;tool_calls&#x27;.&quot;, &#x27;type&#x27;: &#x27;invalid_request_error&#x27;, &#x27;param&#x27;: &#x27;messages.[4].role&#x27;, &#x27;code&#x27;: None}}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You should additionally not pass <code>ToolMessages</code> back to to a model if they are not preceded by an <code>AIMessage</code> with tool calls. For example, this will fail:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">model_with_tools</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">invoke</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">ToolMessage</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">content</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;action completed!&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> tool_call_id</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;dummy&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-output codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-output codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#F5F5F5"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">---------------------------------------------------------------------------</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">BadRequestError                           Traceback (most recent call last)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">Cell In[8], line 1</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">----&gt; 1 model_with_tools.invoke([ToolMessage(content=&quot;action completed!&quot;, tool_call_id=&quot;dummy&quot;)])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/runnables/base.py:5354, in RunnableBindingBase.invoke(self, input, config, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5348 def invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5349     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5350     input: Input,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5351     config: Optional[RunnableConfig] = None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5352     **kwargs: Optional[Any],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5353 ) -&gt; Output:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 5354     return self.bound.invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5355         input,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5356         self._merge_configs(config),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5357         **{**self.kwargs, **kwargs},</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   5358     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:286, in BaseChatModel.invoke(self, input, config, stop, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    275 def invoke(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    276     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    277     input: LanguageModelInput,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    281     **kwargs: Any,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    282 ) -&gt; BaseMessage:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    283     config = ensure_config(config)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    284     return cast(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    285         ChatGeneration,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 286         self.generate_prompt(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    287             [self._convert_input(input)],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    288             stop=stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    289             callbacks=config.get(&quot;callbacks&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    290             tags=config.get(&quot;tags&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    291             metadata=config.get(&quot;metadata&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    292             run_name=config.get(&quot;run_name&quot;),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    293             run_id=config.pop(&quot;run_id&quot;, None),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    294             **kwargs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    295         ).generations[0][0],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    296     ).message</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:786, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    778 def generate_prompt(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    779     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    780     prompts: list[PromptValue],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    783     **kwargs: Any,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    784 ) -&gt; LLMResult:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    785     prompt_messages = [p.to_messages() for p in prompts]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 786     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:643, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    641         if run_managers:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    642             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 643         raise e</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    644 flattened_outputs = [</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    645     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    646     for res in results</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    647 ]</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    648 llm_output = self._combine_llm_outputs([res.llm_output for res in results])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:633, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    630 for i, m in enumerate(messages):</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    631     try:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    632         results.append(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 633             self._generate_with_cache(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    634                 m,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    635                 stop=stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    636                 run_manager=run_managers[i] if run_managers else None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    637                 **kwargs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    638             )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    639         )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    640     except BaseException as e:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    641         if run_managers:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/libs/core/langchain_core/language_models/chat_models.py:851, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    849 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    850     if inspect.signature(self._generate).parameters.get(&quot;run_manager&quot;):</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 851         result = self._generate(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    852             messages, stop=stop, run_manager=run_manager, **kwargs</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    853         )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    854     else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    855         result = self._generate(messages, stop=stop, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:686, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    684     generation_info = {&quot;headers&quot;: dict(raw_response.headers)}</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    685 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 686     response = self.client.create(**payload)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    687 return self._create_chat_result(response, generation_info)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:274, in required_args.&lt;locals&gt;.inner.&lt;locals&gt;.wrapper(*args, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    272             msg = f&quot;Missing required argument: {quote(missing[0])}&quot;</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    273     raise TypeError(msg)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 274 return func(*args, **kwargs)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:742, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    704 @required_args([&quot;messages&quot;, &quot;model&quot;], [&quot;messages&quot;, &quot;model&quot;, &quot;stream&quot;])</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    705 def create(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    706     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    739     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    740 ) -&gt; ChatCompletion | Stream[ChatCompletionChunk]:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    741     validate_response_format(response_format)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 742     return self._post(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    743         &quot;/chat/completions&quot;,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    744         body=maybe_transform(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    745             {</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    746                 &quot;messages&quot;: messages,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    747                 &quot;model&quot;: model,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    748                 &quot;frequency_penalty&quot;: frequency_penalty,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    749                 &quot;function_call&quot;: function_call,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    750                 &quot;functions&quot;: functions,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    751                 &quot;logit_bias&quot;: logit_bias,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    752                 &quot;logprobs&quot;: logprobs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    753                 &quot;max_completion_tokens&quot;: max_completion_tokens,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    754                 &quot;max_tokens&quot;: max_tokens,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    755                 &quot;metadata&quot;: metadata,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    756                 &quot;n&quot;: n,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    757                 &quot;parallel_tool_calls&quot;: parallel_tool_calls,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    758                 &quot;presence_penalty&quot;: presence_penalty,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    759                 &quot;response_format&quot;: response_format,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    760                 &quot;seed&quot;: seed,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    761                 &quot;service_tier&quot;: service_tier,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    762                 &quot;stop&quot;: stop,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    763                 &quot;store&quot;: store,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    764                 &quot;stream&quot;: stream,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    765                 &quot;stream_options&quot;: stream_options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    766                 &quot;temperature&quot;: temperature,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    767                 &quot;tool_choice&quot;: tool_choice,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    768                 &quot;tools&quot;: tools,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    769                 &quot;top_logprobs&quot;: top_logprobs,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    770                 &quot;top_p&quot;: top_p,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    771                 &quot;user&quot;: user,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    772             },</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    773             completion_create_params.CompletionCreateParams,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    774         ),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    775         options=make_request_options(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    776             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    777         ),</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    778         cast_to=ChatCompletion,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    779         stream=stream or False,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    780         stream_cls=Stream[ChatCompletionChunk],</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    781     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1270, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1256 def post(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1257     self,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1258     path: str,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1265     stream_cls: type[_StreamT] | None = None,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1266 ) -&gt; ResponseT | _StreamT:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1267     opts = FinalRequestOptions.construct(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1268         method=&quot;post&quot;, url=path, json_data=body, files=to_httpx_files(files), **options</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1269     )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    944 else:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    945     retries_taken = 0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">--&gt; 947 return self._request(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    948     cast_to=cast_to,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    949     options=options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    950     stream=stream,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    951     stream_cls=stream_cls,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    952     retries_taken=retries_taken,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    953 )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">File ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1048         err.response.read()</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1050     log.debug(&quot;Re-raising status error&quot;)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">-&gt; 1051     raise self._make_status_error_from_response(err.response) from None</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1053 return self._process_response(</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1054     cast_to=cast_to,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1055     options=options,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   (...)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1059     retries_taken=retries_taken,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">   1060 )</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">``````output</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">BadRequestError: Error code: 400 - {&#x27;error&#x27;: {&#x27;message&#x27;: &quot;Invalid parameter: messages with role &#x27;tool&#x27; must be a response to a preceeding message with &#x27;tool_calls&#x27;.&quot;, &#x27;type&#x27;: &#x27;invalid_request_error&#x27;, &#x27;param&#x27;: &#x27;messages.[0].role&#x27;, &#x27;code&#x27;: None}}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>See <a href="/docs/how_to/tool_results_pass_to_model/">this guide</a> for more details on tool calling.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="troubleshooting">Troubleshooting<a href="#troubleshooting" class="hash-link" aria-label="Direct link to Troubleshooting" title="Direct link to Troubleshooting">‚Äã</a></h2>
<p>The following may help resolve this error:</p>
<ul>
<li>If you are using a custom executor rather than a prebuilt one like LangGraph&#x27;s <a href="https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html" target="_blank" rel="noopener noreferrer"><code>ToolNode</code></a>
or the legacy LangChain <a href="/docs/how_to/agent_executor/">AgentExecutor</a>, verify that you are invoking and returning the result for one tool per tool call.</li>
<li>If you are using <a href="/docs/how_to/tools_few_shot/">few-shot tool call examples</a> with messages that you manually create, and you want to simulate a failure,
you still need to pass back a <code>ToolMessage</code> whose content indicates that failure.</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/langchain-ai/langchain/edit/master/docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#troubleshooting" class="table-of-contents__link toc-highlight">Troubleshooting</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://forum.langchain.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LangChain Forum<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/LangChainAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">GitHub</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/langchain-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">Organization<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="footer__link-item">Python<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/langchain-ai/langchainjs" target="_blank" rel="noopener noreferrer" class="footer__link-item">JS/TS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://langchain.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Homepage<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://blog.langchain.dev" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/@LangChain" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2025 LangChain, Inc.</div></div></div></footer></div>
</body>
</html>