<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-guides/productionization/deployments/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Deployment | ü¶úÔ∏èüîó LangChain</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://python.langchain.com/v0.1/img/brand/theme-image.png"><meta data-rh="true" name="twitter:image" content="https://python.langchain.com/v0.1/img/brand/theme-image.png"><meta data-rh="true" property="og:url" content="https://docs.smith.langchain.com/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Deployment | ü¶úÔ∏èüîó LangChain"><meta data-rh="true" name="description" content="In today&#x27;s fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it is crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories:"><meta data-rh="true" property="og:description" content="In today&#x27;s fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it is crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories:"><link data-rh="true" rel="icon" href="/v0.1/img/brand/favicon.png"><link data-rh="true" rel="canonical" href="https://docs.smith.langchain.com/"><link data-rh="true" rel="alternate" href="https://python.langchain.com/v0.1/docs/guides/productionization/deployments/" hreflang="en"><link data-rh="true" rel="alternate" href="https://python.langchain.com/v0.1/docs/guides/productionization/deployments/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://VAU016LAWS-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="ü¶úÔ∏èüîó LangChain" href="/v0.1/opensearch.xml">


<script src="/v0.1/js/google_analytics.js"></script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F" async></script><link rel="stylesheet" href="/v0.1/assets/css/styles.d4021fd6.css">
<link rel="preload" href="/v0.1/assets/js/runtime~main.1cd6ff7f.js" as="script">
<link rel="preload" href="/v0.1/assets/js/main.524fe94c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#FF0000;color:#FFFFFF" role="banner"><div class="content_knG7 announcementBarContent_xLdY">This is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the <a href="https://python.langchain.com/docs/introduction">latest version here</a>.</div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/v0.1/"><div class="navbar__logo"><img src="/v0.1/img/brand/wordmark.png" alt="ü¶úÔ∏èüîó LangChain" class="themedImage_ToTc themedImage--light_HNdA"><img src="/v0.1/img/brand/wordmark-dark.png" alt="ü¶úÔ∏èüîó LangChain" class="themedImage_ToTc themedImage--dark_i4oU"></div></a><a class="navbar__item navbar__link" href="/v0.1/docs/modules/">Components</a><a class="navbar__item navbar__link" href="/v0.1/docs/integrations/platforms/">Integrations</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/v0.1/docs/guides/">Guides</a><a href="https://api.python.langchain.com/en/v0.1/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API Reference<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">More</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/v0.1/docs/people/">People</a></li><li><a class="dropdown__link" href="/v0.1/docs/packages/">Versioning</a></li><li><a class="dropdown__link" href="/v0.1/docs/contributing/">Contributing</a></li><li><a class="dropdown__link" href="/v0.1/docs/templates/">Templates</a></li><li><a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/README.md" target="_blank" rel="noopener noreferrer" class="dropdown__link">Cookbooks<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/v0.1/docs/additional_resources/tutorials/">Tutorials</a></li><li><a class="dropdown__link" href="/v0.1/docs/additional_resources/youtube/">YouTube</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">v0.1</a><ul class="dropdown__menu"><li><a href="https://python.langchain.com/docs/introduction" target="_blank" rel="noopener noreferrer" class="dropdown__link">Latest<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://python.langchain.com/v0.2/docs/introduction" target="_blank" rel="noopener noreferrer" class="dropdown__link">v0.2<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/v0.1/docs/get_started/introduction/">v0.1</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">ü¶úÔ∏èüîó</a><ul class="dropdown__menu"><li><a href="https://smith.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangSmith<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://docs.smith.langchain.com/" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangSmith Docs<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/langchain-ai/langserve" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangServe GitHub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/langchain-ai/langchain/tree/master/templates" target="_blank" rel="noopener noreferrer" class="dropdown__link">Templates GitHub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://templates.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">Templates Hub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://smith.langchain.com/hub" target="_blank" rel="noopener noreferrer" class="dropdown__link">LangChain Hub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://js.langchain.com" target="_blank" rel="noopener noreferrer" class="dropdown__link">JS/TS Docs<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a href="https://chat.langchain.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">üí¨<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link" href="/v0.1/docs/guides/development/">Development</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/development/">Development</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/development/debugging/">Debugging</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/development/extending_langchain/">Extending LangChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/development/local_llms/">Run LLMs locally</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/development/pydantic_compatibility/">Pydantic compatibility</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active" href="/v0.1/docs/guides/productionization/">Productionization</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/productionization/">Productionization</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="menu__link menu__link--sublist menu__link--active" aria-current="page" aria-expanded="true" tabindex="0" href="/v0.1/docs/guides/productionization/deployments/">Deployment</a><button aria-label="Toggle the collapsible sidebar category &#x27;Deployment&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/productionization/deployments/template_repos/">LangChain Templates</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/v0.1/docs/guides/productionization/evaluation/">Evaluation</a><button aria-label="Toggle the collapsible sidebar category &#x27;Evaluation&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/v0.1/docs/guides/productionization/fallbacks/">Fallbacks</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/v0.1/docs/guides/productionization/safety/">Privacy &amp; Safety</a><button aria-label="Toggle the collapsible sidebar category &#x27;Privacy &amp; Safety&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item hidden"><a class="menu__link" href="/v0.1/docs/guides/">Guides</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="theme-doc-version-banner alert alert--warning margin-bottom--md" role="alert"><div>This is documentation for <!-- -->LangChain<!-- --> <b>v0.1</b>, which is no longer actively maintained.</div><div class="margin-top--md">For the current stable version, see <b><a href="https://docs.smith.langchain.com/" target="_blank" rel="noopener noreferrer">this version</a></b> (<!-- -->Latest<!-- -->).</div></div><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/v0.1/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/v0.1/docs/guides/productionization/"><span itemprop="name">Productionization</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Deployment</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Deployment</h1><p>In today&#x27;s fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it is crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories:</p><ul><li><p><strong>Case 1: Utilizing External LLM Providers (OpenAI, Anthropic, etc.)</strong>
In this scenario, most of the computational burden is handled by the LLM providers, while LangChain simplifies the implementation of business logic around these services. This approach includes features such as prompt templating, chat message generation, caching, vector embedding database creation, preprocessing, etc.</p></li><li><p><strong>Case 2: Self-hosted Open-Source Models</strong>
Alternatively, developers can opt to use smaller, yet comparably capable, self-hosted open-source LLM models. This approach can significantly decrease costs, latency, and privacy concerns associated with transferring data to external LLM providers.</p></li></ul><p>Regardless of the framework that forms the backbone of your product, deploying LLM applications comes with its own set of challenges. It&#x27;s vital to understand the trade-offs and key considerations when evaluating serving frameworks.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="outline">Outline<a href="#outline" class="hash-link" aria-label="Direct link to Outline" title="Direct link to Outline">‚Äã</a></h2><p>This guide aims to provide a comprehensive overview of the requirements for deploying LLMs in a production setting, focusing on:</p><ul><li><strong>Designing a Robust LLM Application Service</strong></li><li><strong>Maintaining Cost-Efficiency</strong></li><li><strong>Ensuring Rapid Iteration</strong></li></ul><p>Understanding these components is crucial when assessing serving systems. LangChain integrates with several open-source projects designed to tackle these issues, providing a robust framework for productionizing your LLM applications. Some notable frameworks include:</p><ul><li><a href="/v0.1/docs/integrations/providers/ray_serve/">Ray Serve</a></li><li><a href="https://github.com/bentoml/BentoML" target="_blank" rel="noopener noreferrer">BentoML</a></li><li><a href="/v0.1/docs/integrations/providers/openllm/">OpenLLM</a></li><li><a href="/v0.1/docs/integrations/providers/modal/">Modal</a></li><li><a href="/v0.1/docs/integrations/providers/jina/">Jina</a></li></ul><p>These links will provide further information on each ecosystem, assisting you in finding the best fit for your LLM deployment needs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="designing-a-robust-llm-application-service">Designing a Robust LLM Application Service<a href="#designing-a-robust-llm-application-service" class="hash-link" aria-label="Direct link to Designing a Robust LLM Application Service" title="Direct link to Designing a Robust LLM Application Service">‚Äã</a></h2><p>When deploying an LLM service in production, it&#x27;s imperative to provide a seamless user experience free from outages. Achieving 24/7 service availability involves creating and maintaining several sub-systems surrounding your application.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring">Monitoring<a href="#monitoring" class="hash-link" aria-label="Direct link to Monitoring" title="Direct link to Monitoring">‚Äã</a></h3><p>Monitoring forms an integral part of any system running in a production environment. In the context of LLMs, it is essential to monitor both performance and quality metrics.</p><p><strong>Performance Metrics:</strong> These metrics provide insights into the efficiency and capacity of your model. Here are some key examples:</p><ul><li>Query per second (QPS): This measures the number of queries your model processes in a second, offering insights into its utilization.</li><li>Latency: This metric quantifies the delay from when your client sends a request to when they receive a response.</li><li>Tokens Per Second (TPS): This represents the number of tokens your model can generate in a second.</li></ul><p><strong>Quality Metrics:</strong> These metrics are typically customized according to the business use-case. For instance, how does the output of your system compare to a baseline, such as a previous version? Although these metrics can be calculated offline, you need to log the necessary data to use them later.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="fault-tolerance">Fault tolerance<a href="#fault-tolerance" class="hash-link" aria-label="Direct link to Fault tolerance" title="Direct link to Fault tolerance">‚Äã</a></h3><p>Your application may encounter errors such as exceptions in your model inference or business logic code, causing failures and disrupting traffic. Other potential issues could arise from the machine running your application, such as unexpected hardware breakdowns or loss of spot-instances during high-demand periods. One way to mitigate these risks is by increasing redundancy through replica scaling and implementing recovery mechanisms for failed replicas. However, model replicas aren&#x27;t the only potential points of failure. It&#x27;s essential to build resilience against various failures that could occur at any point in your stack.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="zero-down-time-upgrade">Zero down time upgrade<a href="#zero-down-time-upgrade" class="hash-link" aria-label="Direct link to Zero down time upgrade" title="Direct link to Zero down time upgrade">‚Äã</a></h3><p>System upgrades are often necessary but can result in service disruptions if not handled correctly. One way to prevent downtime during upgrades is by implementing a smooth transition process from the old version to the new one. Ideally, the new version of your LLM service is deployed, and traffic gradually shifts from the old to the new version, maintaining a constant QPS throughout the process.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-balancing">Load balancing<a href="#load-balancing" class="hash-link" aria-label="Direct link to Load balancing" title="Direct link to Load balancing">‚Äã</a></h3><p>Load balancing, in simple terms, is a technique to distribute work evenly across multiple computers, servers, or other resources to optimize the utilization of the system, maximize throughput, minimize response time, and avoid overload of any single resource. Think of it as a traffic officer directing cars (requests) to different roads (servers) so that no single road becomes too congested.</p><p>There are several strategies for load balancing. For example, one common method is the <em>Round Robin</em> strategy, where each request is sent to the next server in line, cycling back to the first when all servers have received a request. This works well when all servers are equally capable. However, if some servers are more powerful than others, you might use a <em>Weighted Round Robin</em> or <em>Least Connections</em> strategy, where more requests are sent to the more powerful servers, or to those currently handling the fewest active requests. Let&#x27;s imagine you&#x27;re running a LLM chain. If your application becomes popular, you could have hundreds or even thousands of users asking questions at the same time. If one server gets too busy (high load), the load balancer would direct new requests to another server that is less busy. This way, all your users get a timely response and the system remains stable.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="maintaining-cost-efficiency-and-scalability">Maintaining Cost-Efficiency and Scalability<a href="#maintaining-cost-efficiency-and-scalability" class="hash-link" aria-label="Direct link to Maintaining Cost-Efficiency and Scalability" title="Direct link to Maintaining Cost-Efficiency and Scalability">‚Äã</a></h2><p>Deploying LLM services can be costly, especially when you&#x27;re handling a large volume of user interactions. Charges by LLM providers are usually based on tokens used, making a chat system inference on these models potentially expensive. However, several strategies can help manage these costs without compromising the quality of the service.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-hosting-models">Self-hosting models<a href="#self-hosting-models" class="hash-link" aria-label="Direct link to Self-hosting models" title="Direct link to Self-hosting models">‚Äã</a></h3><p>Several smaller and open-source LLMs are emerging to tackle the issue of reliance on LLM providers. Self-hosting allows you to maintain similar quality to LLM provider models while managing costs. The challenge lies in building a reliable, high-performing LLM serving system on your own machines. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-management-and-auto-scaling">Resource Management and Auto-Scaling<a href="#resource-management-and-auto-scaling" class="hash-link" aria-label="Direct link to Resource Management and Auto-Scaling" title="Direct link to Resource Management and Auto-Scaling">‚Äã</a></h3><p>Computational logic within your application requires precise resource allocation. For instance, if part of your traffic is served by an OpenAI endpoint and another part by a self-hosted model, it&#x27;s crucial to allocate suitable resources for each. Auto-scaling‚Äîadjusting resource allocation based on traffic‚Äîcan significantly impact the cost of running your application. This strategy requires a balance between cost and responsiveness, ensuring neither resource over-provisioning nor compromised application responsiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="utilizing-spot-instances">Utilizing Spot Instances<a href="#utilizing-spot-instances" class="hash-link" aria-label="Direct link to Utilizing Spot Instances" title="Direct link to Utilizing Spot Instances">‚Äã</a></h3><p>On platforms like AWS, spot instances offer substantial cost savings, typically priced at about a third of on-demand instances. The trade-off is a higher crash rate, necessitating a robust fault-tolerance mechanism for effective use.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="independent-scaling">Independent Scaling<a href="#independent-scaling" class="hash-link" aria-label="Direct link to Independent Scaling" title="Direct link to Independent Scaling">‚Äã</a></h3><p>When self-hosting your models, you should consider independent scaling. For example, if you have two translation models, one fine-tuned for French and another for Spanish, incoming requests might necessitate different scaling requirements for each.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="batching-requests">Batching requests<a href="#batching-requests" class="hash-link" aria-label="Direct link to Batching requests" title="Direct link to Batching requests">‚Äã</a></h3><p>In the context of Large Language Models, batching requests can enhance efficiency by better utilizing your GPU resources. GPUs are inherently parallel processors, designed to handle multiple tasks simultaneously. If you send individual requests to the model, the GPU might not be fully utilized as it&#x27;s only working on a single task at a time. On the other hand, by batching requests together, you&#x27;re allowing the GPU to work on multiple tasks at once, maximizing its utilization and improving inference speed. This not only leads to cost savings but can also improve the overall latency of your LLM service.</p><p>In summary, managing costs while scaling your LLM services requires a strategic approach. Utilizing self-hosting models, managing resources effectively, employing auto-scaling, using spot instances, independently scaling models, and batching requests are key strategies to consider. Open-source libraries such as Ray Serve and BentoML are designed to deal with these complexities. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ensuring-rapid-iteration">Ensuring Rapid Iteration<a href="#ensuring-rapid-iteration" class="hash-link" aria-label="Direct link to Ensuring Rapid Iteration" title="Direct link to Ensuring Rapid Iteration">‚Äã</a></h2><p>The LLM landscape is evolving at an unprecedented pace, with new libraries and model architectures being introduced constantly. Consequently, it&#x27;s crucial to avoid tying yourself to a solution specific to one particular framework. This is especially relevant in serving, where changes to your infrastructure can be time-consuming, expensive, and risky. Strive for infrastructure that is not locked into any specific machine learning library or framework, but instead offers a general-purpose, scalable serving layer. Here are some aspects where flexibility plays a key role:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-composition">Model composition<a href="#model-composition" class="hash-link" aria-label="Direct link to Model composition" title="Direct link to Model composition">‚Äã</a></h3><p>Deploying systems like LangChain demands the ability to piece together different models and connect them via logic. Take the example of building a natural language input SQL query engine. Querying an LLM and obtaining the SQL command is only part of the system. You need to extract metadata from the connected database, construct a prompt for the LLM, run the SQL query on an engine, collect and feedback the response to the LLM as the query runs, and present the results to the user. This demonstrates the need to seamlessly integrate various complex components built in Python into a dynamic chain of logical blocks that can be served together.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cloud-providers">Cloud providers<a href="#cloud-providers" class="hash-link" aria-label="Direct link to Cloud providers" title="Direct link to Cloud providers">‚Äã</a></h2><p>Many hosted solutions are restricted to a single cloud provider, which can limit your options in today&#x27;s multi-cloud world. Depending on where your other infrastructure components are built, you might prefer to stick with your chosen cloud provider.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="infrastructure-as-code-iac">Infrastructure as Code (IaC)<a href="#infrastructure-as-code-iac" class="hash-link" aria-label="Direct link to Infrastructure as Code (IaC)" title="Direct link to Infrastructure as Code (IaC)">‚Äã</a></h2><p>Rapid iteration also involves the ability to recreate your infrastructure quickly and reliably. This is where Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Kubernetes YAML files come into play. They allow you to define your infrastructure in code files, which can be version controlled and quickly deployed, enabling faster and more reliable iterations.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cicd">CI/CD<a href="#cicd" class="hash-link" aria-label="Direct link to CI/CD" title="Direct link to CI/CD">‚Äã</a></h2><p>In a fast-paced environment, implementing CI/CD pipelines can significantly speed up the iteration process. They help automate the testing and deployment of your LLM applications, reducing the risk of errors and enabling faster feedback and iteration.</p></div><div style="display:flex;flex-direction:column"><hr><h4>Help us out by providing feedback on this documentation page:</h4><div style="display:flex;gap:5px"><div style="display:flex;align-items:center;padding-top:10px;padding-bottom:10px;padding-left:22px;padding-right:22px;border:1px solid gray;border-radius:6px;gap:10px;cursor:pointer;font-size:16px;font-weight:600" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="#166534" style="width:24px;height:24px"><path stroke-linecap="round" stroke-linejoin="round" d="M6.633 10.25c.806 0 1.533-.446 2.031-1.08a9.041 9.041 0 0 1 2.861-2.4c.723-.384 1.35-.956 1.653-1.715a4.498 4.498 0 0 0 .322-1.672V2.75a.75.75 0 0 1 .75-.75 2.25 2.25 0 0 1 2.25 2.25c0 1.152-.26 2.243-.723 3.218-.266.558.107 1.282.725 1.282m0 0h3.126c1.026 0 1.945.694 2.054 1.715.045.422.068.85.068 1.285a11.95 11.95 0 0 1-2.649 7.521c-.388.482-.987.729-1.605.729H13.48c-.483 0-.964-.078-1.423-.23l-3.114-1.04a4.501 4.501 0 0 0-1.423-.23H5.904m10.598-9.75H14.25M5.904 18.5c.083.205.173.405.27.602.197.4-.078.898-.523.898h-.908c-.889 0-1.713-.518-1.972-1.368a12 12 0 0 1-.521-3.507c0-1.553.295-3.036.831-4.398C3.387 9.953 4.167 9.5 5 9.5h1.053c.472 0 .745.556.5.96a8.958 8.958 0 0 0-1.302 4.665c0 1.194.232 2.333.654 3.375Z"></path></svg></div><div style="display:flex;align-items:center;padding-top:10px;padding-bottom:10px;padding-left:22px;padding-right:22px;border:1px solid gray;border-radius:6px;gap:10px;cursor:pointer;font-size:16px;font-weight:600" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="#991b1b" style="width:24px;height:24px"><path stroke-linecap="round" stroke-linejoin="round" d="M7.498 15.25H4.372c-1.026 0-1.945-.694-2.054-1.715a12.137 12.137 0 0 1-.068-1.285c0-2.848.992-5.464 2.649-7.521C5.287 4.247 5.886 4 6.504 4h4.016a4.5 4.5 0 0 1 1.423.23l3.114 1.04a4.5 4.5 0 0 0 1.423.23h1.294M7.498 15.25c.618 0 .991.724.725 1.282A7.471 7.471 0 0 0 7.5 19.75 2.25 2.25 0 0 0 9.75 22a.75.75 0 0 0 .75-.75v-.633c0-.573.11-1.14.322-1.672.304-.76.93-1.33 1.653-1.715a9.04 9.04 0 0 0 2.86-2.4c.498-.634 1.226-1.08 2.032-1.08h.384m-10.253 1.5H9.7m8.075-9.75c.01.05.027.1.05.148.593 1.2.925 2.55.925 3.977 0 1.487-.36 2.89-.999 4.125m.023-8.25c-.076-.365.183-.75.575-.75h.908c.889 0 1.713.518 1.972 1.368.339 1.11.521 2.287.521 3.507 0 1.553-.295 3.036-.831 4.398-.306.774-1.086 1.227-1.918 1.227h-1.053c-.472 0-.745-.556-.5-.96a8.95 8.95 0 0 0 .303-.54"></path></svg></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/v0.1/docs/guides/productionization/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Productionization</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/v0.1/docs/guides/productionization/deployments/template_repos/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">LangChain Templates</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#outline" class="table-of-contents__link toc-highlight">Outline</a></li><li><a href="#designing-a-robust-llm-application-service" class="table-of-contents__link toc-highlight">Designing a Robust LLM Application Service</a><ul><li><a href="#monitoring" class="table-of-contents__link toc-highlight">Monitoring</a></li><li><a href="#fault-tolerance" class="table-of-contents__link toc-highlight">Fault tolerance</a></li><li><a href="#zero-down-time-upgrade" class="table-of-contents__link toc-highlight">Zero down time upgrade</a></li><li><a href="#load-balancing" class="table-of-contents__link toc-highlight">Load balancing</a></li></ul></li><li><a href="#maintaining-cost-efficiency-and-scalability" class="table-of-contents__link toc-highlight">Maintaining Cost-Efficiency and Scalability</a><ul><li><a href="#self-hosting-models" class="table-of-contents__link toc-highlight">Self-hosting models</a></li><li><a href="#resource-management-and-auto-scaling" class="table-of-contents__link toc-highlight">Resource Management and Auto-Scaling</a></li><li><a href="#utilizing-spot-instances" class="table-of-contents__link toc-highlight">Utilizing Spot Instances</a></li><li><a href="#independent-scaling" class="table-of-contents__link toc-highlight">Independent Scaling</a></li><li><a href="#batching-requests" class="table-of-contents__link toc-highlight">Batching requests</a></li></ul></li><li><a href="#ensuring-rapid-iteration" class="table-of-contents__link toc-highlight">Ensuring Rapid Iteration</a><ul><li><a href="#model-composition" class="table-of-contents__link toc-highlight">Model composition</a></li></ul></li><li><a href="#cloud-providers" class="table-of-contents__link toc-highlight">Cloud providers</a></li><li><a href="#infrastructure-as-code-iac" class="table-of-contents__link toc-highlight">Infrastructure as Code (IaC)</a></li><li><a href="#cicd" class="table-of-contents__link toc-highlight">CI/CD</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/cU2adEyC7w" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/LangChainAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">GitHub</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer" class="footer__link-item">Python<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/langchain-ai/langchainjs" target="_blank" rel="noopener noreferrer" class="footer__link-item">JS/TS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://langchain.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Homepage<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://blog.langchain.dev" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/@LangChain" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2024 LangChain, Inc.</div></div></div></footer></div>
<script src="/v0.1/assets/js/runtime~main.1cd6ff7f.js"></script>
<script src="/v0.1/assets/js/main.524fe94c.js"></script>
</body>
</html>