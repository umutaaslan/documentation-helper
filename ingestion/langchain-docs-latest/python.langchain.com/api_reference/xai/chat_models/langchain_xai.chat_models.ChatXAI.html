
<!DOCTYPE html>

<html data-content_root="../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F"></script>
<script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-9B66JQQH2F');
    </script>
<title>ChatXAI — 🦜🔗 LangChain  documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!--
    this give us a css class that will be invisible only if js is disabled
  -->
<noscript>
<style>
      .pst-js-only { display: none !important; }

    </style>
</noscript>
<!-- Loaded before other Sphinx assets -->
<link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="../../_static/pygments.css?v=8f2a1f02" rel="stylesheet" type="text/css"/>
<link href="../../_static/autodoc_pydantic.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/custom.css?v=8e9fa5b3" rel="stylesheet" type="text/css"/>
<!-- So that users can add custom icons -->
<script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" rel="preload"/>
<link as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" rel="preload"/>
<script src="../../_static/documentation_options.js?v=3b5cce75"></script>
<script src="../../_static/doctools.js?v=9bcbadda"></script>
<script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=f281be69"></script>
<script src="../../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'xai/chat_models/langchain_xai.chat_models.ChatXAI';</script>
<link href="../../_static/favicon.png" rel="icon"/>
<link href="../../search.html" rel="search" title="Search"/>
<link href="../chat_models.html" rel="prev" title="chat_models"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="" name="docsearch:version"/>
<meta content="Jul 10, 2025" name="docbuild:last-update"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<dialog id="pst-search-dialog">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</dialog>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../../index.html">
<img alt="🦜🔗 LangChain  documentation - Home" class="logo__image only-light" src="../../_static/wordmark-api.svg"/>
<img alt="🦜🔗 LangChain  documentation - Home" class="logo__image only-dark pst-js-only" src="../../_static/wordmark-api-dark.svg"/>
</a></div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../../reference.html">
    Reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item navbar-persistent--container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<button aria-label="Color mode" class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" data-bs-placement="bottom" data-bs-title="Color mode" data-bs-toggle="tooltip">
<i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light" title="Light"></i>
<i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark" title="Dark"></i>
<i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto" title="System Settings"></i>
</button></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="navbar-persistent--mobile">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<dialog id="pst-primary-sidebar-modal"></dialog>
<div class="bd-sidebar-primary bd-sidebar" id="pst-primary-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../../reference.html">
    Reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<button aria-label="Color mode" class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" data-bs-placement="bottom" data-bs-title="Color mode" data-bs-toggle="tooltip">
<i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light" title="Light"></i>
<i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark" title="Dark"></i>
<i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto" title="System Settings"></i>
</button></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Base packages</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../langchain/index.html">Langchain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../text_splitters/index.html">Text Splitters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/index.html">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental/index.html">Experimental</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Integrations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ai21/index.html">AI21</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../anthropic/index.html">Anthropic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../astradb/index.html">AstraDB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../aws/index.html">AWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../azure_ai/index.html">Azure Ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../azure_dynamic_sessions/index.html">Azure Dynamic Sessions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cerebras/index.html">Cerebras</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chroma/index.html">Chroma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cohere/index.html">Cohere</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../db2/index.html">Db2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deepseek/index.html">Deepseek</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../elasticsearch/index.html">Elasticsearch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../exa/index.html">Exa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fireworks/index.html">Fireworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../google_community/index.html">Google Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../google_genai/index.html">Google GenAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../google_vertexai/index.html">Google VertexAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../groq/index.html">Groq</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../huggingface/index.html">Huggingface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ibm/index.html">IBM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../milvus/index.html">Milvus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mistralai/index.html">MistralAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mongodb/index.html">MongoDB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../neo4j/index.html">Neo4J</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nomic/index.html">Nomic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nvidia_ai_endpoints/index.html">Nvidia Ai Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ollama/index.html">Ollama</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openai/index.html">OpenAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perplexity/index.html">Perplexity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pinecone/index.html">Pinecone</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../postgres/index.html">Postgres</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prompty/index.html">Prompty</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../qdrant/index.html">Qdrant</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../redis/index.html">Redis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sema4/index.html">Sema4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../snowflake/index.html">Snowflake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sqlserver/index.html">Sqlserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../standard_tests/index.html">Standard Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tavily/index.html">Tavily</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../together/index.html">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../unstructured/index.html">Unstructured</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../upstage/index.html">Upstage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../weaviate/index.html">Weaviate</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">XAI</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../chat_models.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">chat_models</span></code></a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">ChatXAI</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
<div class="sidebar-primary-item">
<div class="flat" data-ea-manual="true" data-ea-publisher="readthedocs" data-ea-type="readthedocs-sidebar" id="ethical-ad-placement">
</div></div>
</div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../../reference.html">LangChain Python API Reference</a></li>
<li class="breadcrumb-item"><a class="nav-link" href="../index.html">langchain-xai: 0.2.4</a></li>
<li class="breadcrumb-item"><a class="nav-link" href="../chat_models.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">chat_models</span></code></a></li>
<li aria-current="page" class="breadcrumb-item active"><span class="ellipsis">ChatXAI</span></li>
</ul>
</nav>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="chatxai">
<h1>ChatXAI<a class="headerlink" href="#chatxai" title="Link to this heading">#</a></h1>
<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain_xai.chat_models.</span></span><span class="sig-name descname"><span class="pre">ChatXAI</span></span><a class="reference internal" href="../../_modules/langchain_xai/chat_models.html#ChatXAI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_openai.chat_models.base.BaseChatOpenAI" title="langchain_openai.chat_models.base.BaseChatOpenAI"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseChatOpenAI</span></code></a></p>
<p>ChatXAI chat model.</p>
<p>Refer to <a class="reference external" href="https://docs.x.ai/docs/api-reference#chat-completions">xAI’s documentation</a>
for more nuanced details on the API’s behavior and supported parameters.</p>
<dl>
<dt>Setup:</dt><dd><p>Install <code class="docutils literal notranslate"><span class="pre">langchain-xai</span></code> and set environment variable <code class="docutils literal notranslate"><span class="pre">XAI_API_KEY</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>langchain-xai
<span class="nb">export</span><span class="w"> </span><span class="nv">XAI_API_KEY</span><span class="o">=</span><span class="s2">"your-api-key"</span>
</pre></div>
</div>
</dd>
<dt>Key init args — completion params:</dt><dd><dl class="simple">
<dt>model: str</dt><dd><p>Name of model to use.</p>
</dd>
<dt>temperature: float</dt><dd><p>Sampling temperature between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">2</span></code>. Higher values mean more random completions,
while lower values (like <code class="docutils literal notranslate"><span class="pre">0.2</span></code>) mean more focused and deterministic completions.
(Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.)</p>
</dd>
<dt>max_tokens: Optional[int]</dt><dd><p>Max number of tokens to generate. Refer to your <a class="reference external" href="https://docs.x.ai/docs/models#model-pricing">model’s documentation</a>
for the maximum number of tokens it can generate.</p>
</dd>
<dt>logprobs: Optional[bool]</dt><dd><p>Whether to return logprobs.</p>
</dd>
</dl>
</dd>
<dt>Key init args — client params:</dt><dd><dl class="simple">
<dt>timeout: Union[float, Tuple[float, float], Any, None]</dt><dd><p>Timeout for requests.</p>
</dd>
<dt>max_retries: int</dt><dd><p>Max number of retries.</p>
</dd>
<dt>api_key: Optional[str]</dt><dd><p>xAI API key. If not passed in will be read from env var <code class="docutils literal notranslate"><span class="pre">XAI_API_KEY</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Instantiate:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_xai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatXAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatXAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"grok-4"</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="c1"># api_key="...",</span>
    <span class="c1"># other params...</span>
<span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>Invoke:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span>
        <span class="s2">"system"</span><span class="p">,</span>
        <span class="s2">"You are a helpful translator. Translate the user sentence to French."</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="p">(</span><span class="s2">"human"</span><span class="p">,</span> <span class="s2">"I love programming."</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AIMessage</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="s2">"J'adore la programmation."</span><span class="p">,</span>
    <span class="n">response_metadata</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'token_usage'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'completion_tokens'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">'prompt_tokens'</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">'total_tokens'</span><span class="p">:</span> <span class="mi">41</span><span class="p">},</span>
        <span class="s1">'model_name'</span><span class="p">:</span> <span class="s1">'grok-4'</span><span class="p">,</span>
        <span class="s1">'system_fingerprint'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">'finish_reason'</span><span class="p">:</span> <span class="s1">'stop'</span><span class="p">,</span>
        <span class="s1">'logprobs'</span><span class="p">:</span> <span class="kc">None</span>
    <span class="p">},</span>
    <span class="nb">id</span><span class="o">=</span><span class="s1">'run-168dceca-3b8b-4283-94e3-4c739dbc1525-0'</span><span class="p">,</span>
    <span class="n">usage_metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">'input_tokens'</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">'output_tokens'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">'total_tokens'</span><span class="p">:</span> <span class="mi">41</span><span class="p">})</span>
</pre></div>
</div>
</dd>
<dt>Stream:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">text</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">content</span><span class="o">=</span><span class="s1">'J'</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s2">"'"</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s1">'ad'</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s1">'ore'</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s1">' la'</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s1">' programm'</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s1">'ation'</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s1">'.'</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
<span class="n">content</span><span class="o">=</span><span class="s1">''</span> <span class="n">response_metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">'finish_reason'</span><span class="p">:</span> <span class="s1">'stop'</span><span class="p">,</span> <span class="s1">'model_name'</span><span class="p">:</span> <span class="s1">'grok-4'</span><span class="p">}</span> <span class="nb">id</span><span class="o">=</span><span class="s1">'run-1bc996b5-293f-4114-96a1-e0f755c05eb9'</span>
</pre></div>
</div>
</dd>
<dt>Async:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>

<span class="c1"># stream:</span>
<span class="c1"># async for chunk in (await llm.astream(messages))</span>

<span class="c1"># batch:</span>
<span class="c1"># await llm.abatch([messages])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AIMessage</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="s2">"J'adore la programmation."</span><span class="p">,</span>
    <span class="n">response_metadata</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'token_usage'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'completion_tokens'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">'prompt_tokens'</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">'total_tokens'</span><span class="p">:</span> <span class="mi">41</span><span class="p">},</span>
        <span class="s1">'model_name'</span><span class="p">:</span> <span class="s1">'grok-4'</span><span class="p">,</span>
        <span class="s1">'system_fingerprint'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">'finish_reason'</span><span class="p">:</span> <span class="s1">'stop'</span><span class="p">,</span>
        <span class="s1">'logprobs'</span><span class="p">:</span> <span class="kc">None</span>
    <span class="p">},</span>
    <span class="nb">id</span><span class="o">=</span><span class="s1">'run-09371a11-7f72-4c53-8e7c-9de5c238b34c-0'</span><span class="p">,</span>
    <span class="n">usage_metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">'input_tokens'</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">'output_tokens'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">'total_tokens'</span><span class="p">:</span> <span class="mi">41</span><span class="p">})</span>
</pre></div>
</div>
</dd>
<dt>Reasoning:</dt><dd><p><a class="reference external" href="https://docs.x.ai/docs/models#model-pricing">Certain xAI models</a> support reasoning,
which allows the model to provide reasoning content along with the response.</p>
<p>If provided, reasoning content is returned under the <code class="docutils literal notranslate"><span class="pre">additional_kwargs</span></code> field of the
AIMessage or AIMessageChunk.</p>
<p>If supported, reasoning effort can be specified in the model constructor’s <code class="docutils literal notranslate"><span class="pre">extra_body</span></code>
argument, which will control the amount of reasoning the model does. The value can be one of
<code class="docutils literal notranslate"><span class="pre">'low'</span></code> or <code class="docutils literal notranslate"><span class="pre">'high'</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ChatXAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"grok-3-mini"</span><span class="p">,</span>
    <span class="n">extra_body</span><span class="o">=</span><span class="p">{</span><span class="s2">"reasoning_effort"</span><span class="p">:</span> <span class="s2">"high"</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As of 2025-07-10, <code class="docutils literal notranslate"><span class="pre">reasoning_content</span></code> is only returned in Grok 3 models, such as
<a class="reference external" href="https://docs.x.ai/docs/models/grok-3-mini">Grok 3 Mini</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that in <a class="reference external" href="https://docs.x.ai/docs/models/grok-4-0709">Grok 4</a>, as of 2025-07-10,
reasoning is not exposed in <code class="docutils literal notranslate"><span class="pre">reasoning_content</span></code> (other than initial <code class="docutils literal notranslate"><span class="pre">'Thinking...'</span></code> text),
reasoning cannot be disabled, and the <code class="docutils literal notranslate"><span class="pre">reasoning_effort</span></code> cannot be specified.</p>
</div>
</dd>
<dt>Tool calling / function calling:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatXAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"grok-4"</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GetWeather</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''Get the current weather in a given location'''</span>

    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"The city and state, e.g. San Francisco, CA"</span>
    <span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GetPopulation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''Get the current population in a given location'''</span>

    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"The city and state, e.g. San Francisco, CA"</span>
    <span class="p">)</span>

<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">GetWeather</span><span class="p">,</span> <span class="n">GetPopulation</span><span class="p">])</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"Which city is bigger: LA or NY?"</span>
<span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">tool_calls</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">'name'</span><span class="p">:</span> <span class="s1">'GetPopulation'</span><span class="p">,</span>
        <span class="s1">'args'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'location'</span><span class="p">:</span> <span class="s1">'NY'</span><span class="p">},</span>
        <span class="s1">'id'</span><span class="p">:</span> <span class="s1">'call_m5tstyn2004pre9bfuxvom8x'</span><span class="p">,</span>
        <span class="s1">'type'</span><span class="p">:</span> <span class="s1">'tool_call'</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s1">'name'</span><span class="p">:</span> <span class="s1">'GetPopulation'</span><span class="p">,</span>
        <span class="s1">'args'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'location'</span><span class="p">:</span> <span class="s1">'LA'</span><span class="p">},</span>
        <span class="s1">'id'</span><span class="p">:</span> <span class="s1">'call_0vjgq455gq1av5sp9eb1pw6a'</span><span class="p">,</span>
        <span class="s1">'type'</span><span class="p">:</span> <span class="s1">'tool_call'</span>
    <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>With stream response, the tool / function call will be returned in whole in a
single chunk, instead of being streamed across chunks.</p>
</div>
<p>Tool choice can be controlled by setting the <code class="docutils literal notranslate"><span class="pre">tool_choice</span></code> parameter in the model
constructor’s <code class="docutils literal notranslate"><span class="pre">extra_body</span></code> argument. For example, to disable tool / function calling:
.. code-block:: python</p>
<blockquote>
<div><p>llm = ChatXAI(model=”grok-4”, extra_body={“tool_choice”: “none”})</p>
</div></blockquote>
<p>To require that the model always calls a tool / function, set <code class="docutils literal notranslate"><span class="pre">tool_choice</span></code> to <code class="docutils literal notranslate"><span class="pre">'required'</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatXAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"grok-4"</span><span class="p">,</span> <span class="n">extra_body</span><span class="o">=</span><span class="p">{</span><span class="s2">"tool_choice"</span><span class="p">:</span> <span class="s2">"required"</span><span class="p">})</span>
</pre></div>
</div>
<p>To specify a tool / function to call, set <code class="docutils literal notranslate"><span class="pre">tool_choice</span></code> to the name of the tool / function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatXAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"grok-4"</span><span class="p">,</span>
    <span class="n">extra_body</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"tool_choice"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span> <span class="s2">"function"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"name"</span><span class="p">:</span> <span class="s2">"GetWeather"</span><span class="p">}}</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GetWeather</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    \<span class="s2">"</span><span class="se">\"\"</span><span class="s2">Get the current weather in a given location</span><span class="se">\"\"\"</span>

    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">'The city and state, e.g. San Francisco, CA'</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GetPopulation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    \<span class="s2">"</span><span class="se">\"\"</span><span class="s2">Get the current population in a given location</span><span class="se">\"\"\"</span>

    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">'The city and state, e.g. San Francisco, CA'</span><span class="p">)</span>


<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">GetWeather</span><span class="p">,</span> <span class="n">GetPopulation</span><span class="p">])</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"Which city is bigger: LA or NY?"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">tool_calls</span>
</pre></div>
</div>
<p>The resulting tool call would be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[{</span><span class="s1">'name'</span><span class="p">:</span> <span class="s1">'GetWeather'</span><span class="p">,</span>
<span class="s1">'args'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'location'</span><span class="p">:</span> <span class="s1">'Los Angeles, CA'</span><span class="p">},</span>
<span class="s1">'id'</span><span class="p">:</span> <span class="s1">'call_81668711'</span><span class="p">,</span>
<span class="s1">'type'</span><span class="p">:</span> <span class="s1">'tool_call'</span><span class="p">}]</span>
</pre></div>
</div>
</dd>
<dt>Parallel tool calling / parallel function calling:</dt><dd><p>By default, parallel tool / function calling is enabled, so you can process
multiple function calls in one request/response cycle. When two or more tool calls
are required, all of the tool call requests will be included in the response body.</p>
</dd>
<dt>Structured output:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''Joke to tell user.'''</span>

    <span class="n">setup</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The setup of the joke"</span><span class="p">)</span>
    <span class="n">punchline</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The punchline to the joke"</span><span class="p">)</span>
    <span class="n">rating</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"How funny the joke is, from 1 to 10"</span><span class="p">)</span>


<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Joke</span><span class="p">)</span>
<span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"Tell me a joke about cats"</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Joke</span><span class="p">(</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">'Why was the cat sitting on the computer?'</span><span class="p">,</span>
    <span class="n">punchline</span><span class="o">=</span><span class="s1">'To keep an eye on the mouse!'</span><span class="p">,</span>
    <span class="n">rating</span><span class="o">=</span><span class="mi">7</span>
<span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>Live Search:</dt><dd><p>xAI supports a <a class="reference external" href="https://docs.x.ai/docs/guides/live-search">Live Search</a>
feature that enables Grok to ground its answers using results from web searches.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_xai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatXAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatXAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"grok-4"</span><span class="p">,</span>
    <span class="n">search_parameters</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"mode"</span><span class="p">:</span> <span class="s2">"auto"</span><span class="p">,</span>
        <span class="c1"># Example optional parameters below:</span>
        <span class="s2">"max_search_results"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">"from_date"</span><span class="p">:</span> <span class="s2">"2025-05-26"</span><span class="p">,</span>
        <span class="s2">"to_date"</span><span class="p">:</span> <span class="s2">"2025-05-27"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"Provide me a digest of world news in the last 24 hours."</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference external" href="https://docs.x.ai/docs/guides/live-search#returning-citations">Citations</a>
are only available in <a class="reference external" href="https://docs.x.ai/docs/models/grok-3">Grok 3</a>.</p>
</div>
</dd>
<dt>Token usage:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">usage_metadata</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">'input_tokens'</span><span class="p">:</span> <span class="mi">37</span><span class="p">,</span> <span class="s1">'output_tokens'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">'total_tokens'</span><span class="p">:</span> <span class="mi">43</span><span class="p">}</span>
</pre></div>
</div>
</dd>
<dt>Logprobs:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprobs_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">logprobs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">messages</span><span class="o">=</span><span class="p">[(</span><span class="s2">"human"</span><span class="p">,</span><span class="s2">"Say Hello World! Do not return anything else."</span><span class="p">)]</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">logprobs_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">response_metadata</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'content'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">'token_ids'</span><span class="p">:</span> <span class="p">[</span><span class="mi">22557</span><span class="p">,</span> <span class="mi">3304</span><span class="p">,</span> <span class="mi">28808</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="s1">'tokens'</span><span class="p">:</span> <span class="p">[</span><span class="s1">' Hello'</span><span class="p">,</span> <span class="s1">' World'</span><span class="p">,</span> <span class="s1">'!'</span><span class="p">,</span> <span class="s1">'&lt;/s&gt;'</span><span class="p">],</span>
    <span class="s1">'token_logprobs'</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mf">4.7683716e-06</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.9604645e-07</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.057373047</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt>Response metadata</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">response_metadata</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">'token_usage'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">'completion_tokens'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s1">'prompt_tokens'</span><span class="p">:</span> <span class="mi">19</span><span class="p">,</span>
        <span class="s1">'total_tokens'</span><span class="p">:</span> <span class="mi">23</span>
        <span class="p">},</span>
    <span class="s1">'model_name'</span><span class="p">:</span> <span class="s1">'grok-4'</span><span class="p">,</span>
    <span class="s1">'system_fingerprint'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">'finish_reason'</span><span class="p">:</span> <span class="s1">'stop'</span><span class="p">,</span>
    <span class="s1">'logprobs'</span><span class="p">:</span> <span class="kc">None</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ChatXAI implements the standard <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Runnable</span> <span class="pre">Interface</span></code></a>. 🏃</p>
<p>The <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Runnable</span> <span class="pre">Interface</span></code></a> has additional methods that are available on runnables, such as <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config" title="langchain_core.runnables.base.Runnable.with_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">with_config</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_types" title="langchain_core.runnables.base.Runnable.with_types"><code class="xref py py-meth docutils literal notranslate"><span class="pre">with_types</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_retry" title="langchain_core.runnables.base.Runnable.with_retry"><code class="xref py py-meth docutils literal notranslate"><span class="pre">with_retry</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.assign" title="langchain_core.runnables.base.Runnable.assign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">assign</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.bind" title="langchain_core.runnables.base.Runnable.bind"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bind</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.get_graph" title="langchain_core.runnables.base.Runnable.get_graph"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_graph</span></code></a>, and more.</p>
</div>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.cache">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">cache</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="../../core/caches/langchain_core.caches.BaseCache.html#langchain_core.caches.BaseCache" title="langchain_core.caches.BaseCache"><span class="pre">BaseCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.cache" title="Link to this definition">#</a></dt>
<dd><p>Whether to cache the response.</p>
<ul class="simple">
<li><p>If true, will use the global cache.</p></li>
<li><p>If false, will not use a cache</p></li>
<li><p>If None, will use the global cache if it’s set, otherwise no cache.</p></li>
<li><p>If instance of BaseCache, will use the provided cache.</p></li>
</ul>
<p>Caching is not currently supported for streaming methods of models.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.callback_manager">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">callback_manager</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager" title="langchain_core.callbacks.base.BaseCallbackManager"><span class="pre">BaseCallbackManager</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.callback_manager" title="Link to this definition">#</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.1.7: </span>Use <a class="reference internal" href="#langchain_xai.chat_models.ChatXAI.callbacks" title="langchain_xai.chat_models.ChatXAI.callbacks"><code class="xref py py-meth docutils literal notranslate"><span class="pre">callbacks()</span></code></a> instead. It will be removed in pydantic==1.0.</p>
</div>
<p>Callback manager to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.callbacks">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">callbacks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callbacks</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.callbacks" title="Link to this definition">#</a></dt>
<dd><p>Callbacks to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.custom_get_token_ids">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">custom_get_token_ids</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.custom_get_token_ids" title="Link to this definition">#</a></dt>
<dd><p>Optional encoder to use for counting tokens.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.default_headers">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">default_headers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.default_headers" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.default_query">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">default_query</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">object</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.default_query" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.disable_streaming">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">disable_streaming</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'tool_calling'</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.disable_streaming" title="Link to this definition">#</a></dt>
<dd><p>Whether to disable streaming for this model.</p>
<p>If streaming is bypassed, then <code class="docutils literal notranslate"><span class="pre">stream()</span></code>/<code class="docutils literal notranslate"><span class="pre">astream()</span></code>/<code class="docutils literal notranslate"><span class="pre">astream_events()</span></code> will
defer to <code class="docutils literal notranslate"><span class="pre">invoke()</span></code>/<code class="docutils literal notranslate"><span class="pre">ainvoke()</span></code>.</p>
<ul class="simple">
<li><p>If True, will always bypass streaming case.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">'tool_calling'</span></code>, will bypass streaming case only when the model is called
with a <code class="docutils literal notranslate"><span class="pre">tools</span></code> keyword argument. In other words, LangChain will automatically
switch to non-streaming behavior (<code class="docutils literal notranslate"><span class="pre">invoke()</span></code>) only when the tools argument is
provided. This offers the best of both worlds.</p></li>
<li><p>If False (default), will always use streaming case if available.</p></li>
</ul>
<p>The main reason for this flag is that code might be written using <code class="docutils literal notranslate"><span class="pre">.stream()</span></code> and
a user may want to swap out a given model for another model whose the implementation
does not properly support streaming.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.disabled_params">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">disabled_params</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.disabled_params" title="Link to this definition">#</a></dt>
<dd><p>Parameters of the OpenAI client or chat.completions endpoint that should be
disabled for the given model.</p>
<p>Should be specified as <code class="docutils literal notranslate"><span class="pre">{"param":</span> <span class="pre">None</span> <span class="pre">|</span> <span class="pre">['val1',</span> <span class="pre">'val2']}</span></code> where the key is the
parameter and the value is either None, meaning that parameter should never be
used, or it’s a list of disabled values for the parameter.</p>
<p>For example, older models may not support the ‘parallel_tool_calls’ parameter at
all, in which case <code class="docutils literal notranslate"><span class="pre">disabled_params={"parallel_tool_calls":</span> <span class="pre">None}</span></code> can be passed
in.</p>
<p>If a parameter is disabled then it will not be used by default in any methods, e.g.
in <a class="reference internal" href="../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.with_structured_output" title="langchain_openai.chat_models.base.ChatOpenAI.with_structured_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">with_structured_output()</span></code></a>.
However this does not prevent a user from directly passed in the parameter during
invocation.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.extra_body">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">extra_body</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.extra_body" title="Link to this definition">#</a></dt>
<dd><p>Optional additional JSON properties to include in the request parameters when
making requests to OpenAI compatible APIs, such as vLLM.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.frequency_penalty">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">frequency_penalty</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.frequency_penalty" title="Link to this definition">#</a></dt>
<dd><p>Penalizes repeated tokens according to frequency.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.http_async_client">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">http_async_client</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.http_async_client" title="Link to this definition">#</a></dt>
<dd><p>Optional httpx.AsyncClient. Only used for async invocations. Must specify
<code class="docutils literal notranslate"><span class="pre">http_client</span></code> as well if you’d like a custom client for sync invocations.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.http_client">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">http_client</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.http_client" title="Link to this definition">#</a></dt>
<dd><p>Optional <code class="docutils literal notranslate"><span class="pre">httpx.Client</span></code>. Only used for sync invocations. Must specify
<code class="docutils literal notranslate"><span class="pre">http_async_client</span></code> as well if you’d like a custom client for async
invocations.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.include">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">include</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.include" title="Link to this definition">#</a></dt>
<dd><p>Additional fields to include in generations from Responses API.</p>
<p>Supported values:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">"file_search_call.results"</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"message.input_image.image_url"</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"computer_call_output.output.image_url"</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"reasoning.encrypted_content"</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"code_interpreter_call.outputs"</span></code></p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.24.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.include_response_headers">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">include_response_headers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.include_response_headers" title="Link to this definition">#</a></dt>
<dd><p>Whether to include response headers in the output message response_metadata.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.logit_bias">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logit_bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.logit_bias" title="Link to this definition">#</a></dt>
<dd><p>Modify the likelihood of specified tokens appearing in the completion.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.logprobs">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logprobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.logprobs" title="Link to this definition">#</a></dt>
<dd><p>Whether to return logprobs.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.max_retries">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_retries</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.max_retries" title="Link to this definition">#</a></dt>
<dd><p>Maximum number of retries to make when generating.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.max_tokens">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_tokens</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.max_tokens" title="Link to this definition">#</a></dt>
<dd><p>Maximum number of tokens to generate.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.metadata">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">metadata</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.metadata" title="Link to this definition">#</a></dt>
<dd><p>Metadata to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.model_kwargs">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><em class="property"> <span class="pre">[Optional]</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.model_kwargs" title="Link to this definition">#</a></dt>
<dd><p>Holds any model parameters valid for <cite>create</cite> call not explicitly specified.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.model_name">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'grok-4'</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'model')</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.model_name" title="Link to this definition">#</a></dt>
<dd><p>Model name to use.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.n">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.n" title="Link to this definition">#</a></dt>
<dd><p>Number of chat completions to generate for each prompt.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.openai_api_base">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_base</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.openai_api_base" title="Link to this definition">#</a></dt>
<dd><p>Base URL path for API requests, leave blank if not using a proxy or service
emulator.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.openai_api_key">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">SecretStr</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.openai_api_key" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.openai_organization">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_organization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'organization')</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.openai_organization" title="Link to this definition">#</a></dt>
<dd><p>Automatically inferred from env var <cite>OPENAI_ORG_ID</cite> if not provided.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.openai_proxy">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_proxy</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">[Optional]</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.openai_proxy" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.output_version">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">output_version</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'v0'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'responses/v1'</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'v0'</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.output_version" title="Link to this definition">#</a></dt>
<dd><p>Version of AIMessage output format to use.</p>
<p>This field is used to roll-out new output formats for chat model AIMessages
in a backwards-compatible way.</p>
<p>Supported values:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">"v0"</span></code>: AIMessage format as of langchain-openai 0.3.x.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"responses/v1"</span></code>: Formats Responses API output
items into AIMessage content blocks.</p></li>
</ul>
<p>Currently only impacts the Responses API. <code class="docutils literal notranslate"><span class="pre">output_version="responses/v1"</span></code> is
recommended.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.25.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.presence_penalty">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">presence_penalty</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.presence_penalty" title="Link to this definition">#</a></dt>
<dd><p>Penalizes repeated tokens.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.rate_limiter">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">rate_limiter</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="../../core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter" title="langchain_core.rate_limiters.BaseRateLimiter"><span class="pre">BaseRateLimiter</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.rate_limiter" title="Link to this definition">#</a></dt>
<dd><p>An optional rate limiter to use for limiting the number of requests.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.reasoning">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">reasoning</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.reasoning" title="Link to this definition">#</a></dt>
<dd><p>Reasoning parameters for reasoning models, i.e., OpenAI o-series models (o1, o3,
o4-mini, etc.). For use with the Responses API.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reasoning</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">"effort"</span><span class="p">:</span> <span class="s2">"medium"</span><span class="p">,</span>  <span class="c1"># can be "low", "medium", or "high"</span>
    <span class="s2">"summary"</span><span class="p">:</span> <span class="s2">"auto"</span><span class="p">,</span>  <span class="c1"># can be "auto", "concise", or "detailed"</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.24.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.reasoning_effort">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">reasoning_effort</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.reasoning_effort" title="Link to this definition">#</a></dt>
<dd><p>Constrains effort on reasoning for reasoning models. For use with the Chat
Completions API.</p>
<p>Reasoning models only, like OpenAI o1, o3, and o4-mini.</p>
<p>Currently supported values are low, medium, and high. Reducing reasoning effort
can result in faster responses and fewer tokens used on reasoning in a response.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.2.14.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.request_timeout">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">request_timeout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'timeout')</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.request_timeout" title="Link to this definition">#</a></dt>
<dd><p>Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or
None.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.search_parameters">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">search_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.search_parameters" title="Link to this definition">#</a></dt>
<dd><p>Parameters for search requests. Example: <code class="docutils literal notranslate"><span class="pre">{"mode":</span> <span class="pre">"auto"}</span></code>.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.seed">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">seed</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.seed" title="Link to this definition">#</a></dt>
<dd><p>Seed for generation</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.service_tier">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">service_tier</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.service_tier" title="Link to this definition">#</a></dt>
<dd><p>Latency tier for request. Options are <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>, <code class="docutils literal notranslate"><span class="pre">'default'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'flex'</span></code>.
Relevant for users of OpenAI’s scale tier service.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.stop">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stop</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'stop_sequences')</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.stop" title="Link to this definition">#</a></dt>
<dd><p>Default stop sequences.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.store">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.store" title="Link to this definition">#</a></dt>
<dd><p>If True, OpenAI may store response data for future use. Defaults to True
for the Responses API and False for the Chat Completions API.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.24.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.stream_usage">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stream_usage</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.stream_usage" title="Link to this definition">#</a></dt>
<dd><p>Whether to include usage metadata in streaming output. If True, an additional
message chunk will be generated during the stream including usage metadata.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.9.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.streaming">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">streaming</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.streaming" title="Link to this definition">#</a></dt>
<dd><p>Whether to stream the results or not.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.tags">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">tags</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.tags" title="Link to this definition">#</a></dt>
<dd><p>Tags to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.temperature">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">temperature</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.temperature" title="Link to this definition">#</a></dt>
<dd><p>What sampling temperature to use.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.tiktoken_model_name">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">tiktoken_model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.tiktoken_model_name" title="Link to this definition">#</a></dt>
<dd><p>The model name to pass to tiktoken when using this class.
Tiktoken is used to count the number of tokens in documents to constrain
them to be under a certain limit. By default, when set to None, this will
be the same as the embedding model name. However, there are some cases
where you may want to use this Embedding class with a model name not
supported by tiktoken. This can include when using Azure embeddings or
when using one of the many model providers that expose an OpenAI-like
API but with different models. In those cases, in order to avoid erroring
when tiktoken is called, you can specify a model name to use here.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.top_logprobs">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">top_logprobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.top_logprobs" title="Link to this definition">#</a></dt>
<dd><p>Number of most likely tokens to return at each token position, each with
an associated log probability. <cite>logprobs</cite> must be set to true
if this parameter is used.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.top_p">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">top_p</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.top_p" title="Link to this definition">#</a></dt>
<dd><p>Total probability mass of tokens to consider at each step.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.truncation">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">truncation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.truncation" title="Link to this definition">#</a></dt>
<dd><p>Truncation strategy (Responses API). Can be <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> or <code class="docutils literal notranslate"><span class="pre">'disabled'</span></code>
(default). If <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>, model may drop input items from the middle of the
message sequence to fit the context window.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.24.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.use_previous_response_id">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">use_previous_response_id</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.use_previous_response_id" title="Link to this definition">#</a></dt>
<dd><p>If True, always pass <code class="docutils literal notranslate"><span class="pre">previous_response_id</span></code> using the ID of the most recent
response. Responses API only.</p>
<p>Input messages up to the most recent response will be dropped from request
payloads.</p>
<p>For example, the following two are equivalent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"o4-mini"</span><span class="p">,</span>
    <span class="n">use_previous_response_id</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">HumanMessage</span><span class="p">(</span><span class="s2">"Hello"</span><span class="p">),</span>
        <span class="n">AIMessage</span><span class="p">(</span><span class="s2">"Hi there!"</span><span class="p">,</span> <span class="n">response_metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">"id"</span><span class="p">:</span> <span class="s2">"resp_123"</span><span class="p">}),</span>
        <span class="n">HumanMessage</span><span class="p">(</span><span class="s2">"How are you?"</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"o4-mini"</span><span class="p">,</span>
    <span class="n">use_responses_api</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">"How are you?"</span><span class="p">)],</span> <span class="n">previous_response_id</span><span class="o">=</span><span class="s2">"resp_123"</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.26.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.use_responses_api">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">use_responses_api</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.use_responses_api" title="Link to this definition">#</a></dt>
<dd><p>Whether to use the Responses API instead of the Chat API.</p>
<p>If not specified then will be inferred based on invocation params.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.3.9.</span></p>
</div>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.verbose">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">verbose</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"> <span class="pre">[Optional]</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.verbose" title="Link to this definition">#</a></dt>
<dd><p>Whether to print out response text.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.xai_api_base">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">xai_api_base</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'https://api.x.ai/v1/'</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.xai_api_base" title="Link to this definition">#</a></dt>
<dd><p>Base URL path for API requests.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.xai_api_key">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">xai_api_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">SecretStr</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">[Optional]</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'api_key')</span></em><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.xai_api_key" title="Link to this definition">#</a></dt>
<dd><p>xAI API key.</p>
<p>Automatically read from env variable <code class="docutils literal notranslate"><span class="pre">XAI_API_KEY</span></code> if not provided.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain_core.callbacks.base.BaseCallbackHandler" title="langchain_core.callbacks.base.BaseCallbackHandler"><span class="pre">BaseCallbackHandler</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager" title="langchain_core.callbacks.base.BaseCallbackManager"><span class="pre">BaseCallbackManager</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.__call__" title="Link to this definition">#</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.1.7: </span>Use <a class="reference internal" href="#langchain_xai.chat_models.ChatXAI.invoke" title="langchain_xai.chat_models.ChatXAI.invoke"><code class="xref py py-meth docutils literal notranslate"><span class="pre">invoke()</span></code></a> instead. It will not be removed until langchain-core==1.0.</p>
</div>
<p>Call the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>messages</strong> (<em>list</em><em>[</em><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a><em>]</em>) – List of messages.</p></li>
<li><p><strong>stop</strong> (<em>list</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – Stop words to use when generating. Model output is cut off at the
first occurrence of any of these substrings.</p></li>
<li><p><strong>callbacks</strong> (<em>list</em><em>[</em><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain_core.callbacks.base.BaseCallbackHandler" title="langchain_core.callbacks.base.BaseCallbackHandler"><em>BaseCallbackHandler</em></a><em>] </em><em>| </em><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager" title="langchain_core.callbacks.base.BaseCallbackManager"><em>BaseCallbackManager</em></a><em> | </em><em>None</em>) – Callbacks to pass through. Used for executing additional
functionality, such as logging or streaming, throughout generation.</p></li>
<li><p><strong>**kwargs</strong> (<em>Any</em>) – Arbitrary additional keyword arguments. These are usually passed
to the model provider API call.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The model output message.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.abatch">
<em class="property"><span class="k"><span class="pre">async</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">abatch</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.abatch" title="Link to this definition">#</a></dt>
<dd><p>Default implementation runs ainvoke in parallel using asyncio.gather.</p>
<p>The default implementation of batch works well for IO bound runnables.</p>
<p>Subclasses should override this method if they can batch more efficiently;
e.g., if the underlying Runnable uses an API which supports a batch mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>list</em><em>[</em><em>Input</em><em>]</em>) – A list of inputs to the Runnable.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>list</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>) – A config to use when invoking the Runnable.
The config supports standard keys like ‘tags’, ‘metadata’ for tracing
purposes, ‘max_concurrency’ for controlling how much work to do
in parallel, and other keys. Please refer to the RunnableConfig
for more details. Defaults to None.</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>) – Whether to return exceptions instead of raising them.
Defaults to False.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of outputs from the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[<em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.abatch_as_completed">
<em class="property"><span class="k"><span class="pre">async</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">abatch_as_completed</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">AsyncIterator</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Exception</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.abatch_as_completed" title="Link to this definition">#</a></dt>
<dd><p>Run ainvoke in parallel on a list of inputs.</p>
<p>Yields results as they complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Sequence</em><em>[</em><em>Input</em><em>]</em>) – A list of inputs to the Runnable.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>) – A config to use when invoking the Runnable.
The config supports standard keys like ‘tags’, ‘metadata’ for tracing
purposes, ‘max_concurrency’ for controlling how much work to do
in parallel, and other keys. Please refer to the RunnableConfig
for more details. Defaults to None. Defaults to None.</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>) – Whether to return exceptions instead of raising them.
Defaults to False.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple of the index of the input and the output from the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>AsyncIterator</em>[tuple[int, <em>Output</em> | Exception]]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.ainvoke">
<em class="property"><span class="k"><span class="pre">async</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ainvoke</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.ainvoke" title="Link to this definition">#</a></dt>
<dd><p>Default implementation of ainvoke, calls invoke from a thread.</p>
<p>The default implementation allows usage of async code even if
the Runnable did not implement a native async version of invoke.</p>
<p>Subclasses should override this method if they can run asynchronously.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>)</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>)</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage">BaseMessage</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.astream">
<em class="property"><span class="k"><span class="pre">async</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">astream</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">AsyncIterator</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk"><span class="pre">BaseMessageChunk</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.astream" title="Link to this definition">#</a></dt>
<dd><p>Default implementation of astream, which calls ainvoke.</p>
<p>Subclasses should override this method if they support streaming output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>) – The config to use for the Runnable. Defaults to None.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>AsyncIterator[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk">BaseMessageChunk</a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.astream_events">
<em class="property"><span class="k"><span class="pre">async</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">astream_events</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">version</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'v1'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'v2'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'v2'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">include_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">include_types</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">include_tags</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">exclude_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">exclude_types</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">exclude_tags</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">AsyncIterator</span><span class="p"><span class="pre">[</span></span><span class="pre">StreamEvent</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.astream_events" title="Link to this definition">#</a></dt>
<dd><p>Generate a stream of events.</p>
<p>Use to create an iterator over StreamEvents that provide real-time information
about the progress of the Runnable, including StreamEvents from intermediate
results.</p>
<p>A StreamEvent is a dictionary with the following schema:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">event</span></code>: <strong>str</strong> - Event names are of the format:
on_[runnable_type]_(start|stream|end).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code>: <strong>str</strong> - The name of the Runnable that generated the event.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_id</span></code>: <strong>str</strong> - randomly generated ID associated with the given
execution of the Runnable that emitted the event. A child Runnable that gets
invoked as part of the execution of a parent Runnable is assigned its own
unique ID.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parent_ids</span></code>: <strong>list[str]</strong> - The IDs of the parent runnables that generated
the event. The root Runnable will have an empty list. The order of the parent
IDs is from the root to the immediate parent. Only available for v2 version of
the API. The v1 version of the API will return an empty list.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tags</span></code>: <strong>Optional[list[str]]</strong> - The tags of the Runnable that generated
the event.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">metadata</span></code>: <strong>Optional[dict[str, Any]]</strong> - The metadata of the Runnable that
generated the event.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data</span></code>: <strong>dict[str, Any]</strong></p></li>
</ul>
<p>Below is a table that illustrates some events that might be emitted by various
chains. Metadata fields have been omitted from the table for brevity.
Chain definitions have been included after the table.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This reference table is for the V2 version of the schema.</p>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>event</p></th>
<th class="head"><p>name</p></th>
<th class="head"><p>chunk</p></th>
<th class="head"><p>input</p></th>
<th class="head"><p>output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>on_chat_model_start</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>{“messages”: [[SystemMessage, HumanMessage]]}</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>on_chat_model_stream</p></td>
<td><p>[model name]</p></td>
<td><p>AIMessageChunk(content=”hello”)</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_chat_model_end</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>{“messages”: [[SystemMessage, HumanMessage]]}</p></td>
<td><p>AIMessageChunk(content=”hello world”)</p></td>
</tr>
<tr class="row-odd"><td><p>on_llm_start</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>{‘input’: ‘hello’}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_llm_stream</p></td>
<td><p>[model name]</p></td>
<td><p>‘Hello’</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>on_llm_end</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>‘Hello human!’</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_chain_start</p></td>
<td><p>format_docs</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>on_chain_stream</p></td>
<td><p>format_docs</p></td>
<td><p>“hello world!, goodbye world!”</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_chain_end</p></td>
<td><p>format_docs</p></td>
<td></td>
<td><p>[Document(…)]</p></td>
<td><p>“hello world!, goodbye world!”</p></td>
</tr>
<tr class="row-odd"><td><p>on_tool_start</p></td>
<td><p>some_tool</p></td>
<td></td>
<td><p>{“x”: 1, “y”: “2”}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_tool_end</p></td>
<td><p>some_tool</p></td>
<td></td>
<td></td>
<td><p>{“x”: 1, “y”: “2”}</p></td>
</tr>
<tr class="row-odd"><td><p>on_retriever_start</p></td>
<td><p>[retriever name]</p></td>
<td></td>
<td><p>{“query”: “hello”}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_retriever_end</p></td>
<td><p>[retriever name]</p></td>
<td></td>
<td><p>{“query”: “hello”}</p></td>
<td><p>[Document(…), ..]</p></td>
</tr>
<tr class="row-odd"><td><p>on_prompt_start</p></td>
<td><p>[template_name]</p></td>
<td></td>
<td><p>{“question”: “hello”}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_prompt_end</p></td>
<td><p>[template_name]</p></td>
<td></td>
<td><p>{“question”: “hello”}</p></td>
<td><p>ChatPromptValue(messages: [SystemMessage, …])</p></td>
</tr>
</tbody>
</table>
</div>
<p>In addition to the standard events, users can also dispatch custom events (see example below).</p>
<p>Custom events will be only be surfaced with in the <cite>v2</cite> version of the API!</p>
<p>A custom event has following format:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>name</p></td>
<td><p>str</p></td>
<td><p>A user defined name for the event.</p></td>
</tr>
<tr class="row-odd"><td><p>data</p></td>
<td><p>Any</p></td>
<td><p>The data associated with the event. This can be anything, though we suggest making it JSON serializable.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Here are declarations associated with the standard events shown above:</p>
<p><cite>format_docs</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Document</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">'''Format the docs.'''</span>
    <span class="k">return</span> <span class="s2">", "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">])</span>

<span class="n">format_docs</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">format_docs</span><span class="p">)</span>
</pre></div>
</div>
<p><cite>some_tool</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">some_tool</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">'''Some_tool.'''</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">"x"</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
</pre></div>
</div>
<p><cite>prompt</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[(</span><span class="s2">"system"</span><span class="p">,</span> <span class="s2">"You are Cat Agent 007"</span><span class="p">),</span> <span class="p">(</span><span class="s2">"human"</span><span class="p">,</span> <span class="s2">"</span><span class="si">{question}</span><span class="s2">"</span><span class="p">)]</span>
<span class="p">)</span><span class="o">.</span><span class="n">with_config</span><span class="p">({</span><span class="s2">"run_name"</span><span class="p">:</span> <span class="s2">"my_template"</span><span class="p">,</span> <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"my_template"</span><span class="p">]})</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">reverse</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">reverse</span><span class="p">)</span>

<span class="n">events</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">event</span> <span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">chain</span><span class="o">.</span><span class="n">astream_events</span><span class="p">(</span><span class="s2">"hello"</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">"v2"</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># will produce the following events (run_id, and parent_ids</span>
<span class="c1"># has been omitted for brevity):</span>
<span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">"data"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"input"</span><span class="p">:</span> <span class="s2">"hello"</span><span class="p">},</span>
        <span class="s2">"event"</span><span class="p">:</span> <span class="s2">"on_chain_start"</span><span class="p">,</span>
        <span class="s2">"metadata"</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"reverse"</span><span class="p">,</span>
        <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[],</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">"data"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"chunk"</span><span class="p">:</span> <span class="s2">"olleh"</span><span class="p">},</span>
        <span class="s2">"event"</span><span class="p">:</span> <span class="s2">"on_chain_stream"</span><span class="p">,</span>
        <span class="s2">"metadata"</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"reverse"</span><span class="p">,</span>
        <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[],</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">"data"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"output"</span><span class="p">:</span> <span class="s2">"olleh"</span><span class="p">},</span>
        <span class="s2">"event"</span><span class="p">:</span> <span class="s2">"on_chain_end"</span><span class="p">,</span>
        <span class="s2">"metadata"</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"reverse"</span><span class="p">,</span>
        <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[],</span>
    <span class="p">},</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Example: Dispatch Custom Event</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.callbacks.manager</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">adispatch_custom_event</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span><span class="p">,</span> <span class="n">RunnableConfig</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>


<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">slow_thing</span><span class="p">(</span><span class="n">some_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Do something that takes a long time."""</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Placeholder for some slow operation</span>
    <span class="k">await</span> <span class="n">adispatch_custom_event</span><span class="p">(</span>
        <span class="s2">"progress_event"</span><span class="p">,</span>
        <span class="p">{</span><span class="s2">"message"</span><span class="p">:</span> <span class="s2">"Finished step 1 of 3"</span><span class="p">},</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span> <span class="c1"># Must be included for python &lt; 3.10</span>
    <span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Placeholder for some slow operation</span>
    <span class="k">await</span> <span class="n">adispatch_custom_event</span><span class="p">(</span>
        <span class="s2">"progress_event"</span><span class="p">,</span>
        <span class="p">{</span><span class="s2">"message"</span><span class="p">:</span> <span class="s2">"Finished step 2 of 3"</span><span class="p">},</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span> <span class="c1"># Must be included for python &lt; 3.10</span>
    <span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Placeholder for some slow operation</span>
    <span class="k">return</span> <span class="s2">"Done"</span>

<span class="n">slow_thing</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">slow_thing</span><span class="p">)</span>

<span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">slow_thing</span><span class="o">.</span><span class="n">astream_events</span><span class="p">(</span><span class="s2">"some_input"</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">"v2"</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Any</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>) – The config to use for the Runnable.</p></li>
<li><p><strong>version</strong> (<em>Literal</em><em>[</em><em>'v1'</em><em>, </em><em>'v2'</em><em>]</em>) – The version of the schema to use either <cite>v2</cite> or <cite>v1</cite>.
Users should use <cite>v2</cite>.
<cite>v1</cite> is for backwards compatibility and will be deprecated
in 0.4.0.
No default will be assigned until the API is stabilized.
custom events will only be surfaced in <cite>v2</cite>.</p></li>
<li><p><strong>include_names</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Only include events from runnables with matching names.</p></li>
<li><p><strong>include_types</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Only include events from runnables with matching types.</p></li>
<li><p><strong>include_tags</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Only include events from runnables with matching tags.</p></li>
<li><p><strong>exclude_names</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Exclude events from runnables with matching names.</p></li>
<li><p><strong>exclude_types</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Exclude events from runnables with matching types.</p></li>
<li><p><strong>exclude_tags</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Exclude events from runnables with matching tags.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.
These will be passed to astream_log as this implementation
of astream_events is built on top of astream_log.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>An async stream of StreamEvents.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – If the version is not <cite>v1</cite> or <cite>v2</cite>.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>AsyncIterator[StreamEvent]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.batch">
<span class="sig-name descname"><span class="pre">batch</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.batch" title="Link to this definition">#</a></dt>
<dd><p>Default implementation runs invoke in parallel using a thread pool executor.</p>
<p>The default implementation of batch works well for IO bound runnables.</p>
<p>Subclasses should override this method if they can batch more efficiently;
e.g., if the underlying Runnable uses an API which supports a batch mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>list</em><em>[</em><em>Input</em><em>]</em>)</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>list</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>)</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>list[<em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.batch_as_completed">
<span class="sig-name descname"><span class="pre">batch_as_completed</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Exception</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.batch_as_completed" title="Link to this definition">#</a></dt>
<dd><p>Run invoke in parallel on a list of inputs.</p>
<p>Yields results as they complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Sequence</em><em>[</em><em>Input</em><em>]</em>)</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>)</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Iterator</em>[tuple[int, <em>Output</em> | Exception]]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.bind">
<span class="sig-name descname"><span class="pre">bind</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.bind" title="Link to this definition">#</a></dt>
<dd><p>Bind arguments to a Runnable, returning a new Runnable.</p>
<p>Useful when a Runnable in a chain requires an argument that is not
in the output of the previous Runnable or included in the user input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>kwargs</strong> (<em>Any</em>) – The arguments to bind to the Runnable.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the arguments bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">StrOutputParser</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOllama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">'llama2'</span><span class="p">)</span>

<span class="c1"># Without bind.</span>
<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">llm</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"Repeat quoted words exactly: 'One two three four five.'"</span><span class="p">)</span>
<span class="c1"># Output is 'One two three four five.'</span>

<span class="c1"># With bind.</span>
<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">llm</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s2">"three"</span><span class="p">])</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"Repeat quoted words exactly: 'One two three four five.'"</span><span class="p">)</span>
<span class="c1"># Output is 'One two'</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.bind_functions">
<span class="sig-name descname"><span class="pre">bind_functions</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">functions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">type</span><span class="p"><span class="pre">[</span></span><span class="pre">BaseModel</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><span class="pre">BaseTool</span></a><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">function_call</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_FunctionCall</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'auto'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'none'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><span class="pre">PromptValue</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.bind_functions" title="Link to this definition">#</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.2.1: </span>Use <a class="reference internal" href="../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind_tools" title="langchain_openai.chat_models.base.ChatOpenAI.bind_tools"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bind_tools()</span></code></a> instead. It will not be removed until langchain-openai==1.0.0.</p>
</div>
<p>Bind functions (and other objects) to this chat model.</p>
<p>Assumes model is compatible with OpenAI function-calling API.</p>
<dl class="simple">
<dt>NOTE: Using bind_tools is recommended instead, as the <cite>functions</cite> and</dt><dd><p><cite>function_call</cite> request parameters are officially marked as deprecated by
OpenAI.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>functions</strong> (<em>Sequence</em><em>[</em><em>dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>type</em><em>[</em><em>BaseModel</em><em>] </em><em>| </em><em>Callable</em><em> | </em><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><em>BaseTool</em></a><em>]</em>) – A list of function definitions to bind to this chat model.
Can be  a dictionary, pydantic model, or callable. Pydantic
models and callables will be automatically converted to
their schema dictionary representation.</p></li>
<li><p><strong>function_call</strong> (<em>_FunctionCall</em><em> | </em><em>str</em><em> | </em><em>Literal</em><em>[</em><em>'auto'</em><em>, </em><em>'none'</em><em>] </em><em>| </em><em>None</em>) – Which function to require the model to call.
Must be the name of the single provided function or
“auto” to automatically determine which function to call
(if any).</p></li>
<li><p><strong>**kwargs</strong> (<em>Any</em>) – Any additional parameters to pass to the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Runnable</span></code> constructor.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><em>PromptValue</em></a> | str | <em>Sequence</em>[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a> | list[str] | tuple[str, str] | str | dict[str, <em>Any</em>]], <a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.bind_tools">
<span class="sig-name descname"><span class="pre">bind_tools</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">tools</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">type</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><span class="pre">BaseTool</span></a><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">tool_choice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'auto'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'none'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'required'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'any'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">parallel_tool_calls</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><span class="pre">PromptValue</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.bind_tools" title="Link to this definition">#</a></dt>
<dd><p>Bind tool-like objects to this chat model.</p>
<p>Assumes model is compatible with OpenAI tool-calling API.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tools</strong> (<em>Sequence</em><em>[</em><em>dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>type</em><em> | </em><em>Callable</em><em> | </em><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><em>BaseTool</em></a><em>]</em>) – A list of tool definitions to bind to this chat model.
Supports any tool definition handled by
<a class="reference internal" href="../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool" title="langchain_core.utils.function_calling.convert_to_openai_tool"><code class="xref py py-meth docutils literal notranslate"><span class="pre">langchain_core.utils.function_calling.convert_to_openai_tool()</span></code></a>.</p></li>
<li><p><strong>tool_choice</strong> (<em>dict</em><em> | </em><em>str</em><em> | </em><em>Literal</em><em>[</em><em>'auto'</em><em>, </em><em>'none'</em><em>, </em><em>'required'</em><em>, </em><em>'any'</em><em>] </em><em>| </em><em>bool</em><em> | </em><em>None</em>) – <p>Which tool to require the model to call. Options are:</p>
<ul>
<li><p>str of the form <code class="docutils literal notranslate"><span class="pre">"&lt;&lt;tool_name&gt;&gt;"</span></code>: calls &lt;&lt;tool_name&gt;&gt; tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"auto"</span></code>: automatically selects a tool (including no tool).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"none"</span></code>: does not call a tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"any"</span></code> or <code class="docutils literal notranslate"><span class="pre">"required"</span></code> or <code class="docutils literal notranslate"><span class="pre">True</span></code>: force at least one tool to be called.</p></li>
<li><p>dict of the form <code class="docutils literal notranslate"><span class="pre">{"type":</span> <span class="pre">"function",</span> <span class="pre">"function":</span> <span class="pre">{"name":</span> <span class="pre">&lt;&lt;tool_name&gt;&gt;}}</span></code>: calls &lt;&lt;tool_name&gt;&gt; tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code> or <code class="docutils literal notranslate"><span class="pre">None</span></code>: no effect, default OpenAI behavior.</p></li>
</ul>
</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em> | </em><em>None</em>) – If True, model output is guaranteed to exactly match the JSON Schema
provided in the tool definition. If True, the input schema will be
validated according to
<a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas">https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</a>.
If False, input schema will not be validated and model output will not
be validated.
If None, <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument will not be passed to the model.</p></li>
<li><p><strong>parallel_tool_calls</strong> (<em>bool</em><em> | </em><em>None</em>) – Set to <code class="docutils literal notranslate"><span class="pre">False</span></code> to disable parallel tool use.
Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code> (no specification, which allows parallel tool use).</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Any additional parameters are passed directly to
<a class="reference internal" href="../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind" title="langchain_openai.chat_models.base.ChatOpenAI.bind"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bind()</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><em>PromptValue</em></a> | str | <em>Sequence</em>[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a> | list[str] | tuple[str, str] | str | dict[str, <em>Any</em>]], <a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a>]</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.1.21: </span>Support for <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument added.</p>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.configurable_alternatives">
<span class="sig-name descname"><span class="pre">configurable_alternatives</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">which</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><span class="pre">ConfigurableField</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">default_key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'default'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">prefix_keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><span class="pre">RunnableSerializable</span></a></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.configurable_alternatives" title="Link to this definition">#</a></dt>
<dd><p>Configure alternatives for Runnables that can be set at runtime.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>which</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><em>ConfigurableField</em></a>) – The ConfigurableField instance that will be used to select the
alternative.</p></li>
<li><p><strong>default_key</strong> (<em>str</em>) – The default key to use if no alternative is selected.
Defaults to “default”.</p></li>
<li><p><strong>prefix_keys</strong> (<em>bool</em>) – Whether to prefix the keys with the ConfigurableField id.
Defaults to False.</p></li>
<li><p><strong>**kwargs</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>] </em><em>| </em><em>Callable</em><em>[</em><em>[</em><em>]</em><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>]</em><em>]</em>) – A dictionary of keys to Runnable instances or callables that
return Runnable instances.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the alternatives configured.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><em>RunnableSerializable</em></a></p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_anthropic</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConfigurableField</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">"claude-3-sonnet-20240229"</span>
<span class="p">)</span><span class="o">.</span><span class="n">configurable_alternatives</span><span class="p">(</span>
    <span class="n">ConfigurableField</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">"llm"</span><span class="p">),</span>
    <span class="n">default_key</span><span class="o">=</span><span class="s2">"anthropic"</span><span class="p">,</span>
    <span class="n">openai</span><span class="o">=</span><span class="n">ChatOpenAI</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># uses the default model ChatAnthropic</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"which organization created you?"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="c1"># uses ChatOpenAI</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">with_config</span><span class="p">(</span>
        <span class="n">configurable</span><span class="o">=</span><span class="p">{</span><span class="s2">"llm"</span><span class="p">:</span> <span class="s2">"openai"</span><span class="p">}</span>
    <span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"which organization created you?"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.configurable_fields">
<span class="sig-name descname"><span class="pre">configurable_fields</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><span class="pre">ConfigurableField</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldSingleOption.html#langchain_core.runnables.utils.ConfigurableFieldSingleOption" title="langchain_core.runnables.utils.ConfigurableFieldSingleOption"><span class="pre">ConfigurableFieldSingleOption</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldMultiOption.html#langchain_core.runnables.utils.ConfigurableFieldMultiOption" title="langchain_core.runnables.utils.ConfigurableFieldMultiOption"><span class="pre">ConfigurableFieldMultiOption</span></a></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><span class="pre">RunnableSerializable</span></a></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.configurable_fields" title="Link to this definition">#</a></dt>
<dd><p>Configure particular Runnable fields at runtime.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**kwargs</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><em>ConfigurableField</em></a><em> | </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldSingleOption.html#langchain_core.runnables.utils.ConfigurableFieldSingleOption" title="langchain_core.runnables.utils.ConfigurableFieldSingleOption"><em>ConfigurableFieldSingleOption</em></a><em> | </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldMultiOption.html#langchain_core.runnables.utils.ConfigurableFieldMultiOption" title="langchain_core.runnables.utils.ConfigurableFieldMultiOption"><em>ConfigurableFieldMultiOption</em></a>) – A dictionary of ConfigurableField instances to configure.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the fields configured.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><em>RunnableSerializable</em></a></p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConfigurableField</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">configurable_fields</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">ConfigurableField</span><span class="p">(</span>
        <span class="nb">id</span><span class="o">=</span><span class="s2">"output_token_number"</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">"Max tokens in the output"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">"The maximum number of tokens in the output"</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># max_tokens = 20</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"max_tokens_20: "</span><span class="p">,</span>
    <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"tell me something about chess"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="p">)</span>

<span class="c1"># max_tokens = 200</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"max_tokens_200: "</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">with_config</span><span class="p">(</span>
    <span class="n">configurable</span><span class="o">=</span><span class="p">{</span><span class="s2">"output_token_number"</span><span class="p">:</span> <span class="mi">200</span><span class="p">}</span>
    <span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"tell me something about chess"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.get_num_tokens">
<span class="sig-name descname"><span class="pre">get_num_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.get_num_tokens" title="Link to this definition">#</a></dt>
<dd><p>Get the number of tokens present in the text.</p>
<p>Useful for checking if an input fits in a model’s context window.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – The string input to tokenize.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The integer number of tokens in the text.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.get_num_tokens_from_messages">
<span class="sig-name descname"><span class="pre">get_num_tokens_from_messages</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">tools</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">type</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><span class="pre">BaseTool</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.get_num_tokens_from_messages" title="Link to this definition">#</a></dt>
<dd><p>Calculate num tokens for <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code> and <code class="docutils literal notranslate"><span class="pre">gpt-4</span></code> with <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> package.</p>
<p><strong>Requirements</strong>: You must have the <code class="docutils literal notranslate"><span class="pre">pillow</span></code> installed if you want to count
image tokens if you are specifying the image as a base64 string, and you must
have both <code class="docutils literal notranslate"><span class="pre">pillow</span></code> and <code class="docutils literal notranslate"><span class="pre">httpx</span></code> installed if you are specifying the image
as a URL. If these aren’t installed image inputs will be ignored in token
counting.</p>
<p><a class="reference external" href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb">OpenAI reference</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>messages</strong> (<em>list</em><em>[</em><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a><em>]</em>) – The message inputs to tokenize.</p></li>
<li><p><strong>tools</strong> (<em>Sequence</em><em>[</em><em>dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>type</em><em> | </em><em>Callable</em><em> | </em><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><em>BaseTool</em></a><em>] </em><em>| </em><em>None</em>) – If provided, sequence of dict, BaseModel, function, or BaseTools
to be converted to tool schemas.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.get_token_ids">
<span class="sig-name descname"><span class="pre">get_token_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.get_token_ids" title="Link to this definition">#</a></dt>
<dd><p>Get the tokens present in the text with tiktoken package.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>list[int]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.invoke">
<span class="sig-name descname"><span class="pre">invoke</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.invoke" title="Link to this definition">#</a></dt>
<dd><p>Transform a single input into an output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>) – A config to use when invoking the Runnable.
The config supports standard keys like ‘tags’, ‘metadata’ for tracing
purposes, ‘max_concurrency’ for controlling how much work to do
in parallel, and other keys. Please refer to the RunnableConfig
for more details.</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage">BaseMessage</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.stream">
<span class="sig-name descname"><span class="pre">stream</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk"><span class="pre">BaseMessageChunk</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.stream" title="Link to this definition">#</a></dt>
<dd><p>Default implementation of stream, which calls invoke.</p>
<p>Subclasses should override this method if they support streaming output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>) – The config to use for the Runnable. Defaults to None.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Iterator[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk">BaseMessageChunk</a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.with_alisteners">
<span class="sig-name descname"><span class="pre">with_alisteners</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">on_start</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncListener</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">on_end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncListener</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncListener</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.with_alisteners" title="Link to this definition">#</a></dt>
<dd><p>Bind async lifecycle listeners to a Runnable, returning a new Runnable.</p>
<p>on_start: Asynchronously called before the Runnable starts running.
on_end: Asynchronously called after the Runnable finishes running.
on_error: Asynchronously called if the Runnable throws an error.</p>
<p>The Run object contains information about the run, including its id,
type, input, output, error, start_time, end_time, and any tags or metadata
added to the run.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>on_start</strong> (<em>Optional</em><em>[</em><em>AsyncListener</em><em>]</em>) – Asynchronously called before the Runnable starts running.
Defaults to None.</p></li>
<li><p><strong>on_end</strong> (<em>Optional</em><em>[</em><em>AsyncListener</em><em>]</em>) – Asynchronously called after the Runnable finishes running.
Defaults to None.</p></li>
<li><p><strong>on_error</strong> (<em>Optional</em><em>[</em><em>AsyncListener</em><em>]</em>) – Asynchronously called if the Runnable throws an error.
Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the listeners bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable">Runnable</a>[Input, Output]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span><span class="p">,</span> <span class="n">Runnable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timezone</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>

<span class="k">def</span><span class="w"> </span><span class="nf">format_t</span><span class="p">(</span><span class="n">timestamp</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">timestamp</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="n">timezone</span><span class="o">.</span><span class="n">utc</span><span class="p">)</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">test_runnable</span><span class="p">(</span><span class="n">time_to_sleep</span> <span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Runnable[</span><span class="si">{</span><span class="n">time_to_sleep</span><span class="si">}</span><span class="s2">s]: starts at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">time_to_sleep</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Runnable[</span><span class="si">{</span><span class="n">time_to_sleep</span><span class="si">}</span><span class="s2">s]: ends at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">fn_start</span><span class="p">(</span><span class="n">run_obj</span> <span class="p">:</span> <span class="n">Runnable</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on start callback starts at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on start callback ends at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">fn_end</span><span class="p">(</span><span class="n">run_obj</span> <span class="p">:</span> <span class="n">Runnable</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on end callback starts at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on end callback ends at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">runnable</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">test_runnable</span><span class="p">)</span><span class="o">.</span><span class="n">with_alisteners</span><span class="p">(</span>
    <span class="n">on_start</span><span class="o">=</span><span class="n">fn_start</span><span class="p">,</span>
    <span class="n">on_end</span><span class="o">=</span><span class="n">fn_end</span>
<span class="p">)</span>
<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">concurrent_runs</span><span class="p">():</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">runnable</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">runnable</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

<span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">concurrent_runs</span><span class="p">())</span>
<span class="n">Result</span><span class="p">:</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">22.875378</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">22.875495</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">25.878862</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">25.878947</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">2</span><span class="n">s</span><span class="p">]:</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">25.879392</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">3</span><span class="n">s</span><span class="p">]:</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">25.879804</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">2</span><span class="n">s</span><span class="p">]:</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">27.881998</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">27.882360</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">3</span><span class="n">s</span><span class="p">]:</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">28.881737</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">28.882428</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">29.883893</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span><span class="n">T07</span><span class="p">:</span><span class="mi">05</span><span class="p">:</span><span class="mf">30.884831</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.with_config">
<span class="sig-name descname"><span class="pre">with_config</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.with_config" title="Link to this definition">#</a></dt>
<dd><p>Bind config to a Runnable, returning a new Runnable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>None</em>) – The config to bind to the Runnable.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the config bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.with_fallbacks">
<span class="sig-name descname"><span class="pre">with_fallbacks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">fallbacks:</span> <span class="pre">Sequence[Runnable[Input,</span> <span class="pre">Output]],</span> <span class="pre">*,</span> <span class="pre">exceptions_to_handle:</span> <span class="pre">tuple[type[BaseException],</span> <span class="pre">...]</span> <span class="pre">=</span> <span class="pre">(&lt;class</span> <span class="pre">'Exception'&gt;,),</span> <span class="pre">exception_key:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">RunnableWithFallbacksT</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.with_fallbacks" title="Link to this definition">#</a></dt>
<dd><p>Add fallbacks to a Runnable, returning a new Runnable.</p>
<p>The new Runnable will try the original Runnable, and then each fallback
in order, upon failures.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fallbacks</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>]</em><em>]</em>) – A sequence of runnables to try if the original Runnable fails.</p></li>
<li><p><strong>exceptions_to_handle</strong> (<em>tuple</em><em>[</em><em>type</em><em>[</em><em>BaseException</em><em>]</em><em>, </em><em>...</em><em>]</em>) – A tuple of exception types to handle.
Defaults to (Exception,).</p></li>
<li><p><strong>exception_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – If string is specified then handled exceptions will be passed
to fallbacks as part of the input under the specified key. If None,
exceptions will not be passed to fallbacks. If used, the base Runnable
and its fallbacks must accept a dictionary as input. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable that will try the original Runnable, and then each
fallback in order, upon failures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>RunnableWithFallbacksT[Input, Output]</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Iterator</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableGenerator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_generate_immediate_error</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">()</span>
    <span class="k">yield</span> <span class="s2">""</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_generate</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="k">yield from</span> <span class="s2">"foo bar"</span>


<span class="n">runnable</span> <span class="o">=</span> <span class="n">RunnableGenerator</span><span class="p">(</span><span class="n">_generate_immediate_error</span><span class="p">)</span><span class="o">.</span><span class="n">with_fallbacks</span><span class="p">(</span>
    <span class="p">[</span><span class="n">RunnableGenerator</span><span class="p">(</span><span class="n">_generate</span><span class="p">)]</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">runnable</span><span class="o">.</span><span class="n">stream</span><span class="p">({})))</span> <span class="c1">#foo bar</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fallbacks</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>]</em><em>]</em>) – A sequence of runnables to try if the original Runnable fails.</p></li>
<li><p><strong>exceptions_to_handle</strong> (<em>tuple</em><em>[</em><em>type</em><em>[</em><em>BaseException</em><em>]</em><em>, </em><em>...</em><em>]</em>) – A tuple of exception types to handle.</p></li>
<li><p><strong>exception_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – If string is specified then handled exceptions will be passed
to fallbacks as part of the input under the specified key. If None,
exceptions will not be passed to fallbacks. If used, the base Runnable
and its fallbacks must accept a dictionary as input.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable that will try the original Runnable, and then each
fallback in order, upon failures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>RunnableWithFallbacksT[Input, Output]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.with_listeners">
<span class="sig-name descname"><span class="pre">with_listeners</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">on_start</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Run</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Run</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">on_end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Run</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Run</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Run</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Run</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.with_listeners" title="Link to this definition">#</a></dt>
<dd><p>Bind lifecycle listeners to a Runnable, returning a new Runnable.</p>
<p>on_start: Called before the Runnable starts running, with the Run object.
on_end: Called after the Runnable finishes running, with the Run object.
on_error: Called if the Runnable throws an error, with the Run object.</p>
<p>The Run object contains information about the run, including its id,
type, input, output, error, start_time, end_time, and any tags or metadata
added to the run.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>on_start</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>[</em><em>[</em><em>Run</em><em>]</em><em>, </em><em>None</em><em>]</em><em>, </em><em>Callable</em><em>[</em><em>[</em><em>Run</em><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>]</em><em>]</em>) – Called before the Runnable starts running. Defaults to None.</p></li>
<li><p><strong>on_end</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>[</em><em>[</em><em>Run</em><em>]</em><em>, </em><em>None</em><em>]</em><em>, </em><em>Callable</em><em>[</em><em>[</em><em>Run</em><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>]</em><em>]</em>) – Called after the Runnable finishes running. Defaults to None.</p></li>
<li><p><strong>on_error</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>[</em><em>[</em><em>Run</em><em>]</em><em>, </em><em>None</em><em>]</em><em>, </em><em>Callable</em><em>[</em><em>[</em><em>Run</em><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>]</em><em>]</em>) – Called if the Runnable throws an error. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the listeners bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable">Runnable</a>[Input, Output]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tracers.schemas</span><span class="w"> </span><span class="kn">import</span> <span class="n">Run</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_runnable</span><span class="p">(</span><span class="n">time_to_sleep</span> <span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">time_to_sleep</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">fn_start</span><span class="p">(</span><span class="n">run_obj</span><span class="p">:</span> <span class="n">Run</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"start_time:"</span><span class="p">,</span> <span class="n">run_obj</span><span class="o">.</span><span class="n">start_time</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">fn_end</span><span class="p">(</span><span class="n">run_obj</span><span class="p">:</span> <span class="n">Run</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"end_time:"</span><span class="p">,</span> <span class="n">run_obj</span><span class="o">.</span><span class="n">end_time</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">test_runnable</span><span class="p">)</span><span class="o">.</span><span class="n">with_listeners</span><span class="p">(</span>
    <span class="n">on_start</span><span class="o">=</span><span class="n">fn_start</span><span class="p">,</span>
    <span class="n">on_end</span><span class="o">=</span><span class="n">fn_end</span>
<span class="p">)</span>
<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.with_retry">
<span class="sig-name descname"><span class="pre">with_retry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">*,</span> <span class="pre">retry_if_exception_type:</span> <span class="pre">tuple[type[BaseException],</span> <span class="pre">...]</span> <span class="pre">=</span> <span class="pre">(&lt;class</span> <span class="pre">'Exception'&gt;,),</span> <span class="pre">wait_exponential_jitter:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">exponential_jitter_params:</span> <span class="pre">Optional[ExponentialJitterParams]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">stop_after_attempt:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.with_retry" title="Link to this definition">#</a></dt>
<dd><p>Create a new Runnable that retries the original Runnable on exceptions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>retry_if_exception_type</strong> (<em>tuple</em><em>[</em><em>type</em><em>[</em><em>BaseException</em><em>]</em><em>, </em><em>...</em><em>]</em>) – A tuple of exception types to retry on.
Defaults to (Exception,).</p></li>
<li><p><strong>wait_exponential_jitter</strong> (<em>bool</em>) – Whether to add jitter to the wait
time between retries. Defaults to True.</p></li>
<li><p><strong>stop_after_attempt</strong> (<em>int</em>) – The maximum number of attempts to make before
giving up. Defaults to 3.</p></li>
<li><p><strong>exponential_jitter_params</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.retry.ExponentialJitterParams.html#langchain_core.runnables.retry.ExponentialJitterParams" title="langchain_core.runnables.retry.ExponentialJitterParams"><em>ExponentialJitterParams</em></a><em>]</em>) – Parameters for
<code class="docutils literal notranslate"><span class="pre">tenacity.wait_exponential_jitter</span></code>. Namely: <code class="docutils literal notranslate"><span class="pre">initial</span></code>, <code class="docutils literal notranslate"><span class="pre">max</span></code>,
<code class="docutils literal notranslate"><span class="pre">exp_base</span></code>, and <code class="docutils literal notranslate"><span class="pre">jitter</span></code> (all float values).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable that retries the original Runnable on exceptions.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable">Runnable</a>[Input, Output]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span>

<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_lambda</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">global</span> <span class="n">count</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"x is 1"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
         <span class="k">pass</span>


<span class="n">runnable</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">_lambda</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">runnable</span><span class="o">.</span><span class="n">with_retry</span><span class="p">(</span>
        <span class="n">stop_after_attempt</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">retry_if_exception_type</span><span class="o">=</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">,),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="k">assert</span> <span class="p">(</span><span class="n">count</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.with_structured_output">
<span class="sig-name descname"><span class="pre">with_structured_output</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">schema</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">type</span><span class="p"><span class="pre">[</span></span><span class="pre">_BM</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">type</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'function_calling'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'json_mode'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'json_schema'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'function_calling'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">include_raw</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><span class="pre">PromptValue</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">_BM</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/langchain_xai/chat_models.html#ChatXAI.with_structured_output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.with_structured_output" title="Link to this definition">#</a></dt>
<dd><p>Model wrapper that returns outputs formatted to match the given schema.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>schema</strong> (<em>dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>type</em><em>[</em><em>_BM</em><em>] </em><em>| </em><em>type</em><em> | </em><em>None</em>) – <p>The output schema. Can be passed in as:</p>
<ul>
<li><p>an OpenAI function/tool schema,</p></li>
<li><p>a JSON Schema,</p></li>
<li><p>a TypedDict class (support added in 0.1.20),</p></li>
<li><p>or a Pydantic class.</p></li>
</ul>
<p>If <code class="docutils literal notranslate"><span class="pre">schema</span></code> is a Pydantic class then the model output will be a
Pydantic instance of that class, and the model-generated fields will be
validated by the Pydantic class. Otherwise the model output will be a
dict and will not be validated. See <a class="reference internal" href="../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool" title="langchain_core.utils.function_calling.convert_to_openai_tool"><code class="xref py py-meth docutils literal notranslate"><span class="pre">langchain_core.utils.function_calling.convert_to_openai_tool()</span></code></a>
for more on how to properly specify types and descriptions of
schema fields when specifying a Pydantic or TypedDict class.</p>
</p></li>
<li><p><strong>method</strong> (<em>Literal</em><em>[</em><em>'function_calling'</em><em>, </em><em>'json_mode'</em><em>, </em><em>'json_schema'</em><em>]</em>) – <p>The method for steering model generation, one of:</p>
<ul>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'function_calling'</span></code>:</dt><dd><p>Uses xAI’s <a class="reference external" href="https://docs.x.ai/docs/guides/function-calling">tool-calling features</a>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'json_schema'</span></code>:</dt><dd><p>Uses xAI’s <a class="reference external" href="https://docs.x.ai/docs/guides/structured-outputs">structured output feature</a>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'json_mode'</span></code>:</dt><dd><p>Uses xAI’s JSON mode feature.</p>
</dd>
</dl>
</li>
</ul>
</p></li>
<li><p><strong>include_raw</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code> then only the parsed structured output is returned. If
an error occurs during model output parsing it will be raised. If <code class="docutils literal notranslate"><span class="pre">True</span></code>
then both the raw model response (a BaseMessage) and the parsed model
response will be returned. If an error occurs during output parsing it
will be caught and returned as well. The final output is always a dict
with keys <code class="docutils literal notranslate"><span class="pre">'raw'</span></code>, <code class="docutils literal notranslate"><span class="pre">'parsed'</span></code>, and <code class="docutils literal notranslate"><span class="pre">'parsing_error'</span></code>.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em> | </em><em>None</em>) – <ul>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">True</span></code>:</dt><dd><p>Model output is guaranteed to exactly match the schema.
The input schema will also be validated according to <a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas?api-mode=responses#supported-schemas">this schema</a>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">False</span></code>:</dt><dd><p>Input schema will not be validated and model output will not be
validated.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">None</span></code>:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">strict</span></code> argument will not be passed to the model.</p>
</dd>
</dl>
</li>
</ul>
</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword args aren’t supported.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A Runnable that takes same inputs as a <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain_core.language_models.chat.BaseChatModel</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">include_raw</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> and <code class="docutils literal notranslate"><span class="pre">schema</span></code> is a Pydantic class, Runnable outputs an instance of <code class="docutils literal notranslate"><span class="pre">schema</span></code> (i.e., a Pydantic object). Otherwise, if <code class="docutils literal notranslate"><span class="pre">include_raw</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> then Runnable outputs a dict.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">include_raw</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then Runnable outputs a dict with keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'raw'</span></code>: BaseMessage</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'parsed'</span></code>: None if there was a parsing error, otherwise the type depends on the <code class="docutils literal notranslate"><span class="pre">schema</span></code> as described above.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'parsing_error'</span></code>: Optional[BaseException]</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><em>PromptValue</em></a> | str | <em>Sequence</em>[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a> | list[str] | tuple[str, str] | str | dict[str, <em>Any</em>]], dict | <em>_BM</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_xai.chat_models.ChatXAI.with_types">
<span class="sig-name descname"><span class="pre">with_types</span></span><span class="sig-paren">(</span>
<dl>
<dd><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">input_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">type</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">type</span><span class="p"><span class="pre">[</span></span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>
<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_xai.chat_models.ChatXAI.with_types" title="Link to this definition">#</a></dt>
<dd><p>Bind input and output types to a Runnable, returning a new Runnable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_type</strong> (<em>type</em><em>[</em><em>Input</em><em>] </em><em>| </em><em>None</em>) – The input type to bind to the Runnable. Defaults to None.</p></li>
<li><p><strong>output_type</strong> (<em>type</em><em>[</em><em>Output</em><em>] </em><em>| </em><em>None</em>) – The output type to bind to the Runnable. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the types bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<ul class="simple">
</ul>
</section>
</article>
</div>
<dialog id="pst-secondary-sidebar-modal"></dialog>
<div class="bd-sidebar-secondary bd-toc" id="pst-secondary-sidebar"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI"><code class="docutils literal notranslate"><span class="pre">ChatXAI</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.cache"><code class="docutils literal notranslate"><span class="pre">cache</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.callback_manager"><code class="docutils literal notranslate"><span class="pre">callback_manager</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.callbacks"><code class="docutils literal notranslate"><span class="pre">callbacks</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.custom_get_token_ids"><code class="docutils literal notranslate"><span class="pre">custom_get_token_ids</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.default_headers"><code class="docutils literal notranslate"><span class="pre">default_headers</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.default_query"><code class="docutils literal notranslate"><span class="pre">default_query</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.disable_streaming"><code class="docutils literal notranslate"><span class="pre">disable_streaming</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.disabled_params"><code class="docutils literal notranslate"><span class="pre">disabled_params</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.extra_body"><code class="docutils literal notranslate"><span class="pre">extra_body</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.frequency_penalty"><code class="docutils literal notranslate"><span class="pre">frequency_penalty</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.http_async_client"><code class="docutils literal notranslate"><span class="pre">http_async_client</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.http_client"><code class="docutils literal notranslate"><span class="pre">http_client</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.include"><code class="docutils literal notranslate"><span class="pre">include</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.include_response_headers"><code class="docutils literal notranslate"><span class="pre">include_response_headers</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.logit_bias"><code class="docutils literal notranslate"><span class="pre">logit_bias</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.logprobs"><code class="docutils literal notranslate"><span class="pre">logprobs</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.max_retries"><code class="docutils literal notranslate"><span class="pre">max_retries</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.max_tokens"><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.metadata"><code class="docutils literal notranslate"><span class="pre">metadata</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.model_kwargs"><code class="docutils literal notranslate"><span class="pre">model_kwargs</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.model_name"><code class="docutils literal notranslate"><span class="pre">model_name</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.n"><code class="docutils literal notranslate"><span class="pre">n</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.openai_api_base"><code class="docutils literal notranslate"><span class="pre">openai_api_base</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.openai_api_key"><code class="docutils literal notranslate"><span class="pre">openai_api_key</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.openai_organization"><code class="docutils literal notranslate"><span class="pre">openai_organization</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.openai_proxy"><code class="docutils literal notranslate"><span class="pre">openai_proxy</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.output_version"><code class="docutils literal notranslate"><span class="pre">output_version</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.presence_penalty"><code class="docutils literal notranslate"><span class="pre">presence_penalty</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.rate_limiter"><code class="docutils literal notranslate"><span class="pre">rate_limiter</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.reasoning"><code class="docutils literal notranslate"><span class="pre">reasoning</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.reasoning_effort"><code class="docutils literal notranslate"><span class="pre">reasoning_effort</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.request_timeout"><code class="docutils literal notranslate"><span class="pre">request_timeout</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.search_parameters"><code class="docutils literal notranslate"><span class="pre">search_parameters</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.seed"><code class="docutils literal notranslate"><span class="pre">seed</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.service_tier"><code class="docutils literal notranslate"><span class="pre">service_tier</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.stop"><code class="docutils literal notranslate"><span class="pre">stop</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.store"><code class="docutils literal notranslate"><span class="pre">store</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.stream_usage"><code class="docutils literal notranslate"><span class="pre">stream_usage</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.streaming"><code class="docutils literal notranslate"><span class="pre">streaming</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.tags"><code class="docutils literal notranslate"><span class="pre">tags</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.temperature"><code class="docutils literal notranslate"><span class="pre">temperature</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.tiktoken_model_name"><code class="docutils literal notranslate"><span class="pre">tiktoken_model_name</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.top_logprobs"><code class="docutils literal notranslate"><span class="pre">top_logprobs</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.top_p"><code class="docutils literal notranslate"><span class="pre">top_p</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.truncation"><code class="docutils literal notranslate"><span class="pre">truncation</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.use_previous_response_id"><code class="docutils literal notranslate"><span class="pre">use_previous_response_id</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.use_responses_api"><code class="docutils literal notranslate"><span class="pre">use_responses_api</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.xai_api_base"><code class="docutils literal notranslate"><span class="pre">xai_api_base</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.xai_api_key"><code class="docutils literal notranslate"><span class="pre">xai_api_key</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.__call__"><code class="docutils literal notranslate"><span class="pre">__call__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.abatch"><code class="docutils literal notranslate"><span class="pre">abatch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.abatch_as_completed"><code class="docutils literal notranslate"><span class="pre">abatch_as_completed()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.ainvoke"><code class="docutils literal notranslate"><span class="pre">ainvoke()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.astream"><code class="docutils literal notranslate"><span class="pre">astream()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.astream_events"><code class="docutils literal notranslate"><span class="pre">astream_events()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.batch"><code class="docutils literal notranslate"><span class="pre">batch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.batch_as_completed"><code class="docutils literal notranslate"><span class="pre">batch_as_completed()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.bind"><code class="docutils literal notranslate"><span class="pre">bind()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.bind_functions"><code class="docutils literal notranslate"><span class="pre">bind_functions()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.bind_tools"><code class="docutils literal notranslate"><span class="pre">bind_tools()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.configurable_alternatives"><code class="docutils literal notranslate"><span class="pre">configurable_alternatives()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.configurable_fields"><code class="docutils literal notranslate"><span class="pre">configurable_fields()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.get_num_tokens"><code class="docutils literal notranslate"><span class="pre">get_num_tokens()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.get_num_tokens_from_messages"><code class="docutils literal notranslate"><span class="pre">get_num_tokens_from_messages()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.get_token_ids"><code class="docutils literal notranslate"><span class="pre">get_token_ids()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.invoke"><code class="docutils literal notranslate"><span class="pre">invoke()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.stream"><code class="docutils literal notranslate"><span class="pre">stream()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.with_alisteners"><code class="docutils literal notranslate"><span class="pre">with_alisteners()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.with_config"><code class="docutils literal notranslate"><span class="pre">with_config()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.with_fallbacks"><code class="docutils literal notranslate"><span class="pre">with_fallbacks()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.with_listeners"><code class="docutils literal notranslate"><span class="pre">with_listeners()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.with_retry"><code class="docutils literal notranslate"><span class="pre">with_retry()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.with_structured_output"><code class="docutils literal notranslate"><span class="pre">with_structured_output()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_xai.chat_models.ChatXAI.with_types"><code class="docutils literal notranslate"><span class="pre">with_types()</span></code></a></li>
</ul>
</li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script defer="" src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer="" src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2025, LangChain Inc.
      <br/>
</p>
</div>
</div>
</div>
</footer>
</body>
</html>